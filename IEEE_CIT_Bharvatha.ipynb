{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMCnS61wC3Zk",
        "outputId": "34dc9eda-812a-4022-9f1c-d1f29fc96a39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-cifar100'...\n",
            "remote: Enumerating objects: 1191, done.\u001b[K\n",
            "remote: Total 1191 (delta 0), reused 0 (delta 0), pack-reused 1191\u001b[K\n",
            "Receiving objects: 100% (1191/1191), 530.95 KiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (755/755), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/weiaicunzai/pytorch-cifar100.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd pytorch-cifar100\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLtEd2VuD2qa",
        "outputId": "020ab0ac-3c3c-44d8-906d-749c99170d30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-cifar100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6OyP_rIUEBa7",
        "outputId": "10990a32-783f-4f3b-d16d-f2e14a72ea39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/pytorch-cifar100'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir runs"
      ],
      "metadata": {
        "id": "aeT5sseIuFOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#without batch norm\n",
        "!python train.py -net densenet169 -gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIk_KZj5DMt2",
        "outputId": "6d010220-fb3a-44cc-b8dc-9ad9435c6d38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 9 [40832/50000]\tLoss: 4.5494\tLR: 0.010000\n",
            "Training Epoch: 9 [40960/50000]\tLoss: 4.5617\tLR: 0.010000\n",
            "Training Epoch: 9 [41088/50000]\tLoss: 4.5134\tLR: 0.010000\n",
            "Training Epoch: 9 [41216/50000]\tLoss: 4.5631\tLR: 0.010000\n",
            "Training Epoch: 9 [41344/50000]\tLoss: 4.5890\tLR: 0.010000\n",
            "Training Epoch: 9 [41472/50000]\tLoss: 4.6029\tLR: 0.010000\n",
            "Training Epoch: 9 [41600/50000]\tLoss: 4.5634\tLR: 0.010000\n",
            "Training Epoch: 9 [41728/50000]\tLoss: 4.5646\tLR: 0.010000\n",
            "Training Epoch: 9 [41856/50000]\tLoss: 4.5715\tLR: 0.010000\n",
            "Training Epoch: 9 [41984/50000]\tLoss: 4.5408\tLR: 0.010000\n",
            "Training Epoch: 9 [42112/50000]\tLoss: 4.5558\tLR: 0.010000\n",
            "Training Epoch: 9 [42240/50000]\tLoss: 4.5567\tLR: 0.010000\n",
            "Training Epoch: 9 [42368/50000]\tLoss: 4.5784\tLR: 0.010000\n",
            "Training Epoch: 9 [42496/50000]\tLoss: 4.5843\tLR: 0.010000\n",
            "Training Epoch: 9 [42624/50000]\tLoss: 4.5498\tLR: 0.010000\n",
            "Training Epoch: 9 [42752/50000]\tLoss: 4.5525\tLR: 0.010000\n",
            "Training Epoch: 9 [42880/50000]\tLoss: 4.5917\tLR: 0.010000\n",
            "Training Epoch: 9 [43008/50000]\tLoss: 4.6150\tLR: 0.010000\n",
            "Training Epoch: 9 [43136/50000]\tLoss: 4.5556\tLR: 0.010000\n",
            "Training Epoch: 9 [43264/50000]\tLoss: 4.5935\tLR: 0.010000\n",
            "Training Epoch: 9 [43392/50000]\tLoss: 4.5344\tLR: 0.010000\n",
            "Training Epoch: 9 [43520/50000]\tLoss: 4.5577\tLR: 0.010000\n",
            "Training Epoch: 9 [43648/50000]\tLoss: 4.5652\tLR: 0.010000\n",
            "Training Epoch: 9 [43776/50000]\tLoss: 4.4950\tLR: 0.010000\n",
            "Training Epoch: 9 [43904/50000]\tLoss: 4.5338\tLR: 0.010000\n",
            "Training Epoch: 9 [44032/50000]\tLoss: 4.5493\tLR: 0.010000\n",
            "Training Epoch: 9 [44160/50000]\tLoss: 4.5361\tLR: 0.010000\n",
            "Training Epoch: 9 [44288/50000]\tLoss: 4.5340\tLR: 0.010000\n",
            "Training Epoch: 9 [44416/50000]\tLoss: 4.5851\tLR: 0.010000\n",
            "Training Epoch: 9 [44544/50000]\tLoss: 4.5633\tLR: 0.010000\n",
            "Training Epoch: 9 [44672/50000]\tLoss: 4.5742\tLR: 0.010000\n",
            "Training Epoch: 9 [44800/50000]\tLoss: 4.5520\tLR: 0.010000\n",
            "Training Epoch: 9 [44928/50000]\tLoss: 4.5403\tLR: 0.010000\n",
            "Training Epoch: 9 [45056/50000]\tLoss: 4.5341\tLR: 0.010000\n",
            "Training Epoch: 9 [45184/50000]\tLoss: 4.5309\tLR: 0.010000\n",
            "Training Epoch: 9 [45312/50000]\tLoss: 4.5475\tLR: 0.010000\n",
            "Training Epoch: 9 [45440/50000]\tLoss: 4.5514\tLR: 0.010000\n",
            "Training Epoch: 9 [45568/50000]\tLoss: 4.5533\tLR: 0.010000\n",
            "Training Epoch: 9 [45696/50000]\tLoss: 4.5555\tLR: 0.010000\n",
            "Training Epoch: 9 [45824/50000]\tLoss: 4.5890\tLR: 0.010000\n",
            "Training Epoch: 9 [45952/50000]\tLoss: 4.5810\tLR: 0.010000\n",
            "Training Epoch: 9 [46080/50000]\tLoss: 4.5410\tLR: 0.010000\n",
            "Training Epoch: 9 [46208/50000]\tLoss: 4.6153\tLR: 0.010000\n",
            "Training Epoch: 9 [46336/50000]\tLoss: 4.5732\tLR: 0.010000\n",
            "Training Epoch: 9 [46464/50000]\tLoss: 4.5624\tLR: 0.010000\n",
            "Training Epoch: 9 [46592/50000]\tLoss: 4.5804\tLR: 0.010000\n",
            "Training Epoch: 9 [46720/50000]\tLoss: 4.5517\tLR: 0.010000\n",
            "Training Epoch: 9 [46848/50000]\tLoss: 4.5827\tLR: 0.010000\n",
            "Training Epoch: 9 [46976/50000]\tLoss: 4.5764\tLR: 0.010000\n",
            "Training Epoch: 9 [47104/50000]\tLoss: 4.5884\tLR: 0.010000\n",
            "Training Epoch: 9 [47232/50000]\tLoss: 4.5598\tLR: 0.010000\n",
            "Training Epoch: 9 [47360/50000]\tLoss: 4.5251\tLR: 0.010000\n",
            "Training Epoch: 9 [47488/50000]\tLoss: 4.5506\tLR: 0.010000\n",
            "Training Epoch: 9 [47616/50000]\tLoss: 4.5662\tLR: 0.010000\n",
            "Training Epoch: 9 [47744/50000]\tLoss: 4.5788\tLR: 0.010000\n",
            "Training Epoch: 9 [47872/50000]\tLoss: 4.5597\tLR: 0.010000\n",
            "Training Epoch: 9 [48000/50000]\tLoss: 4.5289\tLR: 0.010000\n",
            "Training Epoch: 9 [48128/50000]\tLoss: 4.5452\tLR: 0.010000\n",
            "Training Epoch: 9 [48256/50000]\tLoss: 4.5891\tLR: 0.010000\n",
            "Training Epoch: 9 [48384/50000]\tLoss: 4.5735\tLR: 0.010000\n",
            "Training Epoch: 9 [48512/50000]\tLoss: 4.5439\tLR: 0.010000\n",
            "Training Epoch: 9 [48640/50000]\tLoss: 4.5529\tLR: 0.010000\n",
            "Training Epoch: 9 [48768/50000]\tLoss: 4.5393\tLR: 0.010000\n",
            "Training Epoch: 9 [48896/50000]\tLoss: 4.5370\tLR: 0.010000\n",
            "Training Epoch: 9 [49024/50000]\tLoss: 4.5456\tLR: 0.010000\n",
            "Training Epoch: 9 [49152/50000]\tLoss: 4.5674\tLR: 0.010000\n",
            "Training Epoch: 9 [49280/50000]\tLoss: 4.5935\tLR: 0.010000\n",
            "Training Epoch: 9 [49408/50000]\tLoss: 4.5187\tLR: 0.010000\n",
            "Training Epoch: 9 [49536/50000]\tLoss: 4.5916\tLR: 0.010000\n",
            "Training Epoch: 9 [49664/50000]\tLoss: 4.5723\tLR: 0.010000\n",
            "Training Epoch: 9 [49792/50000]\tLoss: 4.5409\tLR: 0.010000\n",
            "Training Epoch: 9 [49920/50000]\tLoss: 4.5102\tLR: 0.010000\n",
            "Training Epoch: 9 [50000/50000]\tLoss: 4.5275\tLR: 0.010000\n",
            "epoch 9 training time consumed: 144.81s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB |  95548 GiB |  95548 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB |  94342 GiB |  94342 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1205 GiB |   1205 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB |  95548 GiB |  95548 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB |  94342 GiB |  94342 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1205 GiB |   1205 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB |  95288 GiB |  95287 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB |  94082 GiB |  94082 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1205 GiB |   1205 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB |  78910 GiB |  78910 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB |  77548 GiB |  77548 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   1361 GiB |   1361 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    5822 K  |    5821 K  |\n",
            "|       from large pool |       5    |     146    |    2821 K  |    2821 K  |\n",
            "|       from small pool |     516    |     682    |    3000 K  |    2999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    5822 K  |    5821 K  |\n",
            "|       from large pool |       5    |     146    |    2821 K  |    2821 K  |\n",
            "|       from small pool |     516    |     682    |    3000 K  |    2999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |     117    |    2175 K  |    2175 K  |\n",
            "|       from large pool |       4    |      46    |    1335 K  |    1335 K  |\n",
            "|       from small pool |      73    |      89    |     839 K  |     839 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0360, Accuracy: 0.0170, Time consumed:9.46s\n",
            "\n",
            "Training Epoch: 10 [128/50000]\tLoss: 4.5982\tLR: 0.010000\n",
            "Training Epoch: 10 [256/50000]\tLoss: 4.5851\tLR: 0.010000\n",
            "Training Epoch: 10 [384/50000]\tLoss: 4.5241\tLR: 0.010000\n",
            "Training Epoch: 10 [512/50000]\tLoss: 4.6100\tLR: 0.010000\n",
            "Training Epoch: 10 [640/50000]\tLoss: 4.5597\tLR: 0.010000\n",
            "Training Epoch: 10 [768/50000]\tLoss: 4.5024\tLR: 0.010000\n",
            "Training Epoch: 10 [896/50000]\tLoss: 4.5921\tLR: 0.010000\n",
            "Training Epoch: 10 [1024/50000]\tLoss: 4.5386\tLR: 0.010000\n",
            "Training Epoch: 10 [1152/50000]\tLoss: 4.5621\tLR: 0.010000\n",
            "Training Epoch: 10 [1280/50000]\tLoss: 4.5753\tLR: 0.010000\n",
            "Training Epoch: 10 [1408/50000]\tLoss: 4.5507\tLR: 0.010000\n",
            "Training Epoch: 10 [1536/50000]\tLoss: 4.5265\tLR: 0.010000\n",
            "Training Epoch: 10 [1664/50000]\tLoss: 4.5367\tLR: 0.010000\n",
            "Training Epoch: 10 [1792/50000]\tLoss: 4.5399\tLR: 0.010000\n",
            "Training Epoch: 10 [1920/50000]\tLoss: 4.5770\tLR: 0.010000\n",
            "Training Epoch: 10 [2048/50000]\tLoss: 4.5166\tLR: 0.010000\n",
            "Training Epoch: 10 [2176/50000]\tLoss: 4.5644\tLR: 0.010000\n",
            "Training Epoch: 10 [2304/50000]\tLoss: 4.5658\tLR: 0.010000\n",
            "Training Epoch: 10 [2432/50000]\tLoss: 4.5812\tLR: 0.010000\n",
            "Training Epoch: 10 [2560/50000]\tLoss: 4.5524\tLR: 0.010000\n",
            "Training Epoch: 10 [2688/50000]\tLoss: 4.5505\tLR: 0.010000\n",
            "Training Epoch: 10 [2816/50000]\tLoss: 4.5641\tLR: 0.010000\n",
            "Training Epoch: 10 [2944/50000]\tLoss: 4.5486\tLR: 0.010000\n",
            "Training Epoch: 10 [3072/50000]\tLoss: 4.4922\tLR: 0.010000\n",
            "Training Epoch: 10 [3200/50000]\tLoss: 4.5450\tLR: 0.010000\n",
            "Training Epoch: 10 [3328/50000]\tLoss: 4.5045\tLR: 0.010000\n",
            "Training Epoch: 10 [3456/50000]\tLoss: 4.5461\tLR: 0.010000\n",
            "Training Epoch: 10 [3584/50000]\tLoss: 4.5513\tLR: 0.010000\n",
            "Training Epoch: 10 [3712/50000]\tLoss: 4.5941\tLR: 0.010000\n",
            "Training Epoch: 10 [3840/50000]\tLoss: 4.5492\tLR: 0.010000\n",
            "Training Epoch: 10 [3968/50000]\tLoss: 4.5591\tLR: 0.010000\n",
            "Training Epoch: 10 [4096/50000]\tLoss: 4.5451\tLR: 0.010000\n",
            "Training Epoch: 10 [4224/50000]\tLoss: 4.5814\tLR: 0.010000\n",
            "Training Epoch: 10 [4352/50000]\tLoss: 4.5639\tLR: 0.010000\n",
            "Training Epoch: 10 [4480/50000]\tLoss: 4.5928\tLR: 0.010000\n",
            "Training Epoch: 10 [4608/50000]\tLoss: 4.6023\tLR: 0.010000\n",
            "Training Epoch: 10 [4736/50000]\tLoss: 4.5777\tLR: 0.010000\n",
            "Training Epoch: 10 [4864/50000]\tLoss: 4.5477\tLR: 0.010000\n",
            "Training Epoch: 10 [4992/50000]\tLoss: 4.5862\tLR: 0.010000\n",
            "Training Epoch: 10 [5120/50000]\tLoss: 4.5322\tLR: 0.010000\n",
            "Training Epoch: 10 [5248/50000]\tLoss: 4.5081\tLR: 0.010000\n",
            "Training Epoch: 10 [5376/50000]\tLoss: 4.5505\tLR: 0.010000\n",
            "Training Epoch: 10 [5504/50000]\tLoss: 4.5631\tLR: 0.010000\n",
            "Training Epoch: 10 [5632/50000]\tLoss: 4.5589\tLR: 0.010000\n",
            "Training Epoch: 10 [5760/50000]\tLoss: 4.5029\tLR: 0.010000\n",
            "Training Epoch: 10 [5888/50000]\tLoss: 4.5466\tLR: 0.010000\n",
            "Training Epoch: 10 [6016/50000]\tLoss: 4.5475\tLR: 0.010000\n",
            "Training Epoch: 10 [6144/50000]\tLoss: 4.5214\tLR: 0.010000\n",
            "Training Epoch: 10 [6272/50000]\tLoss: 4.5380\tLR: 0.010000\n",
            "Training Epoch: 10 [6400/50000]\tLoss: 4.6205\tLR: 0.010000\n",
            "Training Epoch: 10 [6528/50000]\tLoss: 4.5608\tLR: 0.010000\n",
            "Training Epoch: 10 [6656/50000]\tLoss: 4.5674\tLR: 0.010000\n",
            "Training Epoch: 10 [6784/50000]\tLoss: 4.4952\tLR: 0.010000\n",
            "Training Epoch: 10 [6912/50000]\tLoss: 4.5489\tLR: 0.010000\n",
            "Training Epoch: 10 [7040/50000]\tLoss: 4.5581\tLR: 0.010000\n",
            "Training Epoch: 10 [7168/50000]\tLoss: 4.5855\tLR: 0.010000\n",
            "Training Epoch: 10 [7296/50000]\tLoss: 4.5592\tLR: 0.010000\n",
            "Training Epoch: 10 [7424/50000]\tLoss: 4.5535\tLR: 0.010000\n",
            "Training Epoch: 10 [7552/50000]\tLoss: 4.5576\tLR: 0.010000\n",
            "Training Epoch: 10 [7680/50000]\tLoss: 4.5440\tLR: 0.010000\n",
            "Training Epoch: 10 [7808/50000]\tLoss: 4.5901\tLR: 0.010000\n",
            "Training Epoch: 10 [7936/50000]\tLoss: 4.5576\tLR: 0.010000\n",
            "Training Epoch: 10 [8064/50000]\tLoss: 4.5743\tLR: 0.010000\n",
            "Training Epoch: 10 [8192/50000]\tLoss: 4.5442\tLR: 0.010000\n",
            "Training Epoch: 10 [8320/50000]\tLoss: 4.5486\tLR: 0.010000\n",
            "Training Epoch: 10 [8448/50000]\tLoss: 4.5259\tLR: 0.010000\n",
            "Training Epoch: 10 [8576/50000]\tLoss: 4.5249\tLR: 0.010000\n",
            "Training Epoch: 10 [8704/50000]\tLoss: 4.5463\tLR: 0.010000\n",
            "Training Epoch: 10 [8832/50000]\tLoss: 4.5613\tLR: 0.010000\n",
            "Training Epoch: 10 [8960/50000]\tLoss: 4.5821\tLR: 0.010000\n",
            "Training Epoch: 10 [9088/50000]\tLoss: 4.5458\tLR: 0.010000\n",
            "Training Epoch: 10 [9216/50000]\tLoss: 4.5340\tLR: 0.010000\n",
            "Training Epoch: 10 [9344/50000]\tLoss: 4.5828\tLR: 0.010000\n",
            "Training Epoch: 10 [9472/50000]\tLoss: 4.5215\tLR: 0.010000\n",
            "Training Epoch: 10 [9600/50000]\tLoss: 4.5105\tLR: 0.010000\n",
            "Training Epoch: 10 [9728/50000]\tLoss: 4.5467\tLR: 0.010000\n",
            "Training Epoch: 10 [9856/50000]\tLoss: 4.5325\tLR: 0.010000\n",
            "Training Epoch: 10 [9984/50000]\tLoss: 4.5511\tLR: 0.010000\n",
            "Training Epoch: 10 [10112/50000]\tLoss: 4.5551\tLR: 0.010000\n",
            "Training Epoch: 10 [10240/50000]\tLoss: 4.5699\tLR: 0.010000\n",
            "Training Epoch: 10 [10368/50000]\tLoss: 4.5794\tLR: 0.010000\n",
            "Training Epoch: 10 [10496/50000]\tLoss: 4.5554\tLR: 0.010000\n",
            "Training Epoch: 10 [10624/50000]\tLoss: 4.5754\tLR: 0.010000\n",
            "Training Epoch: 10 [10752/50000]\tLoss: 4.5400\tLR: 0.010000\n",
            "Training Epoch: 10 [10880/50000]\tLoss: 4.5861\tLR: 0.010000\n",
            "Training Epoch: 10 [11008/50000]\tLoss: 4.5197\tLR: 0.010000\n",
            "Training Epoch: 10 [11136/50000]\tLoss: 4.5472\tLR: 0.010000\n",
            "Training Epoch: 10 [11264/50000]\tLoss: 4.5845\tLR: 0.010000\n",
            "Training Epoch: 10 [11392/50000]\tLoss: 4.5467\tLR: 0.010000\n",
            "Training Epoch: 10 [11520/50000]\tLoss: 4.5413\tLR: 0.010000\n",
            "Training Epoch: 10 [11648/50000]\tLoss: 4.5004\tLR: 0.010000\n",
            "Training Epoch: 10 [11776/50000]\tLoss: 4.5366\tLR: 0.010000\n",
            "Training Epoch: 10 [11904/50000]\tLoss: 4.5498\tLR: 0.010000\n",
            "Training Epoch: 10 [12032/50000]\tLoss: 4.5415\tLR: 0.010000\n",
            "Training Epoch: 10 [12160/50000]\tLoss: 4.5545\tLR: 0.010000\n",
            "Training Epoch: 10 [12288/50000]\tLoss: 4.5600\tLR: 0.010000\n",
            "Training Epoch: 10 [12416/50000]\tLoss: 4.5953\tLR: 0.010000\n",
            "Training Epoch: 10 [12544/50000]\tLoss: 4.5082\tLR: 0.010000\n",
            "Training Epoch: 10 [12672/50000]\tLoss: 4.5403\tLR: 0.010000\n",
            "Training Epoch: 10 [12800/50000]\tLoss: 4.5483\tLR: 0.010000\n",
            "Training Epoch: 10 [12928/50000]\tLoss: 4.5494\tLR: 0.010000\n",
            "Training Epoch: 10 [13056/50000]\tLoss: 4.5498\tLR: 0.010000\n",
            "Training Epoch: 10 [13184/50000]\tLoss: 4.5998\tLR: 0.010000\n",
            "Training Epoch: 10 [13312/50000]\tLoss: 4.5539\tLR: 0.010000\n",
            "Training Epoch: 10 [13440/50000]\tLoss: 4.5552\tLR: 0.010000\n",
            "Training Epoch: 10 [13568/50000]\tLoss: 4.5510\tLR: 0.010000\n",
            "Training Epoch: 10 [13696/50000]\tLoss: 4.5447\tLR: 0.010000\n",
            "Training Epoch: 10 [13824/50000]\tLoss: 4.5316\tLR: 0.010000\n",
            "Training Epoch: 10 [13952/50000]\tLoss: 4.6028\tLR: 0.010000\n",
            "Training Epoch: 10 [14080/50000]\tLoss: 4.5436\tLR: 0.010000\n",
            "Training Epoch: 10 [14208/50000]\tLoss: 4.5467\tLR: 0.010000\n",
            "Training Epoch: 10 [14336/50000]\tLoss: 4.5302\tLR: 0.010000\n",
            "Training Epoch: 10 [14464/50000]\tLoss: 4.5539\tLR: 0.010000\n",
            "Training Epoch: 10 [14592/50000]\tLoss: 4.5482\tLR: 0.010000\n",
            "Training Epoch: 10 [14720/50000]\tLoss: 4.5608\tLR: 0.010000\n",
            "Training Epoch: 10 [14848/50000]\tLoss: 4.5568\tLR: 0.010000\n",
            "Training Epoch: 10 [14976/50000]\tLoss: 4.5709\tLR: 0.010000\n",
            "Training Epoch: 10 [15104/50000]\tLoss: 4.5159\tLR: 0.010000\n",
            "Training Epoch: 10 [15232/50000]\tLoss: 4.5328\tLR: 0.010000\n",
            "Training Epoch: 10 [15360/50000]\tLoss: 4.5316\tLR: 0.010000\n",
            "Training Epoch: 10 [15488/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 10 [15616/50000]\tLoss: 4.5117\tLR: 0.010000\n",
            "Training Epoch: 10 [15744/50000]\tLoss: 4.5247\tLR: 0.010000\n",
            "Training Epoch: 10 [15872/50000]\tLoss: 4.5296\tLR: 0.010000\n",
            "Training Epoch: 10 [16000/50000]\tLoss: 4.5121\tLR: 0.010000\n",
            "Training Epoch: 10 [16128/50000]\tLoss: 4.5072\tLR: 0.010000\n",
            "Training Epoch: 10 [16256/50000]\tLoss: 4.5007\tLR: 0.010000\n",
            "Training Epoch: 10 [16384/50000]\tLoss: 4.5626\tLR: 0.010000\n",
            "Training Epoch: 10 [16512/50000]\tLoss: 4.5226\tLR: 0.010000\n",
            "Training Epoch: 10 [16640/50000]\tLoss: 4.5669\tLR: 0.010000\n",
            "Training Epoch: 10 [16768/50000]\tLoss: 4.5364\tLR: 0.010000\n",
            "Training Epoch: 10 [16896/50000]\tLoss: 4.5710\tLR: 0.010000\n",
            "Training Epoch: 10 [17024/50000]\tLoss: 4.5946\tLR: 0.010000\n",
            "Training Epoch: 10 [17152/50000]\tLoss: 4.5167\tLR: 0.010000\n",
            "Training Epoch: 10 [17280/50000]\tLoss: 4.5411\tLR: 0.010000\n",
            "Training Epoch: 10 [17408/50000]\tLoss: 4.5755\tLR: 0.010000\n",
            "Training Epoch: 10 [17536/50000]\tLoss: 4.5058\tLR: 0.010000\n",
            "Training Epoch: 10 [17664/50000]\tLoss: 4.5482\tLR: 0.010000\n",
            "Training Epoch: 10 [17792/50000]\tLoss: 4.5475\tLR: 0.010000\n",
            "Training Epoch: 10 [17920/50000]\tLoss: 4.5189\tLR: 0.010000\n",
            "Training Epoch: 10 [18048/50000]\tLoss: 4.5312\tLR: 0.010000\n",
            "Training Epoch: 10 [18176/50000]\tLoss: 4.5369\tLR: 0.010000\n",
            "Training Epoch: 10 [18304/50000]\tLoss: 4.5197\tLR: 0.010000\n",
            "Training Epoch: 10 [18432/50000]\tLoss: 4.5382\tLR: 0.010000\n",
            "Training Epoch: 10 [18560/50000]\tLoss: 4.5764\tLR: 0.010000\n",
            "Training Epoch: 10 [18688/50000]\tLoss: 4.5268\tLR: 0.010000\n",
            "Training Epoch: 10 [18816/50000]\tLoss: 4.5003\tLR: 0.010000\n",
            "Training Epoch: 10 [18944/50000]\tLoss: 4.5660\tLR: 0.010000\n",
            "Training Epoch: 10 [19072/50000]\tLoss: 4.5583\tLR: 0.010000\n",
            "Training Epoch: 10 [19200/50000]\tLoss: 4.5511\tLR: 0.010000\n",
            "Training Epoch: 10 [19328/50000]\tLoss: 4.5195\tLR: 0.010000\n",
            "Training Epoch: 10 [19456/50000]\tLoss: 4.5364\tLR: 0.010000\n",
            "Training Epoch: 10 [19584/50000]\tLoss: 4.5519\tLR: 0.010000\n",
            "Training Epoch: 10 [19712/50000]\tLoss: 4.5421\tLR: 0.010000\n",
            "Training Epoch: 10 [19840/50000]\tLoss: 4.5367\tLR: 0.010000\n",
            "Training Epoch: 10 [19968/50000]\tLoss: 4.5634\tLR: 0.010000\n",
            "Training Epoch: 10 [20096/50000]\tLoss: 4.5257\tLR: 0.010000\n",
            "Training Epoch: 10 [20224/50000]\tLoss: 4.5805\tLR: 0.010000\n",
            "Training Epoch: 10 [20352/50000]\tLoss: 4.5558\tLR: 0.010000\n",
            "Training Epoch: 10 [20480/50000]\tLoss: 4.5000\tLR: 0.010000\n",
            "Training Epoch: 10 [20608/50000]\tLoss: 4.5603\tLR: 0.010000\n",
            "Training Epoch: 10 [20736/50000]\tLoss: 4.5378\tLR: 0.010000\n",
            "Training Epoch: 10 [20864/50000]\tLoss: 4.5041\tLR: 0.010000\n",
            "Training Epoch: 10 [20992/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 10 [21120/50000]\tLoss: 4.5859\tLR: 0.010000\n",
            "Training Epoch: 10 [21248/50000]\tLoss: 4.5612\tLR: 0.010000\n",
            "Training Epoch: 10 [21376/50000]\tLoss: 4.5495\tLR: 0.010000\n",
            "Training Epoch: 10 [21504/50000]\tLoss: 4.5600\tLR: 0.010000\n",
            "Training Epoch: 10 [21632/50000]\tLoss: 4.5815\tLR: 0.010000\n",
            "Training Epoch: 10 [21760/50000]\tLoss: 4.5514\tLR: 0.010000\n",
            "Training Epoch: 10 [21888/50000]\tLoss: 4.4745\tLR: 0.010000\n",
            "Training Epoch: 10 [22016/50000]\tLoss: 4.5111\tLR: 0.010000\n",
            "Training Epoch: 10 [22144/50000]\tLoss: 4.5208\tLR: 0.010000\n",
            "Training Epoch: 10 [22272/50000]\tLoss: 4.5199\tLR: 0.010000\n",
            "Training Epoch: 10 [22400/50000]\tLoss: 4.5831\tLR: 0.010000\n",
            "Training Epoch: 10 [22528/50000]\tLoss: 4.5336\tLR: 0.010000\n",
            "Training Epoch: 10 [22656/50000]\tLoss: 4.5829\tLR: 0.010000\n",
            "Training Epoch: 10 [22784/50000]\tLoss: 4.5408\tLR: 0.010000\n",
            "Training Epoch: 10 [22912/50000]\tLoss: 4.5394\tLR: 0.010000\n",
            "Training Epoch: 10 [23040/50000]\tLoss: 4.5200\tLR: 0.010000\n",
            "Training Epoch: 10 [23168/50000]\tLoss: 4.5183\tLR: 0.010000\n",
            "Training Epoch: 10 [23296/50000]\tLoss: 4.5872\tLR: 0.010000\n",
            "Training Epoch: 10 [23424/50000]\tLoss: 4.4948\tLR: 0.010000\n",
            "Training Epoch: 10 [23552/50000]\tLoss: 4.5381\tLR: 0.010000\n",
            "Training Epoch: 10 [23680/50000]\tLoss: 4.5189\tLR: 0.010000\n",
            "Training Epoch: 10 [23808/50000]\tLoss: 4.5093\tLR: 0.010000\n",
            "Training Epoch: 10 [23936/50000]\tLoss: 4.5209\tLR: 0.010000\n",
            "Training Epoch: 10 [24064/50000]\tLoss: 4.5576\tLR: 0.010000\n",
            "Training Epoch: 10 [24192/50000]\tLoss: 4.5507\tLR: 0.010000\n",
            "Training Epoch: 10 [24320/50000]\tLoss: 4.5272\tLR: 0.010000\n",
            "Training Epoch: 10 [24448/50000]\tLoss: 4.5480\tLR: 0.010000\n",
            "Training Epoch: 10 [24576/50000]\tLoss: 4.4293\tLR: 0.010000\n",
            "Training Epoch: 10 [24704/50000]\tLoss: 4.5331\tLR: 0.010000\n",
            "Training Epoch: 10 [24832/50000]\tLoss: 4.5091\tLR: 0.010000\n",
            "Training Epoch: 10 [24960/50000]\tLoss: 4.5592\tLR: 0.010000\n",
            "Training Epoch: 10 [25088/50000]\tLoss: 4.5925\tLR: 0.010000\n",
            "Training Epoch: 10 [25216/50000]\tLoss: 4.5162\tLR: 0.010000\n",
            "Training Epoch: 10 [25344/50000]\tLoss: 4.5544\tLR: 0.010000\n",
            "Training Epoch: 10 [25472/50000]\tLoss: 4.4943\tLR: 0.010000\n",
            "Training Epoch: 10 [25600/50000]\tLoss: 4.5205\tLR: 0.010000\n",
            "Training Epoch: 10 [25728/50000]\tLoss: 4.5575\tLR: 0.010000\n",
            "Training Epoch: 10 [25856/50000]\tLoss: 4.5639\tLR: 0.010000\n",
            "Training Epoch: 10 [25984/50000]\tLoss: 4.5930\tLR: 0.010000\n",
            "Training Epoch: 10 [26112/50000]\tLoss: 4.5646\tLR: 0.010000\n",
            "Training Epoch: 10 [26240/50000]\tLoss: 4.5059\tLR: 0.010000\n",
            "Training Epoch: 10 [26368/50000]\tLoss: 4.5533\tLR: 0.010000\n",
            "Training Epoch: 10 [26496/50000]\tLoss: 4.5491\tLR: 0.010000\n",
            "Training Epoch: 10 [26624/50000]\tLoss: 4.5071\tLR: 0.010000\n",
            "Training Epoch: 10 [26752/50000]\tLoss: 4.5297\tLR: 0.010000\n",
            "Training Epoch: 10 [26880/50000]\tLoss: 4.5775\tLR: 0.010000\n",
            "Training Epoch: 10 [27008/50000]\tLoss: 4.5486\tLR: 0.010000\n",
            "Training Epoch: 10 [27136/50000]\tLoss: 4.5306\tLR: 0.010000\n",
            "Training Epoch: 10 [27264/50000]\tLoss: 4.5548\tLR: 0.010000\n",
            "Training Epoch: 10 [27392/50000]\tLoss: 4.5286\tLR: 0.010000\n",
            "Training Epoch: 10 [27520/50000]\tLoss: 4.4986\tLR: 0.010000\n",
            "Training Epoch: 10 [27648/50000]\tLoss: 4.5572\tLR: 0.010000\n",
            "Training Epoch: 10 [27776/50000]\tLoss: 4.5487\tLR: 0.010000\n",
            "Training Epoch: 10 [27904/50000]\tLoss: 4.5453\tLR: 0.010000\n",
            "Training Epoch: 10 [28032/50000]\tLoss: 4.5448\tLR: 0.010000\n",
            "Training Epoch: 10 [28160/50000]\tLoss: 4.5327\tLR: 0.010000\n",
            "Training Epoch: 10 [28288/50000]\tLoss: 4.4973\tLR: 0.010000\n",
            "Training Epoch: 10 [28416/50000]\tLoss: 4.5297\tLR: 0.010000\n",
            "Training Epoch: 10 [28544/50000]\tLoss: 4.5101\tLR: 0.010000\n",
            "Training Epoch: 10 [28672/50000]\tLoss: 4.5084\tLR: 0.010000\n",
            "Training Epoch: 10 [28800/50000]\tLoss: 4.4927\tLR: 0.010000\n",
            "Training Epoch: 10 [28928/50000]\tLoss: 4.5264\tLR: 0.010000\n",
            "Training Epoch: 10 [29056/50000]\tLoss: 4.4791\tLR: 0.010000\n",
            "Training Epoch: 10 [29184/50000]\tLoss: 4.5264\tLR: 0.010000\n",
            "Training Epoch: 10 [29312/50000]\tLoss: 4.5184\tLR: 0.010000\n",
            "Training Epoch: 10 [29440/50000]\tLoss: 4.5449\tLR: 0.010000\n",
            "Training Epoch: 10 [29568/50000]\tLoss: 4.5760\tLR: 0.010000\n",
            "Training Epoch: 10 [29696/50000]\tLoss: 4.5349\tLR: 0.010000\n",
            "Training Epoch: 10 [29824/50000]\tLoss: 4.5198\tLR: 0.010000\n",
            "Training Epoch: 10 [29952/50000]\tLoss: 4.5268\tLR: 0.010000\n",
            "Training Epoch: 10 [30080/50000]\tLoss: 4.5653\tLR: 0.010000\n",
            "Training Epoch: 10 [30208/50000]\tLoss: 4.5128\tLR: 0.010000\n",
            "Training Epoch: 10 [30336/50000]\tLoss: 4.5536\tLR: 0.010000\n",
            "Training Epoch: 10 [30464/50000]\tLoss: 4.5623\tLR: 0.010000\n",
            "Training Epoch: 10 [30592/50000]\tLoss: 4.5269\tLR: 0.010000\n",
            "Training Epoch: 10 [30720/50000]\tLoss: 4.4947\tLR: 0.010000\n",
            "Training Epoch: 10 [30848/50000]\tLoss: 4.5282\tLR: 0.010000\n",
            "Training Epoch: 10 [30976/50000]\tLoss: 4.5420\tLR: 0.010000\n",
            "Training Epoch: 10 [31104/50000]\tLoss: 4.5162\tLR: 0.010000\n",
            "Training Epoch: 10 [31232/50000]\tLoss: 4.5497\tLR: 0.010000\n",
            "Training Epoch: 10 [31360/50000]\tLoss: 4.5357\tLR: 0.010000\n",
            "Training Epoch: 10 [31488/50000]\tLoss: 4.5680\tLR: 0.010000\n",
            "Training Epoch: 10 [31616/50000]\tLoss: 4.5306\tLR: 0.010000\n",
            "Training Epoch: 10 [31744/50000]\tLoss: 4.5592\tLR: 0.010000\n",
            "Training Epoch: 10 [31872/50000]\tLoss: 4.5548\tLR: 0.010000\n",
            "Training Epoch: 10 [32000/50000]\tLoss: 4.5507\tLR: 0.010000\n",
            "Training Epoch: 10 [32128/50000]\tLoss: 4.4338\tLR: 0.010000\n",
            "Training Epoch: 10 [32256/50000]\tLoss: 4.5018\tLR: 0.010000\n",
            "Training Epoch: 10 [32384/50000]\tLoss: 4.5860\tLR: 0.010000\n",
            "Training Epoch: 10 [32512/50000]\tLoss: 4.5023\tLR: 0.010000\n",
            "Training Epoch: 10 [32640/50000]\tLoss: 4.5109\tLR: 0.010000\n",
            "Training Epoch: 10 [32768/50000]\tLoss: 4.5572\tLR: 0.010000\n",
            "Training Epoch: 10 [32896/50000]\tLoss: 4.5377\tLR: 0.010000\n",
            "Training Epoch: 10 [33024/50000]\tLoss: 4.5642\tLR: 0.010000\n",
            "Training Epoch: 10 [33152/50000]\tLoss: 4.5161\tLR: 0.010000\n",
            "Training Epoch: 10 [33280/50000]\tLoss: 4.4732\tLR: 0.010000\n",
            "Training Epoch: 10 [33408/50000]\tLoss: 4.5833\tLR: 0.010000\n",
            "Training Epoch: 10 [33536/50000]\tLoss: 4.5340\tLR: 0.010000\n",
            "Training Epoch: 10 [33664/50000]\tLoss: 4.5615\tLR: 0.010000\n",
            "Training Epoch: 10 [33792/50000]\tLoss: 4.5473\tLR: 0.010000\n",
            "Training Epoch: 10 [33920/50000]\tLoss: 4.5516\tLR: 0.010000\n",
            "Training Epoch: 10 [34048/50000]\tLoss: 4.5706\tLR: 0.010000\n",
            "Training Epoch: 10 [34176/50000]\tLoss: 4.5270\tLR: 0.010000\n",
            "Training Epoch: 10 [34304/50000]\tLoss: 4.5144\tLR: 0.010000\n",
            "Training Epoch: 10 [34432/50000]\tLoss: 4.5518\tLR: 0.010000\n",
            "Training Epoch: 10 [34560/50000]\tLoss: 4.5633\tLR: 0.010000\n",
            "Training Epoch: 10 [34688/50000]\tLoss: 4.5374\tLR: 0.010000\n",
            "Training Epoch: 10 [34816/50000]\tLoss: 4.5208\tLR: 0.010000\n",
            "Training Epoch: 10 [34944/50000]\tLoss: 4.5270\tLR: 0.010000\n",
            "Training Epoch: 10 [35072/50000]\tLoss: 4.5214\tLR: 0.010000\n",
            "Training Epoch: 10 [35200/50000]\tLoss: 4.5317\tLR: 0.010000\n",
            "Training Epoch: 10 [35328/50000]\tLoss: 4.5274\tLR: 0.010000\n",
            "Training Epoch: 10 [35456/50000]\tLoss: 4.5102\tLR: 0.010000\n",
            "Training Epoch: 10 [35584/50000]\tLoss: 4.5588\tLR: 0.010000\n",
            "Training Epoch: 10 [35712/50000]\tLoss: 4.5123\tLR: 0.010000\n",
            "Training Epoch: 10 [35840/50000]\tLoss: 4.5203\tLR: 0.010000\n",
            "Training Epoch: 10 [35968/50000]\tLoss: 4.5330\tLR: 0.010000\n",
            "Training Epoch: 10 [36096/50000]\tLoss: 4.5410\tLR: 0.010000\n",
            "Training Epoch: 10 [36224/50000]\tLoss: 4.5097\tLR: 0.010000\n",
            "Training Epoch: 10 [36352/50000]\tLoss: 4.5514\tLR: 0.010000\n",
            "Training Epoch: 10 [36480/50000]\tLoss: 4.5095\tLR: 0.010000\n",
            "Training Epoch: 10 [36608/50000]\tLoss: 4.5035\tLR: 0.010000\n",
            "Training Epoch: 10 [36736/50000]\tLoss: 4.5167\tLR: 0.010000\n",
            "Training Epoch: 10 [36864/50000]\tLoss: 4.4828\tLR: 0.010000\n",
            "Training Epoch: 10 [36992/50000]\tLoss: 4.5397\tLR: 0.010000\n",
            "Training Epoch: 10 [37120/50000]\tLoss: 4.5703\tLR: 0.010000\n",
            "Training Epoch: 10 [37248/50000]\tLoss: 4.5159\tLR: 0.010000\n",
            "Training Epoch: 10 [37376/50000]\tLoss: 4.5349\tLR: 0.010000\n",
            "Training Epoch: 10 [37504/50000]\tLoss: 4.5843\tLR: 0.010000\n",
            "Training Epoch: 10 [37632/50000]\tLoss: 4.5084\tLR: 0.010000\n",
            "Training Epoch: 10 [37760/50000]\tLoss: 4.5594\tLR: 0.010000\n",
            "Training Epoch: 10 [37888/50000]\tLoss: 4.4987\tLR: 0.010000\n",
            "Training Epoch: 10 [38016/50000]\tLoss: 4.5380\tLR: 0.010000\n",
            "Training Epoch: 10 [38144/50000]\tLoss: 4.4969\tLR: 0.010000\n",
            "Training Epoch: 10 [38272/50000]\tLoss: 4.5228\tLR: 0.010000\n",
            "Training Epoch: 10 [38400/50000]\tLoss: 4.5167\tLR: 0.010000\n",
            "Training Epoch: 10 [38528/50000]\tLoss: 4.5275\tLR: 0.010000\n",
            "Training Epoch: 10 [38656/50000]\tLoss: 4.5262\tLR: 0.010000\n",
            "Training Epoch: 10 [38784/50000]\tLoss: 4.4632\tLR: 0.010000\n",
            "Training Epoch: 10 [38912/50000]\tLoss: 4.4839\tLR: 0.010000\n",
            "Training Epoch: 10 [39040/50000]\tLoss: 4.5160\tLR: 0.010000\n",
            "Training Epoch: 10 [39168/50000]\tLoss: 4.5301\tLR: 0.010000\n",
            "Training Epoch: 10 [39296/50000]\tLoss: 4.4827\tLR: 0.010000\n",
            "Training Epoch: 10 [39424/50000]\tLoss: 4.5423\tLR: 0.010000\n",
            "Training Epoch: 10 [39552/50000]\tLoss: 4.4417\tLR: 0.010000\n",
            "Training Epoch: 10 [39680/50000]\tLoss: 4.4993\tLR: 0.010000\n",
            "Training Epoch: 10 [39808/50000]\tLoss: 4.5770\tLR: 0.010000\n",
            "Training Epoch: 10 [39936/50000]\tLoss: 4.5553\tLR: 0.010000\n",
            "Training Epoch: 10 [40064/50000]\tLoss: 4.5704\tLR: 0.010000\n",
            "Training Epoch: 10 [40192/50000]\tLoss: 4.5254\tLR: 0.010000\n",
            "Training Epoch: 10 [40320/50000]\tLoss: 4.5302\tLR: 0.010000\n",
            "Training Epoch: 10 [40448/50000]\tLoss: 4.5534\tLR: 0.010000\n",
            "Training Epoch: 10 [40576/50000]\tLoss: 4.5104\tLR: 0.010000\n",
            "Training Epoch: 10 [40704/50000]\tLoss: 4.4768\tLR: 0.010000\n",
            "Training Epoch: 10 [40832/50000]\tLoss: 4.5498\tLR: 0.010000\n",
            "Training Epoch: 10 [40960/50000]\tLoss: 4.5549\tLR: 0.010000\n",
            "Training Epoch: 10 [41088/50000]\tLoss: 4.5628\tLR: 0.010000\n",
            "Training Epoch: 10 [41216/50000]\tLoss: 4.5178\tLR: 0.010000\n",
            "Training Epoch: 10 [41344/50000]\tLoss: 4.5769\tLR: 0.010000\n",
            "Training Epoch: 10 [41472/50000]\tLoss: 4.5046\tLR: 0.010000\n",
            "Training Epoch: 10 [41600/50000]\tLoss: 4.5415\tLR: 0.010000\n",
            "Training Epoch: 10 [41728/50000]\tLoss: 4.5721\tLR: 0.010000\n",
            "Training Epoch: 10 [41856/50000]\tLoss: 4.5525\tLR: 0.010000\n",
            "Training Epoch: 10 [41984/50000]\tLoss: 4.5380\tLR: 0.010000\n",
            "Training Epoch: 10 [42112/50000]\tLoss: 4.5280\tLR: 0.010000\n",
            "Training Epoch: 10 [42240/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 10 [42368/50000]\tLoss: 4.5152\tLR: 0.010000\n",
            "Training Epoch: 10 [42496/50000]\tLoss: 4.5387\tLR: 0.010000\n",
            "Training Epoch: 10 [42624/50000]\tLoss: 4.5386\tLR: 0.010000\n",
            "Training Epoch: 10 [42752/50000]\tLoss: 4.5007\tLR: 0.010000\n",
            "Training Epoch: 10 [42880/50000]\tLoss: 4.5476\tLR: 0.010000\n",
            "Training Epoch: 10 [43008/50000]\tLoss: 4.5358\tLR: 0.010000\n",
            "Training Epoch: 10 [43136/50000]\tLoss: 4.4900\tLR: 0.010000\n",
            "Training Epoch: 10 [43264/50000]\tLoss: 4.5100\tLR: 0.010000\n",
            "Training Epoch: 10 [43392/50000]\tLoss: 4.5037\tLR: 0.010000\n",
            "Training Epoch: 10 [43520/50000]\tLoss: 4.4813\tLR: 0.010000\n",
            "Training Epoch: 10 [43648/50000]\tLoss: 4.5385\tLR: 0.010000\n",
            "Training Epoch: 10 [43776/50000]\tLoss: 4.5631\tLR: 0.010000\n",
            "Training Epoch: 10 [43904/50000]\tLoss: 4.4589\tLR: 0.010000\n",
            "Training Epoch: 10 [44032/50000]\tLoss: 4.4926\tLR: 0.010000\n",
            "Training Epoch: 10 [44160/50000]\tLoss: 4.5231\tLR: 0.010000\n",
            "Training Epoch: 10 [44288/50000]\tLoss: 4.5018\tLR: 0.010000\n",
            "Training Epoch: 10 [44416/50000]\tLoss: 4.5339\tLR: 0.010000\n",
            "Training Epoch: 10 [44544/50000]\tLoss: 4.5460\tLR: 0.010000\n",
            "Training Epoch: 10 [44672/50000]\tLoss: 4.5228\tLR: 0.010000\n",
            "Training Epoch: 10 [44800/50000]\tLoss: 4.5478\tLR: 0.010000\n",
            "Training Epoch: 10 [44928/50000]\tLoss: 4.5162\tLR: 0.010000\n",
            "Training Epoch: 10 [45056/50000]\tLoss: 4.5038\tLR: 0.010000\n",
            "Training Epoch: 10 [45184/50000]\tLoss: 4.5490\tLR: 0.010000\n",
            "Training Epoch: 10 [45312/50000]\tLoss: 4.5182\tLR: 0.010000\n",
            "Training Epoch: 10 [45440/50000]\tLoss: 4.5623\tLR: 0.010000\n",
            "Training Epoch: 10 [45568/50000]\tLoss: 4.5353\tLR: 0.010000\n",
            "Training Epoch: 10 [45696/50000]\tLoss: 4.4936\tLR: 0.010000\n",
            "Training Epoch: 10 [45824/50000]\tLoss: 4.5277\tLR: 0.010000\n",
            "Training Epoch: 10 [45952/50000]\tLoss: 4.4554\tLR: 0.010000\n",
            "Training Epoch: 10 [46080/50000]\tLoss: 4.5375\tLR: 0.010000\n",
            "Training Epoch: 10 [46208/50000]\tLoss: 4.5362\tLR: 0.010000\n",
            "Training Epoch: 10 [46336/50000]\tLoss: 4.4752\tLR: 0.010000\n",
            "Training Epoch: 10 [46464/50000]\tLoss: 4.5100\tLR: 0.010000\n",
            "Training Epoch: 10 [46592/50000]\tLoss: 4.5354\tLR: 0.010000\n",
            "Training Epoch: 10 [46720/50000]\tLoss: 4.5226\tLR: 0.010000\n",
            "Training Epoch: 10 [46848/50000]\tLoss: 4.5214\tLR: 0.010000\n",
            "Training Epoch: 10 [46976/50000]\tLoss: 4.5429\tLR: 0.010000\n",
            "Training Epoch: 10 [47104/50000]\tLoss: 4.5377\tLR: 0.010000\n",
            "Training Epoch: 10 [47232/50000]\tLoss: 4.4792\tLR: 0.010000\n",
            "Training Epoch: 10 [47360/50000]\tLoss: 4.4505\tLR: 0.010000\n",
            "Training Epoch: 10 [47488/50000]\tLoss: 4.5380\tLR: 0.010000\n",
            "Training Epoch: 10 [47616/50000]\tLoss: 4.5350\tLR: 0.010000\n",
            "Training Epoch: 10 [47744/50000]\tLoss: 4.5123\tLR: 0.010000\n",
            "Training Epoch: 10 [47872/50000]\tLoss: 4.5558\tLR: 0.010000\n",
            "Training Epoch: 10 [48000/50000]\tLoss: 4.5470\tLR: 0.010000\n",
            "Training Epoch: 10 [48128/50000]\tLoss: 4.4611\tLR: 0.010000\n",
            "Training Epoch: 10 [48256/50000]\tLoss: 4.5197\tLR: 0.010000\n",
            "Training Epoch: 10 [48384/50000]\tLoss: 4.5635\tLR: 0.010000\n",
            "Training Epoch: 10 [48512/50000]\tLoss: 4.5221\tLR: 0.010000\n",
            "Training Epoch: 10 [48640/50000]\tLoss: 4.5524\tLR: 0.010000\n",
            "Training Epoch: 10 [48768/50000]\tLoss: 4.5720\tLR: 0.010000\n",
            "Training Epoch: 10 [48896/50000]\tLoss: 4.4878\tLR: 0.010000\n",
            "Training Epoch: 10 [49024/50000]\tLoss: 4.4979\tLR: 0.010000\n",
            "Training Epoch: 10 [49152/50000]\tLoss: 4.4757\tLR: 0.010000\n",
            "Training Epoch: 10 [49280/50000]\tLoss: 4.5328\tLR: 0.010000\n",
            "Training Epoch: 10 [49408/50000]\tLoss: 4.5836\tLR: 0.010000\n",
            "Training Epoch: 10 [49536/50000]\tLoss: 4.5180\tLR: 0.010000\n",
            "Training Epoch: 10 [49664/50000]\tLoss: 4.5159\tLR: 0.010000\n",
            "Training Epoch: 10 [49792/50000]\tLoss: 4.5378\tLR: 0.010000\n",
            "Training Epoch: 10 [49920/50000]\tLoss: 4.4864\tLR: 0.010000\n",
            "Training Epoch: 10 [50000/50000]\tLoss: 4.5462\tLR: 0.010000\n",
            "epoch 10 training time consumed: 144.67s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 106161 GiB | 106161 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 104821 GiB | 104821 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1339 GiB |   1339 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 106161 GiB | 106161 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 104821 GiB | 104821 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1339 GiB |   1339 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 105871 GiB | 105871 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 104532 GiB | 104532 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1339 GiB |   1339 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB |  87865 GiB |  87865 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB |  86352 GiB |  86352 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   1513 GiB |   1513 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    6468 K  |    6468 K  |\n",
            "|       from large pool |       5    |     146    |    3135 K  |    3135 K  |\n",
            "|       from small pool |     516    |     682    |    3333 K  |    3333 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    6468 K  |    6468 K  |\n",
            "|       from large pool |       5    |     146    |    3135 K  |    3135 K  |\n",
            "|       from small pool |     516    |     682    |    3333 K  |    3333 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |     117    |    2414 K  |    2414 K  |\n",
            "|       from large pool |       4    |      46    |    1482 K  |    1482 K  |\n",
            "|       from small pool |      75    |      89    |     932 K  |     932 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0358, Accuracy: 0.0185, Time consumed:9.51s\n",
            "\n",
            "saving weights file to checkpoint/densenet169/Tuesday_25_July_2023_08h_03m_33s/densenet169-10-regular.pth\n",
            "Training Epoch: 11 [128/50000]\tLoss: 4.5277\tLR: 0.010000\n",
            "Training Epoch: 11 [256/50000]\tLoss: 4.5225\tLR: 0.010000\n",
            "Training Epoch: 11 [384/50000]\tLoss: 4.5274\tLR: 0.010000\n",
            "Training Epoch: 11 [512/50000]\tLoss: 4.4865\tLR: 0.010000\n",
            "Training Epoch: 11 [640/50000]\tLoss: 4.5285\tLR: 0.010000\n",
            "Training Epoch: 11 [768/50000]\tLoss: 4.5155\tLR: 0.010000\n",
            "Training Epoch: 11 [896/50000]\tLoss: 4.5228\tLR: 0.010000\n",
            "Training Epoch: 11 [1024/50000]\tLoss: 4.5250\tLR: 0.010000\n",
            "Training Epoch: 11 [1152/50000]\tLoss: 4.4637\tLR: 0.010000\n",
            "Training Epoch: 11 [1280/50000]\tLoss: 4.5357\tLR: 0.010000\n",
            "Training Epoch: 11 [1408/50000]\tLoss: 4.5291\tLR: 0.010000\n",
            "Training Epoch: 11 [1536/50000]\tLoss: 4.5224\tLR: 0.010000\n",
            "Training Epoch: 11 [1664/50000]\tLoss: 4.5310\tLR: 0.010000\n",
            "Training Epoch: 11 [1792/50000]\tLoss: 4.5138\tLR: 0.010000\n",
            "Training Epoch: 11 [1920/50000]\tLoss: 4.5613\tLR: 0.010000\n",
            "Training Epoch: 11 [2048/50000]\tLoss: 4.5244\tLR: 0.010000\n",
            "Training Epoch: 11 [2176/50000]\tLoss: 4.4948\tLR: 0.010000\n",
            "Training Epoch: 11 [2304/50000]\tLoss: 4.5640\tLR: 0.010000\n",
            "Training Epoch: 11 [2432/50000]\tLoss: 4.5033\tLR: 0.010000\n",
            "Training Epoch: 11 [2560/50000]\tLoss: 4.5696\tLR: 0.010000\n",
            "Training Epoch: 11 [2688/50000]\tLoss: 4.5676\tLR: 0.010000\n",
            "Training Epoch: 11 [2816/50000]\tLoss: 4.5436\tLR: 0.010000\n",
            "Training Epoch: 11 [2944/50000]\tLoss: 4.4278\tLR: 0.010000\n",
            "Training Epoch: 11 [3072/50000]\tLoss: 4.5243\tLR: 0.010000\n",
            "Training Epoch: 11 [3200/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 11 [3328/50000]\tLoss: 4.4703\tLR: 0.010000\n",
            "Training Epoch: 11 [3456/50000]\tLoss: 4.5094\tLR: 0.010000\n",
            "Training Epoch: 11 [3584/50000]\tLoss: 4.5512\tLR: 0.010000\n",
            "Training Epoch: 11 [3712/50000]\tLoss: 4.5168\tLR: 0.010000\n",
            "Training Epoch: 11 [3840/50000]\tLoss: 4.4854\tLR: 0.010000\n",
            "Training Epoch: 11 [3968/50000]\tLoss: 4.5570\tLR: 0.010000\n",
            "Training Epoch: 11 [4096/50000]\tLoss: 4.5181\tLR: 0.010000\n",
            "Training Epoch: 11 [4224/50000]\tLoss: 4.4759\tLR: 0.010000\n",
            "Training Epoch: 11 [4352/50000]\tLoss: 4.4794\tLR: 0.010000\n",
            "Training Epoch: 11 [4480/50000]\tLoss: 4.4452\tLR: 0.010000\n",
            "Training Epoch: 11 [4608/50000]\tLoss: 4.5127\tLR: 0.010000\n",
            "Training Epoch: 11 [4736/50000]\tLoss: 4.5287\tLR: 0.010000\n",
            "Training Epoch: 11 [4864/50000]\tLoss: 4.4834\tLR: 0.010000\n",
            "Training Epoch: 11 [4992/50000]\tLoss: 4.5126\tLR: 0.010000\n",
            "Training Epoch: 11 [5120/50000]\tLoss: 4.4397\tLR: 0.010000\n",
            "Training Epoch: 11 [5248/50000]\tLoss: 4.4107\tLR: 0.010000\n",
            "Training Epoch: 11 [5376/50000]\tLoss: 4.5439\tLR: 0.010000\n",
            "Training Epoch: 11 [5504/50000]\tLoss: 4.4772\tLR: 0.010000\n",
            "Training Epoch: 11 [5632/50000]\tLoss: 4.5549\tLR: 0.010000\n",
            "Training Epoch: 11 [5760/50000]\tLoss: 4.5124\tLR: 0.010000\n",
            "Training Epoch: 11 [5888/50000]\tLoss: 4.5904\tLR: 0.010000\n",
            "Training Epoch: 11 [6016/50000]\tLoss: 4.5448\tLR: 0.010000\n",
            "Training Epoch: 11 [6144/50000]\tLoss: 4.5150\tLR: 0.010000\n",
            "Training Epoch: 11 [6272/50000]\tLoss: 4.5162\tLR: 0.010000\n",
            "Training Epoch: 11 [6400/50000]\tLoss: 4.5190\tLR: 0.010000\n",
            "Training Epoch: 11 [6528/50000]\tLoss: 4.5028\tLR: 0.010000\n",
            "Training Epoch: 11 [6656/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 11 [6784/50000]\tLoss: 4.5708\tLR: 0.010000\n",
            "Training Epoch: 11 [6912/50000]\tLoss: 4.5401\tLR: 0.010000\n",
            "Training Epoch: 11 [7040/50000]\tLoss: 4.4769\tLR: 0.010000\n",
            "Training Epoch: 11 [7168/50000]\tLoss: 4.5431\tLR: 0.010000\n",
            "Training Epoch: 11 [7296/50000]\tLoss: 4.5738\tLR: 0.010000\n",
            "Training Epoch: 11 [7424/50000]\tLoss: 4.4753\tLR: 0.010000\n",
            "Training Epoch: 11 [7552/50000]\tLoss: 4.4747\tLR: 0.010000\n",
            "Training Epoch: 11 [7680/50000]\tLoss: 4.5557\tLR: 0.010000\n",
            "Training Epoch: 11 [7808/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 11 [7936/50000]\tLoss: 4.4969\tLR: 0.010000\n",
            "Training Epoch: 11 [8064/50000]\tLoss: 4.4999\tLR: 0.010000\n",
            "Training Epoch: 11 [8192/50000]\tLoss: 4.5412\tLR: 0.010000\n",
            "Training Epoch: 11 [8320/50000]\tLoss: 4.5131\tLR: 0.010000\n",
            "Training Epoch: 11 [8448/50000]\tLoss: 4.5733\tLR: 0.010000\n",
            "Training Epoch: 11 [8576/50000]\tLoss: 4.5743\tLR: 0.010000\n",
            "Training Epoch: 11 [8704/50000]\tLoss: 4.5167\tLR: 0.010000\n",
            "Training Epoch: 11 [8832/50000]\tLoss: 4.5600\tLR: 0.010000\n",
            "Training Epoch: 11 [8960/50000]\tLoss: 4.5040\tLR: 0.010000\n",
            "Training Epoch: 11 [9088/50000]\tLoss: 4.5200\tLR: 0.010000\n",
            "Training Epoch: 11 [9216/50000]\tLoss: 4.5210\tLR: 0.010000\n",
            "Training Epoch: 11 [9344/50000]\tLoss: 4.5254\tLR: 0.010000\n",
            "Training Epoch: 11 [9472/50000]\tLoss: 4.5054\tLR: 0.010000\n",
            "Training Epoch: 11 [9600/50000]\tLoss: 4.5348\tLR: 0.010000\n",
            "Training Epoch: 11 [9728/50000]\tLoss: 4.5290\tLR: 0.010000\n",
            "Training Epoch: 11 [9856/50000]\tLoss: 4.5780\tLR: 0.010000\n",
            "Training Epoch: 11 [9984/50000]\tLoss: 4.5180\tLR: 0.010000\n",
            "Training Epoch: 11 [10112/50000]\tLoss: 4.5508\tLR: 0.010000\n",
            "Training Epoch: 11 [10240/50000]\tLoss: 4.5024\tLR: 0.010000\n",
            "Training Epoch: 11 [10368/50000]\tLoss: 4.5830\tLR: 0.010000\n",
            "Training Epoch: 11 [10496/50000]\tLoss: 4.5344\tLR: 0.010000\n",
            "Training Epoch: 11 [10624/50000]\tLoss: 4.5177\tLR: 0.010000\n",
            "Training Epoch: 11 [10752/50000]\tLoss: 4.5443\tLR: 0.010000\n",
            "Training Epoch: 11 [10880/50000]\tLoss: 4.4892\tLR: 0.010000\n",
            "Training Epoch: 11 [11008/50000]\tLoss: 4.5090\tLR: 0.010000\n",
            "Training Epoch: 11 [11136/50000]\tLoss: 4.5329\tLR: 0.010000\n",
            "Training Epoch: 11 [11264/50000]\tLoss: 4.4965\tLR: 0.010000\n",
            "Training Epoch: 11 [11392/50000]\tLoss: 4.4802\tLR: 0.010000\n",
            "Training Epoch: 11 [11520/50000]\tLoss: 4.4914\tLR: 0.010000\n",
            "Training Epoch: 11 [11648/50000]\tLoss: 4.5609\tLR: 0.010000\n",
            "Training Epoch: 11 [11776/50000]\tLoss: 4.5243\tLR: 0.010000\n",
            "Training Epoch: 11 [11904/50000]\tLoss: 4.5326\tLR: 0.010000\n",
            "Training Epoch: 11 [12032/50000]\tLoss: 4.5344\tLR: 0.010000\n",
            "Training Epoch: 11 [12160/50000]\tLoss: 4.5035\tLR: 0.010000\n",
            "Training Epoch: 11 [12288/50000]\tLoss: 4.4702\tLR: 0.010000\n",
            "Training Epoch: 11 [12416/50000]\tLoss: 4.5658\tLR: 0.010000\n",
            "Training Epoch: 11 [12544/50000]\tLoss: 4.4850\tLR: 0.010000\n",
            "Training Epoch: 11 [12672/50000]\tLoss: 4.5406\tLR: 0.010000\n",
            "Training Epoch: 11 [12800/50000]\tLoss: 4.5279\tLR: 0.010000\n",
            "Training Epoch: 11 [12928/50000]\tLoss: 4.5470\tLR: 0.010000\n",
            "Training Epoch: 11 [13056/50000]\tLoss: 4.5018\tLR: 0.010000\n",
            "Training Epoch: 11 [13184/50000]\tLoss: 4.5476\tLR: 0.010000\n",
            "Training Epoch: 11 [13312/50000]\tLoss: 4.4797\tLR: 0.010000\n",
            "Training Epoch: 11 [13440/50000]\tLoss: 4.4314\tLR: 0.010000\n",
            "Training Epoch: 11 [13568/50000]\tLoss: 4.5316\tLR: 0.010000\n",
            "Training Epoch: 11 [13696/50000]\tLoss: 4.4833\tLR: 0.010000\n",
            "Training Epoch: 11 [13824/50000]\tLoss: 4.5371\tLR: 0.010000\n",
            "Training Epoch: 11 [13952/50000]\tLoss: 4.5603\tLR: 0.010000\n",
            "Training Epoch: 11 [14080/50000]\tLoss: 4.5363\tLR: 0.010000\n",
            "Training Epoch: 11 [14208/50000]\tLoss: 4.5519\tLR: 0.010000\n",
            "Training Epoch: 11 [14336/50000]\tLoss: 4.5098\tLR: 0.010000\n",
            "Training Epoch: 11 [14464/50000]\tLoss: 4.4991\tLR: 0.010000\n",
            "Training Epoch: 11 [14592/50000]\tLoss: 4.5271\tLR: 0.010000\n",
            "Training Epoch: 11 [14720/50000]\tLoss: 4.5717\tLR: 0.010000\n",
            "Training Epoch: 11 [14848/50000]\tLoss: 4.5343\tLR: 0.010000\n",
            "Training Epoch: 11 [14976/50000]\tLoss: 4.5149\tLR: 0.010000\n",
            "Training Epoch: 11 [15104/50000]\tLoss: 4.5554\tLR: 0.010000\n",
            "Training Epoch: 11 [15232/50000]\tLoss: 4.4400\tLR: 0.010000\n",
            "Training Epoch: 11 [15360/50000]\tLoss: 4.5336\tLR: 0.010000\n",
            "Training Epoch: 11 [15488/50000]\tLoss: 4.4436\tLR: 0.010000\n",
            "Training Epoch: 11 [15616/50000]\tLoss: 4.5057\tLR: 0.010000\n",
            "Training Epoch: 11 [15744/50000]\tLoss: 4.5141\tLR: 0.010000\n",
            "Training Epoch: 11 [15872/50000]\tLoss: 4.5429\tLR: 0.010000\n",
            "Training Epoch: 11 [16000/50000]\tLoss: 4.5452\tLR: 0.010000\n",
            "Training Epoch: 11 [16128/50000]\tLoss: 4.4926\tLR: 0.010000\n",
            "Training Epoch: 11 [16256/50000]\tLoss: 4.5165\tLR: 0.010000\n",
            "Training Epoch: 11 [16384/50000]\tLoss: 4.5125\tLR: 0.010000\n",
            "Training Epoch: 11 [16512/50000]\tLoss: 4.4909\tLR: 0.010000\n",
            "Training Epoch: 11 [16640/50000]\tLoss: 4.5082\tLR: 0.010000\n",
            "Training Epoch: 11 [16768/50000]\tLoss: 4.4534\tLR: 0.010000\n",
            "Training Epoch: 11 [16896/50000]\tLoss: 4.5455\tLR: 0.010000\n",
            "Training Epoch: 11 [17024/50000]\tLoss: 4.4584\tLR: 0.010000\n",
            "Training Epoch: 11 [17152/50000]\tLoss: 4.5098\tLR: 0.010000\n",
            "Training Epoch: 11 [17280/50000]\tLoss: 4.5200\tLR: 0.010000\n",
            "Training Epoch: 11 [17408/50000]\tLoss: 4.5435\tLR: 0.010000\n",
            "Training Epoch: 11 [17536/50000]\tLoss: 4.5605\tLR: 0.010000\n",
            "Training Epoch: 11 [17664/50000]\tLoss: 4.5346\tLR: 0.010000\n",
            "Training Epoch: 11 [17792/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 11 [17920/50000]\tLoss: 4.5465\tLR: 0.010000\n",
            "Training Epoch: 11 [18048/50000]\tLoss: 4.5621\tLR: 0.010000\n",
            "Training Epoch: 11 [18176/50000]\tLoss: 4.5455\tLR: 0.010000\n",
            "Training Epoch: 11 [18304/50000]\tLoss: 4.5314\tLR: 0.010000\n",
            "Training Epoch: 11 [18432/50000]\tLoss: 4.5280\tLR: 0.010000\n",
            "Training Epoch: 11 [18560/50000]\tLoss: 4.5005\tLR: 0.010000\n",
            "Training Epoch: 11 [18688/50000]\tLoss: 4.5849\tLR: 0.010000\n",
            "Training Epoch: 11 [18816/50000]\tLoss: 4.4626\tLR: 0.010000\n",
            "Training Epoch: 11 [18944/50000]\tLoss: 4.5180\tLR: 0.010000\n",
            "Training Epoch: 11 [19072/50000]\tLoss: 4.5222\tLR: 0.010000\n",
            "Training Epoch: 11 [19200/50000]\tLoss: 4.5212\tLR: 0.010000\n",
            "Training Epoch: 11 [19328/50000]\tLoss: 4.4668\tLR: 0.010000\n",
            "Training Epoch: 11 [19456/50000]\tLoss: 4.5270\tLR: 0.010000\n",
            "Training Epoch: 11 [19584/50000]\tLoss: 4.4527\tLR: 0.010000\n",
            "Training Epoch: 11 [19712/50000]\tLoss: 4.5275\tLR: 0.010000\n",
            "Training Epoch: 11 [19840/50000]\tLoss: 4.5118\tLR: 0.010000\n",
            "Training Epoch: 11 [19968/50000]\tLoss: 4.5374\tLR: 0.010000\n",
            "Training Epoch: 11 [20096/50000]\tLoss: 4.5371\tLR: 0.010000\n",
            "Training Epoch: 11 [20224/50000]\tLoss: 4.5772\tLR: 0.010000\n",
            "Training Epoch: 11 [20352/50000]\tLoss: 4.4729\tLR: 0.010000\n",
            "Training Epoch: 11 [20480/50000]\tLoss: 4.5122\tLR: 0.010000\n",
            "Training Epoch: 11 [20608/50000]\tLoss: 4.5031\tLR: 0.010000\n",
            "Training Epoch: 11 [20736/50000]\tLoss: 4.5520\tLR: 0.010000\n",
            "Training Epoch: 11 [20864/50000]\tLoss: 4.5047\tLR: 0.010000\n",
            "Training Epoch: 11 [20992/50000]\tLoss: 4.4965\tLR: 0.010000\n",
            "Training Epoch: 11 [21120/50000]\tLoss: 4.5332\tLR: 0.010000\n",
            "Training Epoch: 11 [21248/50000]\tLoss: 4.5174\tLR: 0.010000\n",
            "Training Epoch: 11 [21376/50000]\tLoss: 4.5356\tLR: 0.010000\n",
            "Training Epoch: 11 [21504/50000]\tLoss: 4.4883\tLR: 0.010000\n",
            "Training Epoch: 11 [21632/50000]\tLoss: 4.5591\tLR: 0.010000\n",
            "Training Epoch: 11 [21760/50000]\tLoss: 4.4328\tLR: 0.010000\n",
            "Training Epoch: 11 [21888/50000]\tLoss: 4.4666\tLR: 0.010000\n",
            "Training Epoch: 11 [22016/50000]\tLoss: 4.5381\tLR: 0.010000\n",
            "Training Epoch: 11 [22144/50000]\tLoss: 4.5752\tLR: 0.010000\n",
            "Training Epoch: 11 [22272/50000]\tLoss: 4.5143\tLR: 0.010000\n",
            "Training Epoch: 11 [22400/50000]\tLoss: 4.5130\tLR: 0.010000\n",
            "Training Epoch: 11 [22528/50000]\tLoss: 4.5186\tLR: 0.010000\n",
            "Training Epoch: 11 [22656/50000]\tLoss: 4.5709\tLR: 0.010000\n",
            "Training Epoch: 11 [22784/50000]\tLoss: 4.5878\tLR: 0.010000\n",
            "Training Epoch: 11 [22912/50000]\tLoss: 4.4361\tLR: 0.010000\n",
            "Training Epoch: 11 [23040/50000]\tLoss: 4.5705\tLR: 0.010000\n",
            "Training Epoch: 11 [23168/50000]\tLoss: 4.4916\tLR: 0.010000\n",
            "Training Epoch: 11 [23296/50000]\tLoss: 4.4326\tLR: 0.010000\n",
            "Training Epoch: 11 [23424/50000]\tLoss: 4.4885\tLR: 0.010000\n",
            "Training Epoch: 11 [23552/50000]\tLoss: 4.5066\tLR: 0.010000\n",
            "Training Epoch: 11 [23680/50000]\tLoss: 4.5154\tLR: 0.010000\n",
            "Training Epoch: 11 [23808/50000]\tLoss: 4.4856\tLR: 0.010000\n",
            "Training Epoch: 11 [23936/50000]\tLoss: 4.5442\tLR: 0.010000\n",
            "Training Epoch: 11 [24064/50000]\tLoss: 4.4764\tLR: 0.010000\n",
            "Training Epoch: 11 [24192/50000]\tLoss: 4.4647\tLR: 0.010000\n",
            "Training Epoch: 11 [24320/50000]\tLoss: 4.5414\tLR: 0.010000\n",
            "Training Epoch: 11 [24448/50000]\tLoss: 4.5792\tLR: 0.010000\n",
            "Training Epoch: 11 [24576/50000]\tLoss: 4.5358\tLR: 0.010000\n",
            "Training Epoch: 11 [24704/50000]\tLoss: 4.5202\tLR: 0.010000\n",
            "Training Epoch: 11 [24832/50000]\tLoss: 4.4734\tLR: 0.010000\n",
            "Training Epoch: 11 [24960/50000]\tLoss: 4.5242\tLR: 0.010000\n",
            "Training Epoch: 11 [25088/50000]\tLoss: 4.4766\tLR: 0.010000\n",
            "Training Epoch: 11 [25216/50000]\tLoss: 4.5104\tLR: 0.010000\n",
            "Training Epoch: 11 [25344/50000]\tLoss: 4.4818\tLR: 0.010000\n",
            "Training Epoch: 11 [25472/50000]\tLoss: 4.5166\tLR: 0.010000\n",
            "Training Epoch: 11 [25600/50000]\tLoss: 4.5402\tLR: 0.010000\n",
            "Training Epoch: 11 [25728/50000]\tLoss: 4.4675\tLR: 0.010000\n",
            "Training Epoch: 11 [25856/50000]\tLoss: 4.4980\tLR: 0.010000\n",
            "Training Epoch: 11 [25984/50000]\tLoss: 4.5364\tLR: 0.010000\n",
            "Training Epoch: 11 [26112/50000]\tLoss: 4.5441\tLR: 0.010000\n",
            "Training Epoch: 11 [26240/50000]\tLoss: 4.5948\tLR: 0.010000\n",
            "Training Epoch: 11 [26368/50000]\tLoss: 4.5233\tLR: 0.010000\n",
            "Training Epoch: 11 [26496/50000]\tLoss: 4.5574\tLR: 0.010000\n",
            "Training Epoch: 11 [26624/50000]\tLoss: 4.4819\tLR: 0.010000\n",
            "Training Epoch: 11 [26752/50000]\tLoss: 4.4869\tLR: 0.010000\n",
            "Training Epoch: 11 [26880/50000]\tLoss: 4.5616\tLR: 0.010000\n",
            "Training Epoch: 11 [27008/50000]\tLoss: 4.4734\tLR: 0.010000\n",
            "Training Epoch: 11 [27136/50000]\tLoss: 4.5726\tLR: 0.010000\n",
            "Training Epoch: 11 [27264/50000]\tLoss: 4.5079\tLR: 0.010000\n",
            "Training Epoch: 11 [27392/50000]\tLoss: 4.5298\tLR: 0.010000\n",
            "Training Epoch: 11 [27520/50000]\tLoss: 4.5049\tLR: 0.010000\n",
            "Training Epoch: 11 [27648/50000]\tLoss: 4.5336\tLR: 0.010000\n",
            "Training Epoch: 11 [27776/50000]\tLoss: 4.5004\tLR: 0.010000\n",
            "Training Epoch: 11 [27904/50000]\tLoss: 4.5067\tLR: 0.010000\n",
            "Training Epoch: 11 [28032/50000]\tLoss: 4.5429\tLR: 0.010000\n",
            "Training Epoch: 11 [28160/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 11 [28288/50000]\tLoss: 4.4952\tLR: 0.010000\n",
            "Training Epoch: 11 [28416/50000]\tLoss: 4.5351\tLR: 0.010000\n",
            "Training Epoch: 11 [28544/50000]\tLoss: 4.4547\tLR: 0.010000\n",
            "Training Epoch: 11 [28672/50000]\tLoss: 4.5526\tLR: 0.010000\n",
            "Training Epoch: 11 [28800/50000]\tLoss: 4.5738\tLR: 0.010000\n",
            "Training Epoch: 11 [28928/50000]\tLoss: 4.5084\tLR: 0.010000\n",
            "Training Epoch: 11 [29056/50000]\tLoss: 4.4471\tLR: 0.010000\n",
            "Training Epoch: 11 [29184/50000]\tLoss: 4.4699\tLR: 0.010000\n",
            "Training Epoch: 11 [29312/50000]\tLoss: 4.5458\tLR: 0.010000\n",
            "Training Epoch: 11 [29440/50000]\tLoss: 4.4885\tLR: 0.010000\n",
            "Training Epoch: 11 [29568/50000]\tLoss: 4.4301\tLR: 0.010000\n",
            "Training Epoch: 11 [29696/50000]\tLoss: 4.4885\tLR: 0.010000\n",
            "Training Epoch: 11 [29824/50000]\tLoss: 4.5391\tLR: 0.010000\n",
            "Training Epoch: 11 [29952/50000]\tLoss: 4.4866\tLR: 0.010000\n",
            "Training Epoch: 11 [30080/50000]\tLoss: 4.4857\tLR: 0.010000\n",
            "Training Epoch: 11 [30208/50000]\tLoss: 4.4944\tLR: 0.010000\n",
            "Training Epoch: 11 [30336/50000]\tLoss: 4.5077\tLR: 0.010000\n",
            "Training Epoch: 11 [30464/50000]\tLoss: 4.4836\tLR: 0.010000\n",
            "Training Epoch: 11 [30592/50000]\tLoss: 4.5372\tLR: 0.010000\n",
            "Training Epoch: 11 [30720/50000]\tLoss: 4.5230\tLR: 0.010000\n",
            "Training Epoch: 11 [30848/50000]\tLoss: 4.5282\tLR: 0.010000\n",
            "Training Epoch: 11 [30976/50000]\tLoss: 4.5547\tLR: 0.010000\n",
            "Training Epoch: 11 [31104/50000]\tLoss: 4.5628\tLR: 0.010000\n",
            "Training Epoch: 11 [31232/50000]\tLoss: 4.4923\tLR: 0.010000\n",
            "Training Epoch: 11 [31360/50000]\tLoss: 4.5113\tLR: 0.010000\n",
            "Training Epoch: 11 [31488/50000]\tLoss: 4.5388\tLR: 0.010000\n",
            "Training Epoch: 11 [31616/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 11 [31744/50000]\tLoss: 4.5534\tLR: 0.010000\n",
            "Training Epoch: 11 [31872/50000]\tLoss: 4.5605\tLR: 0.010000\n",
            "Training Epoch: 11 [32000/50000]\tLoss: 4.4946\tLR: 0.010000\n",
            "Training Epoch: 11 [32128/50000]\tLoss: 4.4778\tLR: 0.010000\n",
            "Training Epoch: 11 [32256/50000]\tLoss: 4.4796\tLR: 0.010000\n",
            "Training Epoch: 11 [32384/50000]\tLoss: 4.4787\tLR: 0.010000\n",
            "Training Epoch: 11 [32512/50000]\tLoss: 4.4840\tLR: 0.010000\n",
            "Training Epoch: 11 [32640/50000]\tLoss: 4.5773\tLR: 0.010000\n",
            "Training Epoch: 11 [32768/50000]\tLoss: 4.5333\tLR: 0.010000\n",
            "Training Epoch: 11 [32896/50000]\tLoss: 4.5107\tLR: 0.010000\n",
            "Training Epoch: 11 [33024/50000]\tLoss: 4.4642\tLR: 0.010000\n",
            "Training Epoch: 11 [33152/50000]\tLoss: 4.4724\tLR: 0.010000\n",
            "Training Epoch: 11 [33280/50000]\tLoss: 4.5471\tLR: 0.010000\n",
            "Training Epoch: 11 [33408/50000]\tLoss: 4.4927\tLR: 0.010000\n",
            "Training Epoch: 11 [33536/50000]\tLoss: 4.4230\tLR: 0.010000\n",
            "Training Epoch: 11 [33664/50000]\tLoss: 4.5030\tLR: 0.010000\n",
            "Training Epoch: 11 [33792/50000]\tLoss: 4.5179\tLR: 0.010000\n",
            "Training Epoch: 11 [33920/50000]\tLoss: 4.5246\tLR: 0.010000\n",
            "Training Epoch: 11 [34048/50000]\tLoss: 4.5633\tLR: 0.010000\n",
            "Training Epoch: 11 [34176/50000]\tLoss: 4.4740\tLR: 0.010000\n",
            "Training Epoch: 11 [34304/50000]\tLoss: 4.5035\tLR: 0.010000\n",
            "Training Epoch: 11 [34432/50000]\tLoss: 4.4389\tLR: 0.010000\n",
            "Training Epoch: 11 [34560/50000]\tLoss: 4.4929\tLR: 0.010000\n",
            "Training Epoch: 11 [34688/50000]\tLoss: 4.4755\tLR: 0.010000\n",
            "Training Epoch: 11 [34816/50000]\tLoss: 4.4497\tLR: 0.010000\n",
            "Training Epoch: 11 [34944/50000]\tLoss: 4.5458\tLR: 0.010000\n",
            "Training Epoch: 11 [35072/50000]\tLoss: 4.5262\tLR: 0.010000\n",
            "Training Epoch: 11 [35200/50000]\tLoss: 4.4630\tLR: 0.010000\n",
            "Training Epoch: 11 [35328/50000]\tLoss: 4.5023\tLR: 0.010000\n",
            "Training Epoch: 11 [35456/50000]\tLoss: 4.5349\tLR: 0.010000\n",
            "Training Epoch: 11 [35584/50000]\tLoss: 4.4273\tLR: 0.010000\n",
            "Training Epoch: 11 [35712/50000]\tLoss: 4.5227\tLR: 0.010000\n",
            "Training Epoch: 11 [35840/50000]\tLoss: 4.5002\tLR: 0.010000\n",
            "Training Epoch: 11 [35968/50000]\tLoss: 4.4795\tLR: 0.010000\n",
            "Training Epoch: 11 [36096/50000]\tLoss: 4.4716\tLR: 0.010000\n",
            "Training Epoch: 11 [36224/50000]\tLoss: 4.5620\tLR: 0.010000\n",
            "Training Epoch: 11 [36352/50000]\tLoss: 4.5863\tLR: 0.010000\n",
            "Training Epoch: 11 [36480/50000]\tLoss: 4.5084\tLR: 0.010000\n",
            "Training Epoch: 11 [36608/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 11 [36736/50000]\tLoss: 4.5582\tLR: 0.010000\n",
            "Training Epoch: 11 [36864/50000]\tLoss: 4.5427\tLR: 0.010000\n",
            "Training Epoch: 11 [36992/50000]\tLoss: 4.5113\tLR: 0.010000\n",
            "Training Epoch: 11 [37120/50000]\tLoss: 4.5019\tLR: 0.010000\n",
            "Training Epoch: 11 [37248/50000]\tLoss: 4.5165\tLR: 0.010000\n",
            "Training Epoch: 11 [37376/50000]\tLoss: 4.5460\tLR: 0.010000\n",
            "Training Epoch: 11 [37504/50000]\tLoss: 4.4957\tLR: 0.010000\n",
            "Training Epoch: 11 [37632/50000]\tLoss: 4.5083\tLR: 0.010000\n",
            "Training Epoch: 11 [37760/50000]\tLoss: 4.5224\tLR: 0.010000\n",
            "Training Epoch: 11 [37888/50000]\tLoss: 4.5456\tLR: 0.010000\n",
            "Training Epoch: 11 [38016/50000]\tLoss: 4.4892\tLR: 0.010000\n",
            "Training Epoch: 11 [38144/50000]\tLoss: 4.4647\tLR: 0.010000\n",
            "Training Epoch: 11 [38272/50000]\tLoss: 4.5248\tLR: 0.010000\n",
            "Training Epoch: 11 [38400/50000]\tLoss: 4.4897\tLR: 0.010000\n",
            "Training Epoch: 11 [38528/50000]\tLoss: 4.4688\tLR: 0.010000\n",
            "Training Epoch: 11 [38656/50000]\tLoss: 4.5857\tLR: 0.010000\n",
            "Training Epoch: 11 [38784/50000]\tLoss: 4.5125\tLR: 0.010000\n",
            "Training Epoch: 11 [38912/50000]\tLoss: 4.4915\tLR: 0.010000\n",
            "Training Epoch: 11 [39040/50000]\tLoss: 4.4341\tLR: 0.010000\n",
            "Training Epoch: 11 [39168/50000]\tLoss: 4.5430\tLR: 0.010000\n",
            "Training Epoch: 11 [39296/50000]\tLoss: 4.5255\tLR: 0.010000\n",
            "Training Epoch: 11 [39424/50000]\tLoss: 4.5397\tLR: 0.010000\n",
            "Training Epoch: 11 [39552/50000]\tLoss: 4.5122\tLR: 0.010000\n",
            "Training Epoch: 11 [39680/50000]\tLoss: 4.5069\tLR: 0.010000\n",
            "Training Epoch: 11 [39808/50000]\tLoss: 4.5320\tLR: 0.010000\n",
            "Training Epoch: 11 [39936/50000]\tLoss: 4.5802\tLR: 0.010000\n",
            "Training Epoch: 11 [40064/50000]\tLoss: 4.5389\tLR: 0.010000\n",
            "Training Epoch: 11 [40192/50000]\tLoss: 4.5007\tLR: 0.010000\n",
            "Training Epoch: 11 [40320/50000]\tLoss: 4.4816\tLR: 0.010000\n",
            "Training Epoch: 11 [40448/50000]\tLoss: 4.4693\tLR: 0.010000\n",
            "Training Epoch: 11 [40576/50000]\tLoss: 4.5502\tLR: 0.010000\n",
            "Training Epoch: 11 [40704/50000]\tLoss: 4.5981\tLR: 0.010000\n",
            "Training Epoch: 11 [40832/50000]\tLoss: 4.4430\tLR: 0.010000\n",
            "Training Epoch: 11 [40960/50000]\tLoss: 4.4908\tLR: 0.010000\n",
            "Training Epoch: 11 [41088/50000]\tLoss: 4.5934\tLR: 0.010000\n",
            "Training Epoch: 11 [41216/50000]\tLoss: 4.4369\tLR: 0.010000\n",
            "Training Epoch: 11 [41344/50000]\tLoss: 4.5558\tLR: 0.010000\n",
            "Training Epoch: 11 [41472/50000]\tLoss: 4.4419\tLR: 0.010000\n",
            "Training Epoch: 11 [41600/50000]\tLoss: 4.4439\tLR: 0.010000\n",
            "Training Epoch: 11 [41728/50000]\tLoss: 4.5111\tLR: 0.010000\n",
            "Training Epoch: 11 [41856/50000]\tLoss: 4.5420\tLR: 0.010000\n",
            "Training Epoch: 11 [41984/50000]\tLoss: 4.5049\tLR: 0.010000\n",
            "Training Epoch: 11 [42112/50000]\tLoss: 4.5294\tLR: 0.010000\n",
            "Training Epoch: 11 [42240/50000]\tLoss: 4.5230\tLR: 0.010000\n",
            "Training Epoch: 11 [42368/50000]\tLoss: 4.4842\tLR: 0.010000\n",
            "Training Epoch: 11 [42496/50000]\tLoss: 4.4911\tLR: 0.010000\n",
            "Training Epoch: 11 [42624/50000]\tLoss: 4.4707\tLR: 0.010000\n",
            "Training Epoch: 11 [42752/50000]\tLoss: 4.5541\tLR: 0.010000\n",
            "Training Epoch: 11 [42880/50000]\tLoss: 4.5228\tLR: 0.010000\n",
            "Training Epoch: 11 [43008/50000]\tLoss: 4.5249\tLR: 0.010000\n",
            "Training Epoch: 11 [43136/50000]\tLoss: 4.5052\tLR: 0.010000\n",
            "Training Epoch: 11 [43264/50000]\tLoss: 4.4931\tLR: 0.010000\n",
            "Training Epoch: 11 [43392/50000]\tLoss: 4.5390\tLR: 0.010000\n",
            "Training Epoch: 11 [43520/50000]\tLoss: 4.4836\tLR: 0.010000\n",
            "Training Epoch: 11 [43648/50000]\tLoss: 4.4959\tLR: 0.010000\n",
            "Training Epoch: 11 [43776/50000]\tLoss: 4.4732\tLR: 0.010000\n",
            "Training Epoch: 11 [43904/50000]\tLoss: 4.5070\tLR: 0.010000\n",
            "Training Epoch: 11 [44032/50000]\tLoss: 4.4826\tLR: 0.010000\n",
            "Training Epoch: 11 [44160/50000]\tLoss: 4.5597\tLR: 0.010000\n",
            "Training Epoch: 11 [44288/50000]\tLoss: 4.6316\tLR: 0.010000\n",
            "Training Epoch: 11 [44416/50000]\tLoss: 4.5755\tLR: 0.010000\n",
            "Training Epoch: 11 [44544/50000]\tLoss: 4.5475\tLR: 0.010000\n",
            "Training Epoch: 11 [44672/50000]\tLoss: 4.5869\tLR: 0.010000\n",
            "Training Epoch: 11 [44800/50000]\tLoss: 4.5292\tLR: 0.010000\n",
            "Training Epoch: 11 [44928/50000]\tLoss: 4.4874\tLR: 0.010000\n",
            "Training Epoch: 11 [45056/50000]\tLoss: 4.5148\tLR: 0.010000\n",
            "Training Epoch: 11 [45184/50000]\tLoss: 4.4395\tLR: 0.010000\n",
            "Training Epoch: 11 [45312/50000]\tLoss: 4.5252\tLR: 0.010000\n",
            "Training Epoch: 11 [45440/50000]\tLoss: 4.5448\tLR: 0.010000\n",
            "Training Epoch: 11 [45568/50000]\tLoss: 4.4849\tLR: 0.010000\n",
            "Training Epoch: 11 [45696/50000]\tLoss: 4.5508\tLR: 0.010000\n",
            "Training Epoch: 11 [45824/50000]\tLoss: 4.5010\tLR: 0.010000\n",
            "Training Epoch: 11 [45952/50000]\tLoss: 4.5498\tLR: 0.010000\n",
            "Training Epoch: 11 [46080/50000]\tLoss: 4.4920\tLR: 0.010000\n",
            "Training Epoch: 11 [46208/50000]\tLoss: 4.5493\tLR: 0.010000\n",
            "Training Epoch: 11 [46336/50000]\tLoss: 4.5058\tLR: 0.010000\n",
            "Training Epoch: 11 [46464/50000]\tLoss: 4.4860\tLR: 0.010000\n",
            "Training Epoch: 11 [46592/50000]\tLoss: 4.4972\tLR: 0.010000\n",
            "Training Epoch: 11 [46720/50000]\tLoss: 4.4820\tLR: 0.010000\n",
            "Training Epoch: 11 [46848/50000]\tLoss: 4.4822\tLR: 0.010000\n",
            "Training Epoch: 11 [46976/50000]\tLoss: 4.4843\tLR: 0.010000\n",
            "Training Epoch: 11 [47104/50000]\tLoss: 4.4963\tLR: 0.010000\n",
            "Training Epoch: 11 [47232/50000]\tLoss: 4.5776\tLR: 0.010000\n",
            "Training Epoch: 11 [47360/50000]\tLoss: 4.5360\tLR: 0.010000\n",
            "Training Epoch: 11 [47488/50000]\tLoss: 4.5014\tLR: 0.010000\n",
            "Training Epoch: 11 [47616/50000]\tLoss: 4.5618\tLR: 0.010000\n",
            "Training Epoch: 11 [47744/50000]\tLoss: 4.4963\tLR: 0.010000\n",
            "Training Epoch: 11 [47872/50000]\tLoss: 4.5123\tLR: 0.010000\n",
            "Training Epoch: 11 [48000/50000]\tLoss: 4.5298\tLR: 0.010000\n",
            "Training Epoch: 11 [48128/50000]\tLoss: 4.4925\tLR: 0.010000\n",
            "Training Epoch: 11 [48256/50000]\tLoss: 4.5284\tLR: 0.010000\n",
            "Training Epoch: 11 [48384/50000]\tLoss: 4.4363\tLR: 0.010000\n",
            "Training Epoch: 11 [48512/50000]\tLoss: 4.4806\tLR: 0.010000\n",
            "Training Epoch: 11 [48640/50000]\tLoss: 4.5240\tLR: 0.010000\n",
            "Training Epoch: 11 [48768/50000]\tLoss: 4.5360\tLR: 0.010000\n",
            "Training Epoch: 11 [48896/50000]\tLoss: 4.5292\tLR: 0.010000\n",
            "Training Epoch: 11 [49024/50000]\tLoss: 4.5239\tLR: 0.010000\n",
            "Training Epoch: 11 [49152/50000]\tLoss: 4.4878\tLR: 0.010000\n",
            "Training Epoch: 11 [49280/50000]\tLoss: 4.4868\tLR: 0.010000\n",
            "Training Epoch: 11 [49408/50000]\tLoss: 4.4757\tLR: 0.010000\n",
            "Training Epoch: 11 [49536/50000]\tLoss: 4.4845\tLR: 0.010000\n",
            "Training Epoch: 11 [49664/50000]\tLoss: 4.5333\tLR: 0.010000\n",
            "Training Epoch: 11 [49792/50000]\tLoss: 4.5635\tLR: 0.010000\n",
            "Training Epoch: 11 [49920/50000]\tLoss: 4.4658\tLR: 0.010000\n",
            "Training Epoch: 11 [50000/50000]\tLoss: 4.4993\tLR: 0.010000\n",
            "epoch 11 training time consumed: 145.54s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 116774 GiB | 116773 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 115300 GiB | 115300 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1473 GiB |   1473 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 116774 GiB | 116773 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 115300 GiB | 115300 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1473 GiB |   1473 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 116455 GiB | 116455 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 114982 GiB | 114982 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1473 GiB |   1473 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB |  96821 GiB |  96821 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB |  95156 GiB |  95156 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   1664 GiB |   1664 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    7115 K  |    7114 K  |\n",
            "|       from large pool |       5    |     146    |    3448 K  |    3448 K  |\n",
            "|       from small pool |     516    |     682    |    3666 K  |    3666 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    7115 K  |    7114 K  |\n",
            "|       from large pool |       5    |     146    |    3448 K  |    3448 K  |\n",
            "|       from small pool |     516    |     682    |    3666 K  |    3666 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |     117    |    2653 K  |    2653 K  |\n",
            "|       from large pool |       4    |      46    |    1628 K  |    1628 K  |\n",
            "|       from small pool |      74    |      89    |    1024 K  |    1024 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0359, Accuracy: 0.0190, Time consumed:9.33s\n",
            "\n",
            "Training Epoch: 12 [128/50000]\tLoss: 4.5056\tLR: 0.010000\n",
            "Training Epoch: 12 [256/50000]\tLoss: 4.4740\tLR: 0.010000\n",
            "Training Epoch: 12 [384/50000]\tLoss: 4.5278\tLR: 0.010000\n",
            "Training Epoch: 12 [512/50000]\tLoss: 4.4554\tLR: 0.010000\n",
            "Training Epoch: 12 [640/50000]\tLoss: 4.5055\tLR: 0.010000\n",
            "Training Epoch: 12 [768/50000]\tLoss: 4.5363\tLR: 0.010000\n",
            "Training Epoch: 12 [896/50000]\tLoss: 4.4698\tLR: 0.010000\n",
            "Training Epoch: 12 [1024/50000]\tLoss: 4.5195\tLR: 0.010000\n",
            "Training Epoch: 12 [1152/50000]\tLoss: 4.5432\tLR: 0.010000\n",
            "Training Epoch: 12 [1280/50000]\tLoss: 4.5346\tLR: 0.010000\n",
            "Training Epoch: 12 [1408/50000]\tLoss: 4.4825\tLR: 0.010000\n",
            "Training Epoch: 12 [1536/50000]\tLoss: 4.5082\tLR: 0.010000\n",
            "Training Epoch: 12 [1664/50000]\tLoss: 4.4348\tLR: 0.010000\n",
            "Training Epoch: 12 [1792/50000]\tLoss: 4.4352\tLR: 0.010000\n",
            "Training Epoch: 12 [1920/50000]\tLoss: 4.4922\tLR: 0.010000\n",
            "Training Epoch: 12 [2048/50000]\tLoss: 4.4748\tLR: 0.010000\n",
            "Training Epoch: 12 [2176/50000]\tLoss: 4.5267\tLR: 0.010000\n",
            "Training Epoch: 12 [2304/50000]\tLoss: 4.5017\tLR: 0.010000\n",
            "Training Epoch: 12 [2432/50000]\tLoss: 4.5156\tLR: 0.010000\n",
            "Training Epoch: 12 [2560/50000]\tLoss: 4.5369\tLR: 0.010000\n",
            "Training Epoch: 12 [2688/50000]\tLoss: 4.5218\tLR: 0.010000\n",
            "Training Epoch: 12 [2816/50000]\tLoss: 4.5226\tLR: 0.010000\n",
            "Training Epoch: 12 [2944/50000]\tLoss: 4.4604\tLR: 0.010000\n",
            "Training Epoch: 12 [3072/50000]\tLoss: 4.5222\tLR: 0.010000\n",
            "Training Epoch: 12 [3200/50000]\tLoss: 4.5425\tLR: 0.010000\n",
            "Training Epoch: 12 [3328/50000]\tLoss: 4.4927\tLR: 0.010000\n",
            "Training Epoch: 12 [3456/50000]\tLoss: 4.5208\tLR: 0.010000\n",
            "Training Epoch: 12 [3584/50000]\tLoss: 4.4802\tLR: 0.010000\n",
            "Training Epoch: 12 [3712/50000]\tLoss: 4.5624\tLR: 0.010000\n",
            "Training Epoch: 12 [3840/50000]\tLoss: 4.4419\tLR: 0.010000\n",
            "Training Epoch: 12 [3968/50000]\tLoss: 4.4850\tLR: 0.010000\n",
            "Training Epoch: 12 [4096/50000]\tLoss: 4.5077\tLR: 0.010000\n",
            "Training Epoch: 12 [4224/50000]\tLoss: 4.5140\tLR: 0.010000\n",
            "Training Epoch: 12 [4352/50000]\tLoss: 4.5281\tLR: 0.010000\n",
            "Training Epoch: 12 [4480/50000]\tLoss: 4.5607\tLR: 0.010000\n",
            "Training Epoch: 12 [4608/50000]\tLoss: 4.5366\tLR: 0.010000\n",
            "Training Epoch: 12 [4736/50000]\tLoss: 4.4888\tLR: 0.010000\n",
            "Training Epoch: 12 [4864/50000]\tLoss: 4.4923\tLR: 0.010000\n",
            "Training Epoch: 12 [4992/50000]\tLoss: 4.5365\tLR: 0.010000\n",
            "Training Epoch: 12 [5120/50000]\tLoss: 4.4365\tLR: 0.010000\n",
            "Training Epoch: 12 [5248/50000]\tLoss: 4.5014\tLR: 0.010000\n",
            "Training Epoch: 12 [5376/50000]\tLoss: 4.5036\tLR: 0.010000\n",
            "Training Epoch: 12 [5504/50000]\tLoss: 4.4830\tLR: 0.010000\n",
            "Training Epoch: 12 [5632/50000]\tLoss: 4.4684\tLR: 0.010000\n",
            "Training Epoch: 12 [5760/50000]\tLoss: 4.5047\tLR: 0.010000\n",
            "Training Epoch: 12 [5888/50000]\tLoss: 4.4456\tLR: 0.010000\n",
            "Training Epoch: 12 [6016/50000]\tLoss: 4.4804\tLR: 0.010000\n",
            "Training Epoch: 12 [6144/50000]\tLoss: 4.5293\tLR: 0.010000\n",
            "Training Epoch: 12 [6272/50000]\tLoss: 4.4546\tLR: 0.010000\n",
            "Training Epoch: 12 [6400/50000]\tLoss: 4.5404\tLR: 0.010000\n",
            "Training Epoch: 12 [6528/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 12 [6656/50000]\tLoss: 4.5931\tLR: 0.010000\n",
            "Training Epoch: 12 [6784/50000]\tLoss: 4.5321\tLR: 0.010000\n",
            "Training Epoch: 12 [6912/50000]\tLoss: 4.4822\tLR: 0.010000\n",
            "Training Epoch: 12 [7040/50000]\tLoss: 4.5665\tLR: 0.010000\n",
            "Training Epoch: 12 [7168/50000]\tLoss: 4.5839\tLR: 0.010000\n",
            "Training Epoch: 12 [7296/50000]\tLoss: 4.5118\tLR: 0.010000\n",
            "Training Epoch: 12 [7424/50000]\tLoss: 4.5568\tLR: 0.010000\n",
            "Training Epoch: 12 [7552/50000]\tLoss: 4.5072\tLR: 0.010000\n",
            "Training Epoch: 12 [7680/50000]\tLoss: 4.5352\tLR: 0.010000\n",
            "Training Epoch: 12 [7808/50000]\tLoss: 4.5564\tLR: 0.010000\n",
            "Training Epoch: 12 [7936/50000]\tLoss: 4.5553\tLR: 0.010000\n",
            "Training Epoch: 12 [8064/50000]\tLoss: 4.4789\tLR: 0.010000\n",
            "Training Epoch: 12 [8192/50000]\tLoss: 4.4138\tLR: 0.010000\n",
            "Training Epoch: 12 [8320/50000]\tLoss: 4.4817\tLR: 0.010000\n",
            "Training Epoch: 12 [8448/50000]\tLoss: 4.4865\tLR: 0.010000\n",
            "Training Epoch: 12 [8576/50000]\tLoss: 4.4644\tLR: 0.010000\n",
            "Training Epoch: 12 [8704/50000]\tLoss: 4.5252\tLR: 0.010000\n",
            "Training Epoch: 12 [8832/50000]\tLoss: 4.4880\tLR: 0.010000\n",
            "Training Epoch: 12 [8960/50000]\tLoss: 4.5022\tLR: 0.010000\n",
            "Training Epoch: 12 [9088/50000]\tLoss: 4.4918\tLR: 0.010000\n",
            "Training Epoch: 12 [9216/50000]\tLoss: 4.5544\tLR: 0.010000\n",
            "Training Epoch: 12 [9344/50000]\tLoss: 4.5605\tLR: 0.010000\n",
            "Training Epoch: 12 [9472/50000]\tLoss: 4.4831\tLR: 0.010000\n",
            "Training Epoch: 12 [9600/50000]\tLoss: 4.5691\tLR: 0.010000\n",
            "Training Epoch: 12 [9728/50000]\tLoss: 4.4886\tLR: 0.010000\n",
            "Training Epoch: 12 [9856/50000]\tLoss: 4.5528\tLR: 0.010000\n",
            "Training Epoch: 12 [9984/50000]\tLoss: 4.5161\tLR: 0.010000\n",
            "Training Epoch: 12 [10112/50000]\tLoss: 4.4934\tLR: 0.010000\n",
            "Training Epoch: 12 [10240/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 12 [10368/50000]\tLoss: 4.5021\tLR: 0.010000\n",
            "Training Epoch: 12 [10496/50000]\tLoss: 4.5867\tLR: 0.010000\n",
            "Training Epoch: 12 [10624/50000]\tLoss: 4.5064\tLR: 0.010000\n",
            "Training Epoch: 12 [10752/50000]\tLoss: 4.4417\tLR: 0.010000\n",
            "Training Epoch: 12 [10880/50000]\tLoss: 4.5089\tLR: 0.010000\n",
            "Training Epoch: 12 [11008/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 12 [11136/50000]\tLoss: 4.5024\tLR: 0.010000\n",
            "Training Epoch: 12 [11264/50000]\tLoss: 4.4733\tLR: 0.010000\n",
            "Training Epoch: 12 [11392/50000]\tLoss: 4.5026\tLR: 0.010000\n",
            "Training Epoch: 12 [11520/50000]\tLoss: 4.5397\tLR: 0.010000\n",
            "Training Epoch: 12 [11648/50000]\tLoss: 4.4889\tLR: 0.010000\n",
            "Training Epoch: 12 [11776/50000]\tLoss: 4.5334\tLR: 0.010000\n",
            "Training Epoch: 12 [11904/50000]\tLoss: 4.5211\tLR: 0.010000\n",
            "Training Epoch: 12 [12032/50000]\tLoss: 4.4605\tLR: 0.010000\n",
            "Training Epoch: 12 [12160/50000]\tLoss: 4.4589\tLR: 0.010000\n",
            "Training Epoch: 12 [12288/50000]\tLoss: 4.5330\tLR: 0.010000\n",
            "Training Epoch: 12 [12416/50000]\tLoss: 4.4519\tLR: 0.010000\n",
            "Training Epoch: 12 [12544/50000]\tLoss: 4.5100\tLR: 0.010000\n",
            "Training Epoch: 12 [12672/50000]\tLoss: 4.4685\tLR: 0.010000\n",
            "Training Epoch: 12 [12800/50000]\tLoss: 4.5078\tLR: 0.010000\n",
            "Training Epoch: 12 [12928/50000]\tLoss: 4.5336\tLR: 0.010000\n",
            "Training Epoch: 12 [13056/50000]\tLoss: 4.5410\tLR: 0.010000\n",
            "Training Epoch: 12 [13184/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 12 [13312/50000]\tLoss: 4.5132\tLR: 0.010000\n",
            "Training Epoch: 12 [13440/50000]\tLoss: 4.4646\tLR: 0.010000\n",
            "Training Epoch: 12 [13568/50000]\tLoss: 4.4726\tLR: 0.010000\n",
            "Training Epoch: 12 [13696/50000]\tLoss: 4.4957\tLR: 0.010000\n",
            "Training Epoch: 12 [13824/50000]\tLoss: 4.5473\tLR: 0.010000\n",
            "Training Epoch: 12 [13952/50000]\tLoss: 4.5344\tLR: 0.010000\n",
            "Training Epoch: 12 [14080/50000]\tLoss: 4.4631\tLR: 0.010000\n",
            "Training Epoch: 12 [14208/50000]\tLoss: 4.4629\tLR: 0.010000\n",
            "Training Epoch: 12 [14336/50000]\tLoss: 4.5251\tLR: 0.010000\n",
            "Training Epoch: 12 [14464/50000]\tLoss: 4.5016\tLR: 0.010000\n",
            "Training Epoch: 12 [14592/50000]\tLoss: 4.4389\tLR: 0.010000\n",
            "Training Epoch: 12 [14720/50000]\tLoss: 4.4961\tLR: 0.010000\n",
            "Training Epoch: 12 [14848/50000]\tLoss: 4.4969\tLR: 0.010000\n",
            "Training Epoch: 12 [14976/50000]\tLoss: 4.5362\tLR: 0.010000\n",
            "Training Epoch: 12 [15104/50000]\tLoss: 4.5275\tLR: 0.010000\n",
            "Training Epoch: 12 [15232/50000]\tLoss: 4.5049\tLR: 0.010000\n",
            "Training Epoch: 12 [15360/50000]\tLoss: 4.4992\tLR: 0.010000\n",
            "Training Epoch: 12 [15488/50000]\tLoss: 4.4414\tLR: 0.010000\n",
            "Training Epoch: 12 [15616/50000]\tLoss: 4.5199\tLR: 0.010000\n",
            "Training Epoch: 12 [15744/50000]\tLoss: 4.4547\tLR: 0.010000\n",
            "Training Epoch: 12 [15872/50000]\tLoss: 4.5036\tLR: 0.010000\n",
            "Training Epoch: 12 [16000/50000]\tLoss: 4.4505\tLR: 0.010000\n",
            "Training Epoch: 12 [16128/50000]\tLoss: 4.5078\tLR: 0.010000\n",
            "Training Epoch: 12 [16256/50000]\tLoss: 4.5691\tLR: 0.010000\n",
            "Training Epoch: 12 [16384/50000]\tLoss: 4.5710\tLR: 0.010000\n",
            "Training Epoch: 12 [16512/50000]\tLoss: 4.5333\tLR: 0.010000\n",
            "Training Epoch: 12 [16640/50000]\tLoss: 4.5970\tLR: 0.010000\n",
            "Training Epoch: 12 [16768/50000]\tLoss: 4.4517\tLR: 0.010000\n",
            "Training Epoch: 12 [16896/50000]\tLoss: 4.5473\tLR: 0.010000\n",
            "Training Epoch: 12 [17024/50000]\tLoss: 4.5126\tLR: 0.010000\n",
            "Training Epoch: 12 [17152/50000]\tLoss: 4.4814\tLR: 0.010000\n",
            "Training Epoch: 12 [17280/50000]\tLoss: 4.5010\tLR: 0.010000\n",
            "Training Epoch: 12 [17408/50000]\tLoss: 4.5318\tLR: 0.010000\n",
            "Training Epoch: 12 [17536/50000]\tLoss: 4.4552\tLR: 0.010000\n",
            "Training Epoch: 12 [17664/50000]\tLoss: 4.5346\tLR: 0.010000\n",
            "Training Epoch: 12 [17792/50000]\tLoss: 4.5352\tLR: 0.010000\n",
            "Training Epoch: 12 [17920/50000]\tLoss: 4.4590\tLR: 0.010000\n",
            "Training Epoch: 12 [18048/50000]\tLoss: 4.5929\tLR: 0.010000\n",
            "Training Epoch: 12 [18176/50000]\tLoss: 4.4472\tLR: 0.010000\n",
            "Training Epoch: 12 [18304/50000]\tLoss: 4.5569\tLR: 0.010000\n",
            "Training Epoch: 12 [18432/50000]\tLoss: 4.4992\tLR: 0.010000\n",
            "Training Epoch: 12 [18560/50000]\tLoss: 4.5017\tLR: 0.010000\n",
            "Training Epoch: 12 [18688/50000]\tLoss: 4.4828\tLR: 0.010000\n",
            "Training Epoch: 12 [18816/50000]\tLoss: 4.5362\tLR: 0.010000\n",
            "Training Epoch: 12 [18944/50000]\tLoss: 4.5440\tLR: 0.010000\n",
            "Training Epoch: 12 [19072/50000]\tLoss: 4.5635\tLR: 0.010000\n",
            "Training Epoch: 12 [19200/50000]\tLoss: 4.5648\tLR: 0.010000\n",
            "Training Epoch: 12 [19328/50000]\tLoss: 4.5308\tLR: 0.010000\n",
            "Training Epoch: 12 [19456/50000]\tLoss: 4.4468\tLR: 0.010000\n",
            "Training Epoch: 12 [19584/50000]\tLoss: 4.5141\tLR: 0.010000\n",
            "Training Epoch: 12 [19712/50000]\tLoss: 4.4784\tLR: 0.010000\n",
            "Training Epoch: 12 [19840/50000]\tLoss: 4.4820\tLR: 0.010000\n",
            "Training Epoch: 12 [19968/50000]\tLoss: 4.5182\tLR: 0.010000\n",
            "Training Epoch: 12 [20096/50000]\tLoss: 4.5309\tLR: 0.010000\n",
            "Training Epoch: 12 [20224/50000]\tLoss: 4.4945\tLR: 0.010000\n",
            "Training Epoch: 12 [20352/50000]\tLoss: 4.5158\tLR: 0.010000\n",
            "Training Epoch: 12 [20480/50000]\tLoss: 4.5502\tLR: 0.010000\n",
            "Training Epoch: 12 [20608/50000]\tLoss: 4.5277\tLR: 0.010000\n",
            "Training Epoch: 12 [20736/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 12 [20864/50000]\tLoss: 4.4871\tLR: 0.010000\n",
            "Training Epoch: 12 [20992/50000]\tLoss: 4.5870\tLR: 0.010000\n",
            "Training Epoch: 12 [21120/50000]\tLoss: 4.4778\tLR: 0.010000\n",
            "Training Epoch: 12 [21248/50000]\tLoss: 4.4730\tLR: 0.010000\n",
            "Training Epoch: 12 [21376/50000]\tLoss: 4.5731\tLR: 0.010000\n",
            "Training Epoch: 12 [21504/50000]\tLoss: 4.5050\tLR: 0.010000\n",
            "Training Epoch: 12 [21632/50000]\tLoss: 4.5187\tLR: 0.010000\n",
            "Training Epoch: 12 [21760/50000]\tLoss: 4.5291\tLR: 0.010000\n",
            "Training Epoch: 12 [21888/50000]\tLoss: 4.4796\tLR: 0.010000\n",
            "Training Epoch: 12 [22016/50000]\tLoss: 4.6129\tLR: 0.010000\n",
            "Training Epoch: 12 [22144/50000]\tLoss: 4.4969\tLR: 0.010000\n",
            "Training Epoch: 12 [22272/50000]\tLoss: 4.5359\tLR: 0.010000\n",
            "Training Epoch: 12 [22400/50000]\tLoss: 4.4873\tLR: 0.010000\n",
            "Training Epoch: 12 [22528/50000]\tLoss: 4.5750\tLR: 0.010000\n",
            "Training Epoch: 12 [22656/50000]\tLoss: 4.5399\tLR: 0.010000\n",
            "Training Epoch: 12 [22784/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 12 [22912/50000]\tLoss: 4.4991\tLR: 0.010000\n",
            "Training Epoch: 12 [23040/50000]\tLoss: 4.5783\tLR: 0.010000\n",
            "Training Epoch: 12 [23168/50000]\tLoss: 4.5773\tLR: 0.010000\n",
            "Training Epoch: 12 [23296/50000]\tLoss: 4.5521\tLR: 0.010000\n",
            "Training Epoch: 12 [23424/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 12 [23552/50000]\tLoss: 4.5050\tLR: 0.010000\n",
            "Training Epoch: 12 [23680/50000]\tLoss: 4.5442\tLR: 0.010000\n",
            "Training Epoch: 12 [23808/50000]\tLoss: 4.5293\tLR: 0.010000\n",
            "Training Epoch: 12 [23936/50000]\tLoss: 4.5326\tLR: 0.010000\n",
            "Training Epoch: 12 [24064/50000]\tLoss: 4.5679\tLR: 0.010000\n",
            "Training Epoch: 12 [24192/50000]\tLoss: 4.4939\tLR: 0.010000\n",
            "Training Epoch: 12 [24320/50000]\tLoss: 4.5551\tLR: 0.010000\n",
            "Training Epoch: 12 [24448/50000]\tLoss: 4.4766\tLR: 0.010000\n",
            "Training Epoch: 12 [24576/50000]\tLoss: 4.4747\tLR: 0.010000\n",
            "Training Epoch: 12 [24704/50000]\tLoss: 4.4910\tLR: 0.010000\n",
            "Training Epoch: 12 [24832/50000]\tLoss: 4.5456\tLR: 0.010000\n",
            "Training Epoch: 12 [24960/50000]\tLoss: 4.5688\tLR: 0.010000\n",
            "Training Epoch: 12 [25088/50000]\tLoss: 4.5221\tLR: 0.010000\n",
            "Training Epoch: 12 [25216/50000]\tLoss: 4.5386\tLR: 0.010000\n",
            "Training Epoch: 12 [25344/50000]\tLoss: 4.5013\tLR: 0.010000\n",
            "Training Epoch: 12 [25472/50000]\tLoss: 4.5207\tLR: 0.010000\n",
            "Training Epoch: 12 [25600/50000]\tLoss: 4.5751\tLR: 0.010000\n",
            "Training Epoch: 12 [25728/50000]\tLoss: 4.4985\tLR: 0.010000\n",
            "Training Epoch: 12 [25856/50000]\tLoss: 4.4534\tLR: 0.010000\n",
            "Training Epoch: 12 [25984/50000]\tLoss: 4.5568\tLR: 0.010000\n",
            "Training Epoch: 12 [26112/50000]\tLoss: 4.5049\tLR: 0.010000\n",
            "Training Epoch: 12 [26240/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 12 [26368/50000]\tLoss: 4.5284\tLR: 0.010000\n",
            "Training Epoch: 12 [26496/50000]\tLoss: 4.5090\tLR: 0.010000\n",
            "Training Epoch: 12 [26624/50000]\tLoss: 4.5515\tLR: 0.010000\n",
            "Training Epoch: 12 [26752/50000]\tLoss: 4.4894\tLR: 0.010000\n",
            "Training Epoch: 12 [26880/50000]\tLoss: 4.4280\tLR: 0.010000\n",
            "Training Epoch: 12 [27008/50000]\tLoss: 4.5116\tLR: 0.010000\n",
            "Training Epoch: 12 [27136/50000]\tLoss: 4.5068\tLR: 0.010000\n",
            "Training Epoch: 12 [27264/50000]\tLoss: 4.4025\tLR: 0.010000\n",
            "Training Epoch: 12 [27392/50000]\tLoss: 4.5049\tLR: 0.010000\n",
            "Training Epoch: 12 [27520/50000]\tLoss: 4.4646\tLR: 0.010000\n",
            "Training Epoch: 12 [27648/50000]\tLoss: 4.5939\tLR: 0.010000\n",
            "Training Epoch: 12 [27776/50000]\tLoss: 4.4736\tLR: 0.010000\n",
            "Training Epoch: 12 [27904/50000]\tLoss: 4.4976\tLR: 0.010000\n",
            "Training Epoch: 12 [28032/50000]\tLoss: 4.5266\tLR: 0.010000\n",
            "Training Epoch: 12 [28160/50000]\tLoss: 4.4624\tLR: 0.010000\n",
            "Training Epoch: 12 [28288/50000]\tLoss: 4.5494\tLR: 0.010000\n",
            "Training Epoch: 12 [28416/50000]\tLoss: 4.5121\tLR: 0.010000\n",
            "Training Epoch: 12 [28544/50000]\tLoss: 4.4441\tLR: 0.010000\n",
            "Training Epoch: 12 [28672/50000]\tLoss: 4.4768\tLR: 0.010000\n",
            "Training Epoch: 12 [28800/50000]\tLoss: 4.4824\tLR: 0.010000\n",
            "Training Epoch: 12 [28928/50000]\tLoss: 4.5380\tLR: 0.010000\n",
            "Training Epoch: 12 [29056/50000]\tLoss: 4.5054\tLR: 0.010000\n",
            "Training Epoch: 12 [29184/50000]\tLoss: 4.4988\tLR: 0.010000\n",
            "Training Epoch: 12 [29312/50000]\tLoss: 4.4822\tLR: 0.010000\n",
            "Training Epoch: 12 [29440/50000]\tLoss: 4.5078\tLR: 0.010000\n",
            "Training Epoch: 12 [29568/50000]\tLoss: 4.5924\tLR: 0.010000\n",
            "Training Epoch: 12 [29696/50000]\tLoss: 4.5424\tLR: 0.010000\n",
            "Training Epoch: 12 [29824/50000]\tLoss: 4.5685\tLR: 0.010000\n",
            "Training Epoch: 12 [29952/50000]\tLoss: 4.5332\tLR: 0.010000\n",
            "Training Epoch: 12 [30080/50000]\tLoss: 4.4548\tLR: 0.010000\n",
            "Training Epoch: 12 [30208/50000]\tLoss: 4.4772\tLR: 0.010000\n",
            "Training Epoch: 12 [30336/50000]\tLoss: 4.5376\tLR: 0.010000\n",
            "Training Epoch: 12 [30464/50000]\tLoss: 4.4670\tLR: 0.010000\n",
            "Training Epoch: 12 [30592/50000]\tLoss: 4.4190\tLR: 0.010000\n",
            "Training Epoch: 12 [30720/50000]\tLoss: 4.5402\tLR: 0.010000\n",
            "Training Epoch: 12 [30848/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 12 [30976/50000]\tLoss: 4.5152\tLR: 0.010000\n",
            "Training Epoch: 12 [31104/50000]\tLoss: 4.4731\tLR: 0.010000\n",
            "Training Epoch: 12 [31232/50000]\tLoss: 4.4956\tLR: 0.010000\n",
            "Training Epoch: 12 [31360/50000]\tLoss: 4.5342\tLR: 0.010000\n",
            "Training Epoch: 12 [31488/50000]\tLoss: 4.5266\tLR: 0.010000\n",
            "Training Epoch: 12 [31616/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 12 [31744/50000]\tLoss: 4.4608\tLR: 0.010000\n",
            "Training Epoch: 12 [31872/50000]\tLoss: 4.5182\tLR: 0.010000\n",
            "Training Epoch: 12 [32000/50000]\tLoss: 4.4593\tLR: 0.010000\n",
            "Training Epoch: 12 [32128/50000]\tLoss: 4.5411\tLR: 0.010000\n",
            "Training Epoch: 12 [32256/50000]\tLoss: 4.5051\tLR: 0.010000\n",
            "Training Epoch: 12 [32384/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 12 [32512/50000]\tLoss: 4.5237\tLR: 0.010000\n",
            "Training Epoch: 12 [32640/50000]\tLoss: 4.5087\tLR: 0.010000\n",
            "Training Epoch: 12 [32768/50000]\tLoss: 4.4436\tLR: 0.010000\n",
            "Training Epoch: 12 [32896/50000]\tLoss: 4.4867\tLR: 0.010000\n",
            "Training Epoch: 12 [33024/50000]\tLoss: 4.5134\tLR: 0.010000\n",
            "Training Epoch: 12 [33152/50000]\tLoss: 4.5044\tLR: 0.010000\n",
            "Training Epoch: 12 [33280/50000]\tLoss: 4.5563\tLR: 0.010000\n",
            "Training Epoch: 12 [33408/50000]\tLoss: 4.5201\tLR: 0.010000\n",
            "Training Epoch: 12 [33536/50000]\tLoss: 4.4563\tLR: 0.010000\n",
            "Training Epoch: 12 [33664/50000]\tLoss: 4.5086\tLR: 0.010000\n",
            "Training Epoch: 12 [33792/50000]\tLoss: 4.5381\tLR: 0.010000\n",
            "Training Epoch: 12 [33920/50000]\tLoss: 4.5091\tLR: 0.010000\n",
            "Training Epoch: 12 [34048/50000]\tLoss: 4.5553\tLR: 0.010000\n",
            "Training Epoch: 12 [34176/50000]\tLoss: 4.4721\tLR: 0.010000\n",
            "Training Epoch: 12 [34304/50000]\tLoss: 4.5099\tLR: 0.010000\n",
            "Training Epoch: 12 [34432/50000]\tLoss: 4.4213\tLR: 0.010000\n",
            "Training Epoch: 12 [34560/50000]\tLoss: 4.5703\tLR: 0.010000\n",
            "Training Epoch: 12 [34688/50000]\tLoss: 4.5078\tLR: 0.010000\n",
            "Training Epoch: 12 [34816/50000]\tLoss: 4.5004\tLR: 0.010000\n",
            "Training Epoch: 12 [34944/50000]\tLoss: 4.5403\tLR: 0.010000\n",
            "Training Epoch: 12 [35072/50000]\tLoss: 4.4890\tLR: 0.010000\n",
            "Training Epoch: 12 [35200/50000]\tLoss: 4.5134\tLR: 0.010000\n",
            "Training Epoch: 12 [35328/50000]\tLoss: 4.4432\tLR: 0.010000\n",
            "Training Epoch: 12 [35456/50000]\tLoss: 4.4870\tLR: 0.010000\n",
            "Training Epoch: 12 [35584/50000]\tLoss: 4.5241\tLR: 0.010000\n",
            "Training Epoch: 12 [35712/50000]\tLoss: 4.5423\tLR: 0.010000\n",
            "Training Epoch: 12 [35840/50000]\tLoss: 4.4599\tLR: 0.010000\n",
            "Training Epoch: 12 [35968/50000]\tLoss: 4.5091\tLR: 0.010000\n",
            "Training Epoch: 12 [36096/50000]\tLoss: 4.5169\tLR: 0.010000\n",
            "Training Epoch: 12 [36224/50000]\tLoss: 4.5108\tLR: 0.010000\n",
            "Training Epoch: 12 [36352/50000]\tLoss: 4.5385\tLR: 0.010000\n",
            "Training Epoch: 12 [36480/50000]\tLoss: 4.5213\tLR: 0.010000\n",
            "Training Epoch: 12 [36608/50000]\tLoss: 4.3666\tLR: 0.010000\n",
            "Training Epoch: 12 [36736/50000]\tLoss: 4.5994\tLR: 0.010000\n",
            "Training Epoch: 12 [36864/50000]\tLoss: 4.4765\tLR: 0.010000\n",
            "Training Epoch: 12 [36992/50000]\tLoss: 4.5471\tLR: 0.010000\n",
            "Training Epoch: 12 [37120/50000]\tLoss: 4.4603\tLR: 0.010000\n",
            "Training Epoch: 12 [37248/50000]\tLoss: 4.4809\tLR: 0.010000\n",
            "Training Epoch: 12 [37376/50000]\tLoss: 4.5528\tLR: 0.010000\n",
            "Training Epoch: 12 [37504/50000]\tLoss: 4.4911\tLR: 0.010000\n",
            "Training Epoch: 12 [37632/50000]\tLoss: 4.4658\tLR: 0.010000\n",
            "Training Epoch: 12 [37760/50000]\tLoss: 4.5060\tLR: 0.010000\n",
            "Training Epoch: 12 [37888/50000]\tLoss: 4.4963\tLR: 0.010000\n",
            "Training Epoch: 12 [38016/50000]\tLoss: 4.4265\tLR: 0.010000\n",
            "Training Epoch: 12 [38144/50000]\tLoss: 4.4933\tLR: 0.010000\n",
            "Training Epoch: 12 [38272/50000]\tLoss: 4.4667\tLR: 0.010000\n",
            "Training Epoch: 12 [38400/50000]\tLoss: 4.5109\tLR: 0.010000\n",
            "Training Epoch: 12 [38528/50000]\tLoss: 4.4824\tLR: 0.010000\n",
            "Training Epoch: 12 [38656/50000]\tLoss: 4.5543\tLR: 0.010000\n",
            "Training Epoch: 12 [38784/50000]\tLoss: 4.5391\tLR: 0.010000\n",
            "Training Epoch: 12 [38912/50000]\tLoss: 4.5482\tLR: 0.010000\n",
            "Training Epoch: 12 [39040/50000]\tLoss: 4.5220\tLR: 0.010000\n",
            "Training Epoch: 12 [39168/50000]\tLoss: 4.4966\tLR: 0.010000\n",
            "Training Epoch: 12 [39296/50000]\tLoss: 4.4788\tLR: 0.010000\n",
            "Training Epoch: 12 [39424/50000]\tLoss: 4.5312\tLR: 0.010000\n",
            "Training Epoch: 12 [39552/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 12 [39680/50000]\tLoss: 4.5561\tLR: 0.010000\n",
            "Training Epoch: 12 [39808/50000]\tLoss: 4.4254\tLR: 0.010000\n",
            "Training Epoch: 12 [39936/50000]\tLoss: 4.4713\tLR: 0.010000\n",
            "Training Epoch: 12 [40064/50000]\tLoss: 4.5432\tLR: 0.010000\n",
            "Training Epoch: 12 [40192/50000]\tLoss: 4.5521\tLR: 0.010000\n",
            "Training Epoch: 12 [40320/50000]\tLoss: 4.4928\tLR: 0.010000\n",
            "Training Epoch: 12 [40448/50000]\tLoss: 4.5209\tLR: 0.010000\n",
            "Training Epoch: 12 [40576/50000]\tLoss: 4.4915\tLR: 0.010000\n",
            "Training Epoch: 12 [40704/50000]\tLoss: 4.5183\tLR: 0.010000\n",
            "Training Epoch: 12 [40832/50000]\tLoss: 4.4630\tLR: 0.010000\n",
            "Training Epoch: 12 [40960/50000]\tLoss: 4.5356\tLR: 0.010000\n",
            "Training Epoch: 12 [41088/50000]\tLoss: 4.5053\tLR: 0.010000\n",
            "Training Epoch: 12 [41216/50000]\tLoss: 4.4788\tLR: 0.010000\n",
            "Training Epoch: 12 [41344/50000]\tLoss: 4.4873\tLR: 0.010000\n",
            "Training Epoch: 12 [41472/50000]\tLoss: 4.5173\tLR: 0.010000\n",
            "Training Epoch: 12 [41600/50000]\tLoss: 4.4265\tLR: 0.010000\n",
            "Training Epoch: 12 [41728/50000]\tLoss: 4.5352\tLR: 0.010000\n",
            "Training Epoch: 12 [41856/50000]\tLoss: 4.5394\tLR: 0.010000\n",
            "Training Epoch: 12 [41984/50000]\tLoss: 4.4870\tLR: 0.010000\n",
            "Training Epoch: 12 [42112/50000]\tLoss: 4.4833\tLR: 0.010000\n",
            "Training Epoch: 12 [42240/50000]\tLoss: 4.5523\tLR: 0.010000\n",
            "Training Epoch: 12 [42368/50000]\tLoss: 4.5178\tLR: 0.010000\n",
            "Training Epoch: 12 [42496/50000]\tLoss: 4.4831\tLR: 0.010000\n",
            "Training Epoch: 12 [42624/50000]\tLoss: 4.4940\tLR: 0.010000\n",
            "Training Epoch: 12 [42752/50000]\tLoss: 4.4984\tLR: 0.010000\n",
            "Training Epoch: 12 [42880/50000]\tLoss: 4.5388\tLR: 0.010000\n",
            "Training Epoch: 12 [43008/50000]\tLoss: 4.5065\tLR: 0.010000\n",
            "Training Epoch: 12 [43136/50000]\tLoss: 4.5245\tLR: 0.010000\n",
            "Training Epoch: 12 [43264/50000]\tLoss: 4.4939\tLR: 0.010000\n",
            "Training Epoch: 12 [43392/50000]\tLoss: 4.4808\tLR: 0.010000\n",
            "Training Epoch: 12 [43520/50000]\tLoss: 4.5185\tLR: 0.010000\n",
            "Training Epoch: 12 [43648/50000]\tLoss: 4.4587\tLR: 0.010000\n",
            "Training Epoch: 12 [43776/50000]\tLoss: 4.5267\tLR: 0.010000\n",
            "Training Epoch: 12 [43904/50000]\tLoss: 4.5663\tLR: 0.010000\n",
            "Training Epoch: 12 [44032/50000]\tLoss: 4.5915\tLR: 0.010000\n",
            "Training Epoch: 12 [44160/50000]\tLoss: 4.4776\tLR: 0.010000\n",
            "Training Epoch: 12 [44288/50000]\tLoss: 4.4690\tLR: 0.010000\n",
            "Training Epoch: 12 [44416/50000]\tLoss: 4.4568\tLR: 0.010000\n",
            "Training Epoch: 12 [44544/50000]\tLoss: 4.4520\tLR: 0.010000\n",
            "Training Epoch: 12 [44672/50000]\tLoss: 4.5458\tLR: 0.010000\n",
            "Training Epoch: 12 [44800/50000]\tLoss: 4.4721\tLR: 0.010000\n",
            "Training Epoch: 12 [44928/50000]\tLoss: 4.5150\tLR: 0.010000\n",
            "Training Epoch: 12 [45056/50000]\tLoss: 4.4570\tLR: 0.010000\n",
            "Training Epoch: 12 [45184/50000]\tLoss: 4.5469\tLR: 0.010000\n",
            "Training Epoch: 12 [45312/50000]\tLoss: 4.4636\tLR: 0.010000\n",
            "Training Epoch: 12 [45440/50000]\tLoss: 4.5123\tLR: 0.010000\n",
            "Training Epoch: 12 [45568/50000]\tLoss: 4.5282\tLR: 0.010000\n",
            "Training Epoch: 12 [45696/50000]\tLoss: 4.5743\tLR: 0.010000\n",
            "Training Epoch: 12 [45824/50000]\tLoss: 4.5538\tLR: 0.010000\n",
            "Training Epoch: 12 [45952/50000]\tLoss: 4.5033\tLR: 0.010000\n",
            "Training Epoch: 12 [46080/50000]\tLoss: 4.4993\tLR: 0.010000\n",
            "Training Epoch: 12 [46208/50000]\tLoss: 4.4708\tLR: 0.010000\n",
            "Training Epoch: 12 [46336/50000]\tLoss: 4.4719\tLR: 0.010000\n",
            "Training Epoch: 12 [46464/50000]\tLoss: 4.5146\tLR: 0.010000\n",
            "Training Epoch: 12 [46592/50000]\tLoss: 4.5051\tLR: 0.010000\n",
            "Training Epoch: 12 [46720/50000]\tLoss: 4.4034\tLR: 0.010000\n",
            "Training Epoch: 12 [46848/50000]\tLoss: 4.4751\tLR: 0.010000\n",
            "Training Epoch: 12 [46976/50000]\tLoss: 4.5197\tLR: 0.010000\n",
            "Training Epoch: 12 [47104/50000]\tLoss: 4.5004\tLR: 0.010000\n",
            "Training Epoch: 12 [47232/50000]\tLoss: 4.4432\tLR: 0.010000\n",
            "Training Epoch: 12 [47360/50000]\tLoss: 4.5145\tLR: 0.010000\n",
            "Training Epoch: 12 [47488/50000]\tLoss: 4.5661\tLR: 0.010000\n",
            "Training Epoch: 12 [47616/50000]\tLoss: 4.5098\tLR: 0.010000\n",
            "Training Epoch: 12 [47744/50000]\tLoss: 4.4362\tLR: 0.010000\n",
            "Training Epoch: 12 [47872/50000]\tLoss: 4.5041\tLR: 0.010000\n",
            "Training Epoch: 12 [48000/50000]\tLoss: 4.5912\tLR: 0.010000\n",
            "Training Epoch: 12 [48128/50000]\tLoss: 4.5192\tLR: 0.010000\n",
            "Training Epoch: 12 [48256/50000]\tLoss: 4.5451\tLR: 0.010000\n",
            "Training Epoch: 12 [48384/50000]\tLoss: 4.4763\tLR: 0.010000\n",
            "Training Epoch: 12 [48512/50000]\tLoss: 4.4653\tLR: 0.010000\n",
            "Training Epoch: 12 [48640/50000]\tLoss: 4.5808\tLR: 0.010000\n",
            "Training Epoch: 12 [48768/50000]\tLoss: 4.4858\tLR: 0.010000\n",
            "Training Epoch: 12 [48896/50000]\tLoss: 4.5090\tLR: 0.010000\n",
            "Training Epoch: 12 [49024/50000]\tLoss: 4.5105\tLR: 0.010000\n",
            "Training Epoch: 12 [49152/50000]\tLoss: 4.5096\tLR: 0.010000\n",
            "Training Epoch: 12 [49280/50000]\tLoss: 4.5200\tLR: 0.010000\n",
            "Training Epoch: 12 [49408/50000]\tLoss: 4.5321\tLR: 0.010000\n",
            "Training Epoch: 12 [49536/50000]\tLoss: 4.4434\tLR: 0.010000\n",
            "Training Epoch: 12 [49664/50000]\tLoss: 4.4809\tLR: 0.010000\n",
            "Training Epoch: 12 [49792/50000]\tLoss: 4.5151\tLR: 0.010000\n",
            "Training Epoch: 12 [49920/50000]\tLoss: 4.5077\tLR: 0.010000\n",
            "Training Epoch: 12 [50000/50000]\tLoss: 4.5522\tLR: 0.010000\n",
            "epoch 12 training time consumed: 145.77s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 127386 GiB | 127386 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 125779 GiB | 125779 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1607 GiB |   1607 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 127386 GiB | 127386 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 125779 GiB | 125779 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1607 GiB |   1607 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 127038 GiB | 127038 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 125431 GiB | 125431 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1607 GiB |   1606 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 105776 GiB | 105776 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 103960 GiB | 103960 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   1815 GiB |   1815 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    7762 K  |    7761 K  |\n",
            "|       from large pool |       5    |     146    |    3762 K  |    3762 K  |\n",
            "|       from small pool |     516    |     682    |    3999 K  |    3999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    7762 K  |    7761 K  |\n",
            "|       from large pool |       5    |     146    |    3762 K  |    3762 K  |\n",
            "|       from small pool |     516    |     682    |    3999 K  |    3999 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |     117    |    2893 K  |    2893 K  |\n",
            "|       from large pool |       4    |      46    |    1775 K  |    1775 K  |\n",
            "|       from small pool |      76    |      89    |    1118 K  |    1118 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0358, Accuracy: 0.0214, Time consumed:9.35s\n",
            "\n",
            "Training Epoch: 13 [128/50000]\tLoss: 4.4616\tLR: 0.010000\n",
            "Training Epoch: 13 [256/50000]\tLoss: 4.4880\tLR: 0.010000\n",
            "Training Epoch: 13 [384/50000]\tLoss: 4.5491\tLR: 0.010000\n",
            "Training Epoch: 13 [512/50000]\tLoss: 4.4952\tLR: 0.010000\n",
            "Training Epoch: 13 [640/50000]\tLoss: 4.5564\tLR: 0.010000\n",
            "Training Epoch: 13 [768/50000]\tLoss: 4.5332\tLR: 0.010000\n",
            "Training Epoch: 13 [896/50000]\tLoss: 4.4732\tLR: 0.010000\n",
            "Training Epoch: 13 [1024/50000]\tLoss: 4.5117\tLR: 0.010000\n",
            "Training Epoch: 13 [1152/50000]\tLoss: 4.5213\tLR: 0.010000\n",
            "Training Epoch: 13 [1280/50000]\tLoss: 4.5259\tLR: 0.010000\n",
            "Training Epoch: 13 [1408/50000]\tLoss: 4.4669\tLR: 0.010000\n",
            "Training Epoch: 13 [1536/50000]\tLoss: 4.4487\tLR: 0.010000\n",
            "Training Epoch: 13 [1664/50000]\tLoss: 4.5116\tLR: 0.010000\n",
            "Training Epoch: 13 [1792/50000]\tLoss: 4.4876\tLR: 0.010000\n",
            "Training Epoch: 13 [1920/50000]\tLoss: 4.5261\tLR: 0.010000\n",
            "Training Epoch: 13 [2048/50000]\tLoss: 4.4962\tLR: 0.010000\n",
            "Training Epoch: 13 [2176/50000]\tLoss: 4.5184\tLR: 0.010000\n",
            "Training Epoch: 13 [2304/50000]\tLoss: 4.5079\tLR: 0.010000\n",
            "Training Epoch: 13 [2432/50000]\tLoss: 4.4945\tLR: 0.010000\n",
            "Training Epoch: 13 [2560/50000]\tLoss: 4.4772\tLR: 0.010000\n",
            "Training Epoch: 13 [2688/50000]\tLoss: 4.4679\tLR: 0.010000\n",
            "Training Epoch: 13 [2816/50000]\tLoss: 4.5455\tLR: 0.010000\n",
            "Training Epoch: 13 [2944/50000]\tLoss: 4.5175\tLR: 0.010000\n",
            "Training Epoch: 13 [3072/50000]\tLoss: 4.4930\tLR: 0.010000\n",
            "Training Epoch: 13 [3200/50000]\tLoss: 4.4921\tLR: 0.010000\n",
            "Training Epoch: 13 [3328/50000]\tLoss: 4.4923\tLR: 0.010000\n",
            "Training Epoch: 13 [3456/50000]\tLoss: 4.4910\tLR: 0.010000\n",
            "Training Epoch: 13 [3584/50000]\tLoss: 4.5103\tLR: 0.010000\n",
            "Training Epoch: 13 [3712/50000]\tLoss: 4.4565\tLR: 0.010000\n",
            "Training Epoch: 13 [3840/50000]\tLoss: 4.5074\tLR: 0.010000\n",
            "Training Epoch: 13 [3968/50000]\tLoss: 4.5267\tLR: 0.010000\n",
            "Training Epoch: 13 [4096/50000]\tLoss: 4.5118\tLR: 0.010000\n",
            "Training Epoch: 13 [4224/50000]\tLoss: 4.4250\tLR: 0.010000\n",
            "Training Epoch: 13 [4352/50000]\tLoss: 4.4846\tLR: 0.010000\n",
            "Training Epoch: 13 [4480/50000]\tLoss: 4.4507\tLR: 0.010000\n",
            "Training Epoch: 13 [4608/50000]\tLoss: 4.4843\tLR: 0.010000\n",
            "Training Epoch: 13 [4736/50000]\tLoss: 4.4604\tLR: 0.010000\n",
            "Training Epoch: 13 [4864/50000]\tLoss: 4.5147\tLR: 0.010000\n",
            "Training Epoch: 13 [4992/50000]\tLoss: 4.5036\tLR: 0.010000\n",
            "Training Epoch: 13 [5120/50000]\tLoss: 4.5183\tLR: 0.010000\n",
            "Training Epoch: 13 [5248/50000]\tLoss: 4.5052\tLR: 0.010000\n",
            "Training Epoch: 13 [5376/50000]\tLoss: 4.4821\tLR: 0.010000\n",
            "Training Epoch: 13 [5504/50000]\tLoss: 4.5066\tLR: 0.010000\n",
            "Training Epoch: 13 [5632/50000]\tLoss: 4.3932\tLR: 0.010000\n",
            "Training Epoch: 13 [5760/50000]\tLoss: 4.5044\tLR: 0.010000\n",
            "Training Epoch: 13 [5888/50000]\tLoss: 4.5159\tLR: 0.010000\n",
            "Training Epoch: 13 [6016/50000]\tLoss: 4.5738\tLR: 0.010000\n",
            "Training Epoch: 13 [6144/50000]\tLoss: 4.4840\tLR: 0.010000\n",
            "Training Epoch: 13 [6272/50000]\tLoss: 4.5330\tLR: 0.010000\n",
            "Training Epoch: 13 [6400/50000]\tLoss: 4.5092\tLR: 0.010000\n",
            "Training Epoch: 13 [6528/50000]\tLoss: 4.4423\tLR: 0.010000\n",
            "Training Epoch: 13 [6656/50000]\tLoss: 4.4786\tLR: 0.010000\n",
            "Training Epoch: 13 [6784/50000]\tLoss: 4.4980\tLR: 0.010000\n",
            "Training Epoch: 13 [6912/50000]\tLoss: 4.4730\tLR: 0.010000\n",
            "Training Epoch: 13 [7040/50000]\tLoss: 4.5595\tLR: 0.010000\n",
            "Training Epoch: 13 [7168/50000]\tLoss: 4.4796\tLR: 0.010000\n",
            "Training Epoch: 13 [7296/50000]\tLoss: 4.4981\tLR: 0.010000\n",
            "Training Epoch: 13 [7424/50000]\tLoss: 4.5229\tLR: 0.010000\n",
            "Training Epoch: 13 [7552/50000]\tLoss: 4.5091\tLR: 0.010000\n",
            "Training Epoch: 13 [7680/50000]\tLoss: 4.5365\tLR: 0.010000\n",
            "Training Epoch: 13 [7808/50000]\tLoss: 4.5120\tLR: 0.010000\n",
            "Training Epoch: 13 [7936/50000]\tLoss: 4.4792\tLR: 0.010000\n",
            "Training Epoch: 13 [8064/50000]\tLoss: 4.5168\tLR: 0.010000\n",
            "Training Epoch: 13 [8192/50000]\tLoss: 4.4870\tLR: 0.010000\n",
            "Training Epoch: 13 [8320/50000]\tLoss: 4.4455\tLR: 0.010000\n",
            "Training Epoch: 13 [8448/50000]\tLoss: 4.4736\tLR: 0.010000\n",
            "Training Epoch: 13 [8576/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 13 [8704/50000]\tLoss: 4.5806\tLR: 0.010000\n",
            "Training Epoch: 13 [8832/50000]\tLoss: 4.4688\tLR: 0.010000\n",
            "Training Epoch: 13 [8960/50000]\tLoss: 4.4675\tLR: 0.010000\n",
            "Training Epoch: 13 [9088/50000]\tLoss: 4.4429\tLR: 0.010000\n",
            "Training Epoch: 13 [9216/50000]\tLoss: 4.4791\tLR: 0.010000\n",
            "Training Epoch: 13 [9344/50000]\tLoss: 4.5661\tLR: 0.010000\n",
            "Training Epoch: 13 [9472/50000]\tLoss: 4.5284\tLR: 0.010000\n",
            "Training Epoch: 13 [9600/50000]\tLoss: 4.5072\tLR: 0.010000\n",
            "Training Epoch: 13 [9728/50000]\tLoss: 4.4925\tLR: 0.010000\n",
            "Training Epoch: 13 [9856/50000]\tLoss: 4.4890\tLR: 0.010000\n",
            "Training Epoch: 13 [9984/50000]\tLoss: 4.5173\tLR: 0.010000\n",
            "Training Epoch: 13 [10112/50000]\tLoss: 4.4947\tLR: 0.010000\n",
            "Training Epoch: 13 [10240/50000]\tLoss: 4.4510\tLR: 0.010000\n",
            "Training Epoch: 13 [10368/50000]\tLoss: 4.4673\tLR: 0.010000\n",
            "Training Epoch: 13 [10496/50000]\tLoss: 4.5255\tLR: 0.010000\n",
            "Training Epoch: 13 [10624/50000]\tLoss: 4.4992\tLR: 0.010000\n",
            "Training Epoch: 13 [10752/50000]\tLoss: 4.5279\tLR: 0.010000\n",
            "Training Epoch: 13 [10880/50000]\tLoss: 4.5056\tLR: 0.010000\n",
            "Training Epoch: 13 [11008/50000]\tLoss: 4.5426\tLR: 0.010000\n",
            "Training Epoch: 13 [11136/50000]\tLoss: 4.4867\tLR: 0.010000\n",
            "Training Epoch: 13 [11264/50000]\tLoss: 4.5039\tLR: 0.010000\n",
            "Training Epoch: 13 [11392/50000]\tLoss: 4.5044\tLR: 0.010000\n",
            "Training Epoch: 13 [11520/50000]\tLoss: 4.5283\tLR: 0.010000\n",
            "Training Epoch: 13 [11648/50000]\tLoss: 4.5122\tLR: 0.010000\n",
            "Training Epoch: 13 [11776/50000]\tLoss: 4.4909\tLR: 0.010000\n",
            "Training Epoch: 13 [11904/50000]\tLoss: 4.4769\tLR: 0.010000\n",
            "Training Epoch: 13 [12032/50000]\tLoss: 4.5067\tLR: 0.010000\n",
            "Training Epoch: 13 [12160/50000]\tLoss: 4.5180\tLR: 0.010000\n",
            "Training Epoch: 13 [12288/50000]\tLoss: 4.5332\tLR: 0.010000\n",
            "Training Epoch: 13 [12416/50000]\tLoss: 4.4646\tLR: 0.010000\n",
            "Training Epoch: 13 [12544/50000]\tLoss: 4.5145\tLR: 0.010000\n",
            "Training Epoch: 13 [12672/50000]\tLoss: 4.5730\tLR: 0.010000\n",
            "Training Epoch: 13 [12800/50000]\tLoss: 4.4776\tLR: 0.010000\n",
            "Training Epoch: 13 [12928/50000]\tLoss: 4.4677\tLR: 0.010000\n",
            "Training Epoch: 13 [13056/50000]\tLoss: 4.5232\tLR: 0.010000\n",
            "Training Epoch: 13 [13184/50000]\tLoss: 4.4836\tLR: 0.010000\n",
            "Training Epoch: 13 [13312/50000]\tLoss: 4.5032\tLR: 0.010000\n",
            "Training Epoch: 13 [13440/50000]\tLoss: 4.5048\tLR: 0.010000\n",
            "Training Epoch: 13 [13568/50000]\tLoss: 4.5307\tLR: 0.010000\n",
            "Training Epoch: 13 [13696/50000]\tLoss: 4.4488\tLR: 0.010000\n",
            "Training Epoch: 13 [13824/50000]\tLoss: 4.4754\tLR: 0.010000\n",
            "Training Epoch: 13 [13952/50000]\tLoss: 4.4226\tLR: 0.010000\n",
            "Training Epoch: 13 [14080/50000]\tLoss: 4.4997\tLR: 0.010000\n",
            "Training Epoch: 13 [14208/50000]\tLoss: 4.4668\tLR: 0.010000\n",
            "Training Epoch: 13 [14336/50000]\tLoss: 4.4998\tLR: 0.010000\n",
            "Training Epoch: 13 [14464/50000]\tLoss: 4.5197\tLR: 0.010000\n",
            "Training Epoch: 13 [14592/50000]\tLoss: 4.4657\tLR: 0.010000\n",
            "Training Epoch: 13 [14720/50000]\tLoss: 4.5107\tLR: 0.010000\n",
            "Training Epoch: 13 [14848/50000]\tLoss: 4.4950\tLR: 0.010000\n",
            "Training Epoch: 13 [14976/50000]\tLoss: 4.4336\tLR: 0.010000\n",
            "Training Epoch: 13 [15104/50000]\tLoss: 4.5397\tLR: 0.010000\n",
            "Training Epoch: 13 [15232/50000]\tLoss: 4.5895\tLR: 0.010000\n",
            "Training Epoch: 13 [15360/50000]\tLoss: 4.5003\tLR: 0.010000\n",
            "Training Epoch: 13 [15488/50000]\tLoss: 4.4934\tLR: 0.010000\n",
            "Training Epoch: 13 [15616/50000]\tLoss: 4.5062\tLR: 0.010000\n",
            "Training Epoch: 13 [15744/50000]\tLoss: 4.4750\tLR: 0.010000\n",
            "Training Epoch: 13 [15872/50000]\tLoss: 4.4525\tLR: 0.010000\n",
            "Training Epoch: 13 [16000/50000]\tLoss: 4.5264\tLR: 0.010000\n",
            "Training Epoch: 13 [16128/50000]\tLoss: 4.4904\tLR: 0.010000\n",
            "Training Epoch: 13 [16256/50000]\tLoss: 4.5550\tLR: 0.010000\n",
            "Training Epoch: 13 [16384/50000]\tLoss: 4.5431\tLR: 0.010000\n",
            "Training Epoch: 13 [16512/50000]\tLoss: 4.4335\tLR: 0.010000\n",
            "Training Epoch: 13 [16640/50000]\tLoss: 4.4806\tLR: 0.010000\n",
            "Training Epoch: 13 [16768/50000]\tLoss: 4.5431\tLR: 0.010000\n",
            "Training Epoch: 13 [16896/50000]\tLoss: 4.4589\tLR: 0.010000\n",
            "Training Epoch: 13 [17024/50000]\tLoss: 4.5199\tLR: 0.010000\n",
            "Training Epoch: 13 [17152/50000]\tLoss: 4.5527\tLR: 0.010000\n",
            "Training Epoch: 13 [17280/50000]\tLoss: 4.4484\tLR: 0.010000\n",
            "Training Epoch: 13 [17408/50000]\tLoss: 4.4736\tLR: 0.010000\n",
            "Training Epoch: 13 [17536/50000]\tLoss: 4.5077\tLR: 0.010000\n",
            "Training Epoch: 13 [17664/50000]\tLoss: 4.4975\tLR: 0.010000\n",
            "Training Epoch: 13 [17792/50000]\tLoss: 4.5004\tLR: 0.010000\n",
            "Training Epoch: 13 [17920/50000]\tLoss: 4.4811\tLR: 0.010000\n",
            "Training Epoch: 13 [18048/50000]\tLoss: 4.4941\tLR: 0.010000\n",
            "Training Epoch: 13 [18176/50000]\tLoss: 4.4820\tLR: 0.010000\n",
            "Training Epoch: 13 [18304/50000]\tLoss: 4.4800\tLR: 0.010000\n",
            "Training Epoch: 13 [18432/50000]\tLoss: 4.4948\tLR: 0.010000\n",
            "Training Epoch: 13 [18560/50000]\tLoss: 4.5035\tLR: 0.010000\n",
            "Training Epoch: 13 [18688/50000]\tLoss: 4.5054\tLR: 0.010000\n",
            "Training Epoch: 13 [18816/50000]\tLoss: 4.4896\tLR: 0.010000\n",
            "Training Epoch: 13 [18944/50000]\tLoss: 4.4584\tLR: 0.010000\n",
            "Training Epoch: 13 [19072/50000]\tLoss: 4.5178\tLR: 0.010000\n",
            "Training Epoch: 13 [19200/50000]\tLoss: 4.4185\tLR: 0.010000\n",
            "Training Epoch: 13 [19328/50000]\tLoss: 4.4424\tLR: 0.010000\n",
            "Training Epoch: 13 [19456/50000]\tLoss: 4.4903\tLR: 0.010000\n",
            "Training Epoch: 13 [19584/50000]\tLoss: 4.5037\tLR: 0.010000\n",
            "Training Epoch: 13 [19712/50000]\tLoss: 4.5032\tLR: 0.010000\n",
            "Training Epoch: 13 [19840/50000]\tLoss: 4.4752\tLR: 0.010000\n",
            "Training Epoch: 13 [19968/50000]\tLoss: 4.5014\tLR: 0.010000\n",
            "Training Epoch: 13 [20096/50000]\tLoss: 4.5048\tLR: 0.010000\n",
            "Training Epoch: 13 [20224/50000]\tLoss: 4.5652\tLR: 0.010000\n",
            "Training Epoch: 13 [20352/50000]\tLoss: 4.5236\tLR: 0.010000\n",
            "Training Epoch: 13 [20480/50000]\tLoss: 4.4223\tLR: 0.010000\n",
            "Training Epoch: 13 [20608/50000]\tLoss: 4.5374\tLR: 0.010000\n",
            "Training Epoch: 13 [20736/50000]\tLoss: 4.5531\tLR: 0.010000\n",
            "Training Epoch: 13 [20864/50000]\tLoss: 4.4941\tLR: 0.010000\n",
            "Training Epoch: 13 [20992/50000]\tLoss: 4.5431\tLR: 0.010000\n",
            "Training Epoch: 13 [21120/50000]\tLoss: 4.4820\tLR: 0.010000\n",
            "Training Epoch: 13 [21248/50000]\tLoss: 4.5054\tLR: 0.010000\n",
            "Training Epoch: 13 [21376/50000]\tLoss: 4.5186\tLR: 0.010000\n",
            "Training Epoch: 13 [21504/50000]\tLoss: 4.4870\tLR: 0.010000\n",
            "Training Epoch: 13 [21632/50000]\tLoss: 4.5439\tLR: 0.010000\n",
            "Training Epoch: 13 [21760/50000]\tLoss: 4.4785\tLR: 0.010000\n",
            "Training Epoch: 13 [21888/50000]\tLoss: 4.5327\tLR: 0.010000\n",
            "Training Epoch: 13 [22016/50000]\tLoss: 4.5225\tLR: 0.010000\n",
            "Training Epoch: 13 [22144/50000]\tLoss: 4.5173\tLR: 0.010000\n",
            "Training Epoch: 13 [22272/50000]\tLoss: 4.4705\tLR: 0.010000\n",
            "Training Epoch: 13 [22400/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 13 [22528/50000]\tLoss: 4.5573\tLR: 0.010000\n",
            "Training Epoch: 13 [22656/50000]\tLoss: 4.4725\tLR: 0.010000\n",
            "Training Epoch: 13 [22784/50000]\tLoss: 4.4463\tLR: 0.010000\n",
            "Training Epoch: 13 [22912/50000]\tLoss: 4.4757\tLR: 0.010000\n",
            "Training Epoch: 13 [23040/50000]\tLoss: 4.5306\tLR: 0.010000\n",
            "Training Epoch: 13 [23168/50000]\tLoss: 4.5341\tLR: 0.010000\n",
            "Training Epoch: 13 [23296/50000]\tLoss: 4.5494\tLR: 0.010000\n",
            "Training Epoch: 13 [23424/50000]\tLoss: 4.5038\tLR: 0.010000\n",
            "Training Epoch: 13 [23552/50000]\tLoss: 4.4507\tLR: 0.010000\n",
            "Training Epoch: 13 [23680/50000]\tLoss: 4.5114\tLR: 0.010000\n",
            "Training Epoch: 13 [23808/50000]\tLoss: 4.5040\tLR: 0.010000\n",
            "Training Epoch: 13 [23936/50000]\tLoss: 4.4867\tLR: 0.010000\n",
            "Training Epoch: 13 [24064/50000]\tLoss: 4.5028\tLR: 0.010000\n",
            "Training Epoch: 13 [24192/50000]\tLoss: 4.5247\tLR: 0.010000\n",
            "Training Epoch: 13 [24320/50000]\tLoss: 4.4351\tLR: 0.010000\n",
            "Training Epoch: 13 [24448/50000]\tLoss: 4.5569\tLR: 0.010000\n",
            "Training Epoch: 13 [24576/50000]\tLoss: 4.4624\tLR: 0.010000\n",
            "Training Epoch: 13 [24704/50000]\tLoss: 4.4588\tLR: 0.010000\n",
            "Training Epoch: 13 [24832/50000]\tLoss: 4.5144\tLR: 0.010000\n",
            "Training Epoch: 13 [24960/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 13 [25088/50000]\tLoss: 4.5411\tLR: 0.010000\n",
            "Training Epoch: 13 [25216/50000]\tLoss: 4.5629\tLR: 0.010000\n",
            "Training Epoch: 13 [25344/50000]\tLoss: 4.4325\tLR: 0.010000\n",
            "Training Epoch: 13 [25472/50000]\tLoss: 4.4976\tLR: 0.010000\n",
            "Training Epoch: 13 [25600/50000]\tLoss: 4.4655\tLR: 0.010000\n",
            "Training Epoch: 13 [25728/50000]\tLoss: 4.4133\tLR: 0.010000\n",
            "Training Epoch: 13 [25856/50000]\tLoss: 4.4330\tLR: 0.010000\n",
            "Training Epoch: 13 [25984/50000]\tLoss: 4.4597\tLR: 0.010000\n",
            "Training Epoch: 13 [26112/50000]\tLoss: 4.4780\tLR: 0.010000\n",
            "Training Epoch: 13 [26240/50000]\tLoss: 4.4517\tLR: 0.010000\n",
            "Training Epoch: 13 [26368/50000]\tLoss: 4.4969\tLR: 0.010000\n",
            "Training Epoch: 13 [26496/50000]\tLoss: 4.4952\tLR: 0.010000\n",
            "Training Epoch: 13 [26624/50000]\tLoss: 4.4603\tLR: 0.010000\n",
            "Training Epoch: 13 [26752/50000]\tLoss: 4.5107\tLR: 0.010000\n",
            "Training Epoch: 13 [26880/50000]\tLoss: 4.5592\tLR: 0.010000\n",
            "Training Epoch: 13 [27008/50000]\tLoss: 4.5615\tLR: 0.010000\n",
            "Training Epoch: 13 [27136/50000]\tLoss: 4.5111\tLR: 0.010000\n",
            "Training Epoch: 13 [27264/50000]\tLoss: 4.5389\tLR: 0.010000\n",
            "Training Epoch: 13 [27392/50000]\tLoss: 4.5173\tLR: 0.010000\n",
            "Training Epoch: 13 [27520/50000]\tLoss: 4.5005\tLR: 0.010000\n",
            "Training Epoch: 13 [27648/50000]\tLoss: 4.3944\tLR: 0.010000\n",
            "Training Epoch: 13 [27776/50000]\tLoss: 4.4599\tLR: 0.010000\n",
            "Training Epoch: 13 [27904/50000]\tLoss: 4.5045\tLR: 0.010000\n",
            "Training Epoch: 13 [28032/50000]\tLoss: 4.4914\tLR: 0.010000\n",
            "Training Epoch: 13 [28160/50000]\tLoss: 4.4978\tLR: 0.010000\n",
            "Training Epoch: 13 [28288/50000]\tLoss: 4.4869\tLR: 0.010000\n",
            "Training Epoch: 13 [28416/50000]\tLoss: 4.4571\tLR: 0.010000\n",
            "Training Epoch: 13 [28544/50000]\tLoss: 4.4957\tLR: 0.010000\n",
            "Training Epoch: 13 [28672/50000]\tLoss: 4.5680\tLR: 0.010000\n",
            "Training Epoch: 13 [28800/50000]\tLoss: 4.4500\tLR: 0.010000\n",
            "Training Epoch: 13 [28928/50000]\tLoss: 4.4555\tLR: 0.010000\n",
            "Training Epoch: 13 [29056/50000]\tLoss: 4.5234\tLR: 0.010000\n",
            "Training Epoch: 13 [29184/50000]\tLoss: 4.4267\tLR: 0.010000\n",
            "Training Epoch: 13 [29312/50000]\tLoss: 4.4772\tLR: 0.010000\n",
            "Training Epoch: 13 [29440/50000]\tLoss: 4.4783\tLR: 0.010000\n",
            "Training Epoch: 13 [29568/50000]\tLoss: 4.4494\tLR: 0.010000\n",
            "Training Epoch: 13 [29696/50000]\tLoss: 4.4818\tLR: 0.010000\n",
            "Training Epoch: 13 [29824/50000]\tLoss: 4.4316\tLR: 0.010000\n",
            "Training Epoch: 13 [29952/50000]\tLoss: 4.4339\tLR: 0.010000\n",
            "Training Epoch: 13 [30080/50000]\tLoss: 4.5943\tLR: 0.010000\n",
            "Training Epoch: 13 [30208/50000]\tLoss: 4.4579\tLR: 0.010000\n",
            "Training Epoch: 13 [30336/50000]\tLoss: 4.4764\tLR: 0.010000\n",
            "Training Epoch: 13 [30464/50000]\tLoss: 4.5103\tLR: 0.010000\n",
            "Training Epoch: 13 [30592/50000]\tLoss: 4.4515\tLR: 0.010000\n",
            "Training Epoch: 13 [30720/50000]\tLoss: 4.4865\tLR: 0.010000\n",
            "Training Epoch: 13 [30848/50000]\tLoss: 4.4820\tLR: 0.010000\n",
            "Training Epoch: 13 [30976/50000]\tLoss: 4.4765\tLR: 0.010000\n",
            "Training Epoch: 13 [31104/50000]\tLoss: 4.4954\tLR: 0.010000\n",
            "Training Epoch: 13 [31232/50000]\tLoss: 4.5221\tLR: 0.010000\n",
            "Training Epoch: 13 [31360/50000]\tLoss: 4.4674\tLR: 0.010000\n",
            "Training Epoch: 13 [31488/50000]\tLoss: 4.4178\tLR: 0.010000\n",
            "Training Epoch: 13 [31616/50000]\tLoss: 4.4740\tLR: 0.010000\n",
            "Training Epoch: 13 [31744/50000]\tLoss: 4.5142\tLR: 0.010000\n",
            "Training Epoch: 13 [31872/50000]\tLoss: 4.4937\tLR: 0.010000\n",
            "Training Epoch: 13 [32000/50000]\tLoss: 4.5502\tLR: 0.010000\n",
            "Training Epoch: 13 [32128/50000]\tLoss: 4.4651\tLR: 0.010000\n",
            "Training Epoch: 13 [32256/50000]\tLoss: 4.5044\tLR: 0.010000\n",
            "Training Epoch: 13 [32384/50000]\tLoss: 4.5571\tLR: 0.010000\n",
            "Training Epoch: 13 [32512/50000]\tLoss: 4.5177\tLR: 0.010000\n",
            "Training Epoch: 13 [32640/50000]\tLoss: 4.3942\tLR: 0.010000\n",
            "Training Epoch: 13 [32768/50000]\tLoss: 4.4782\tLR: 0.010000\n",
            "Training Epoch: 13 [32896/50000]\tLoss: 4.5342\tLR: 0.010000\n",
            "Training Epoch: 13 [33024/50000]\tLoss: 4.5421\tLR: 0.010000\n",
            "Training Epoch: 13 [33152/50000]\tLoss: 4.4763\tLR: 0.010000\n",
            "Training Epoch: 13 [33280/50000]\tLoss: 4.5228\tLR: 0.010000\n",
            "Training Epoch: 13 [33408/50000]\tLoss: 4.4702\tLR: 0.010000\n",
            "Training Epoch: 13 [33536/50000]\tLoss: 4.5063\tLR: 0.010000\n",
            "Training Epoch: 13 [33664/50000]\tLoss: 4.5037\tLR: 0.010000\n",
            "Training Epoch: 13 [33792/50000]\tLoss: 4.4133\tLR: 0.010000\n",
            "Training Epoch: 13 [33920/50000]\tLoss: 4.5250\tLR: 0.010000\n",
            "Training Epoch: 13 [34048/50000]\tLoss: 4.4100\tLR: 0.010000\n",
            "Training Epoch: 13 [34176/50000]\tLoss: 4.5016\tLR: 0.010000\n",
            "Training Epoch: 13 [34304/50000]\tLoss: 4.4473\tLR: 0.010000\n",
            "Training Epoch: 13 [34432/50000]\tLoss: 4.4258\tLR: 0.010000\n",
            "Training Epoch: 13 [34560/50000]\tLoss: 4.4861\tLR: 0.010000\n",
            "Training Epoch: 13 [34688/50000]\tLoss: 4.4719\tLR: 0.010000\n",
            "Training Epoch: 13 [34816/50000]\tLoss: 4.5096\tLR: 0.010000\n",
            "Training Epoch: 13 [34944/50000]\tLoss: 4.4537\tLR: 0.010000\n",
            "Training Epoch: 13 [35072/50000]\tLoss: 4.5188\tLR: 0.010000\n",
            "Training Epoch: 13 [35200/50000]\tLoss: 4.4807\tLR: 0.010000\n",
            "Training Epoch: 13 [35328/50000]\tLoss: 4.4671\tLR: 0.010000\n",
            "Training Epoch: 13 [35456/50000]\tLoss: 4.4570\tLR: 0.010000\n",
            "Training Epoch: 13 [35584/50000]\tLoss: 4.4135\tLR: 0.010000\n",
            "Training Epoch: 13 [35712/50000]\tLoss: 4.5692\tLR: 0.010000\n",
            "Training Epoch: 13 [35840/50000]\tLoss: 4.5143\tLR: 0.010000\n",
            "Training Epoch: 13 [35968/50000]\tLoss: 4.4112\tLR: 0.010000\n",
            "Training Epoch: 13 [36096/50000]\tLoss: 4.5833\tLR: 0.010000\n",
            "Training Epoch: 13 [36224/50000]\tLoss: 4.4387\tLR: 0.010000\n",
            "Training Epoch: 13 [36352/50000]\tLoss: 4.4577\tLR: 0.010000\n",
            "Training Epoch: 13 [36480/50000]\tLoss: 4.4032\tLR: 0.010000\n",
            "Training Epoch: 13 [36608/50000]\tLoss: 4.4955\tLR: 0.010000\n",
            "Training Epoch: 13 [36736/50000]\tLoss: 4.4881\tLR: 0.010000\n",
            "Training Epoch: 13 [36864/50000]\tLoss: 4.4655\tLR: 0.010000\n",
            "Training Epoch: 13 [36992/50000]\tLoss: 4.4266\tLR: 0.010000\n",
            "Training Epoch: 13 [37120/50000]\tLoss: 4.5649\tLR: 0.010000\n",
            "Training Epoch: 13 [37248/50000]\tLoss: 4.5536\tLR: 0.010000\n",
            "Training Epoch: 13 [37376/50000]\tLoss: 4.4924\tLR: 0.010000\n",
            "Training Epoch: 13 [37504/50000]\tLoss: 4.4240\tLR: 0.010000\n",
            "Training Epoch: 13 [37632/50000]\tLoss: 4.4829\tLR: 0.010000\n",
            "Training Epoch: 13 [37760/50000]\tLoss: 4.4767\tLR: 0.010000\n",
            "Training Epoch: 13 [37888/50000]\tLoss: 4.4689\tLR: 0.010000\n",
            "Training Epoch: 13 [38016/50000]\tLoss: 4.4458\tLR: 0.010000\n",
            "Training Epoch: 13 [38144/50000]\tLoss: 4.4963\tLR: 0.010000\n",
            "Training Epoch: 13 [38272/50000]\tLoss: 4.4846\tLR: 0.010000\n",
            "Training Epoch: 13 [38400/50000]\tLoss: 4.4686\tLR: 0.010000\n",
            "Training Epoch: 13 [38528/50000]\tLoss: 4.5219\tLR: 0.010000\n",
            "Training Epoch: 13 [38656/50000]\tLoss: 4.4687\tLR: 0.010000\n",
            "Training Epoch: 13 [38784/50000]\tLoss: 4.5332\tLR: 0.010000\n",
            "Training Epoch: 13 [38912/50000]\tLoss: 4.5728\tLR: 0.010000\n",
            "Training Epoch: 13 [39040/50000]\tLoss: 4.4506\tLR: 0.010000\n",
            "Training Epoch: 13 [39168/50000]\tLoss: 4.4620\tLR: 0.010000\n",
            "Training Epoch: 13 [39296/50000]\tLoss: 4.4584\tLR: 0.010000\n",
            "Training Epoch: 13 [39424/50000]\tLoss: 4.4490\tLR: 0.010000\n",
            "Training Epoch: 13 [39552/50000]\tLoss: 4.4510\tLR: 0.010000\n",
            "Training Epoch: 13 [39680/50000]\tLoss: 4.4608\tLR: 0.010000\n",
            "Training Epoch: 13 [39808/50000]\tLoss: 4.5610\tLR: 0.010000\n",
            "Training Epoch: 13 [39936/50000]\tLoss: 4.4438\tLR: 0.010000\n",
            "Training Epoch: 13 [40064/50000]\tLoss: 4.5283\tLR: 0.010000\n",
            "Training Epoch: 13 [40192/50000]\tLoss: 4.4660\tLR: 0.010000\n",
            "Training Epoch: 13 [40320/50000]\tLoss: 4.4778\tLR: 0.010000\n",
            "Training Epoch: 13 [40448/50000]\tLoss: 4.4406\tLR: 0.010000\n",
            "Training Epoch: 13 [40576/50000]\tLoss: 4.4722\tLR: 0.010000\n",
            "Training Epoch: 13 [40704/50000]\tLoss: 4.5101\tLR: 0.010000\n",
            "Training Epoch: 13 [40832/50000]\tLoss: 4.4498\tLR: 0.010000\n",
            "Training Epoch: 13 [40960/50000]\tLoss: 4.5122\tLR: 0.010000\n",
            "Training Epoch: 13 [41088/50000]\tLoss: 4.5114\tLR: 0.010000\n",
            "Training Epoch: 13 [41216/50000]\tLoss: 4.4926\tLR: 0.010000\n",
            "Training Epoch: 13 [41344/50000]\tLoss: 4.4931\tLR: 0.010000\n",
            "Training Epoch: 13 [41472/50000]\tLoss: 4.4402\tLR: 0.010000\n",
            "Training Epoch: 13 [41600/50000]\tLoss: 4.5140\tLR: 0.010000\n",
            "Training Epoch: 13 [41728/50000]\tLoss: 4.5030\tLR: 0.010000\n",
            "Training Epoch: 13 [41856/50000]\tLoss: 4.5185\tLR: 0.010000\n",
            "Training Epoch: 13 [41984/50000]\tLoss: 4.4294\tLR: 0.010000\n",
            "Training Epoch: 13 [42112/50000]\tLoss: 4.4010\tLR: 0.010000\n",
            "Training Epoch: 13 [42240/50000]\tLoss: 4.4459\tLR: 0.010000\n",
            "Training Epoch: 13 [42368/50000]\tLoss: 4.4760\tLR: 0.010000\n",
            "Training Epoch: 13 [42496/50000]\tLoss: 4.4358\tLR: 0.010000\n",
            "Training Epoch: 13 [42624/50000]\tLoss: 4.4297\tLR: 0.010000\n",
            "Training Epoch: 13 [42752/50000]\tLoss: 4.4821\tLR: 0.010000\n",
            "Training Epoch: 13 [42880/50000]\tLoss: 4.5179\tLR: 0.010000\n",
            "Training Epoch: 13 [43008/50000]\tLoss: 4.5098\tLR: 0.010000\n",
            "Training Epoch: 13 [43136/50000]\tLoss: 4.4872\tLR: 0.010000\n",
            "Training Epoch: 13 [43264/50000]\tLoss: 4.5370\tLR: 0.010000\n",
            "Training Epoch: 13 [43392/50000]\tLoss: 4.4925\tLR: 0.010000\n",
            "Training Epoch: 13 [43520/50000]\tLoss: 4.4243\tLR: 0.010000\n",
            "Training Epoch: 13 [43648/50000]\tLoss: 4.5125\tLR: 0.010000\n",
            "Training Epoch: 13 [43776/50000]\tLoss: 4.4947\tLR: 0.010000\n",
            "Training Epoch: 13 [43904/50000]\tLoss: 4.4462\tLR: 0.010000\n",
            "Training Epoch: 13 [44032/50000]\tLoss: 4.5528\tLR: 0.010000\n",
            "Training Epoch: 13 [44160/50000]\tLoss: 4.4462\tLR: 0.010000\n",
            "Training Epoch: 13 [44288/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 13 [44416/50000]\tLoss: 4.4837\tLR: 0.010000\n",
            "Training Epoch: 13 [44544/50000]\tLoss: 4.4373\tLR: 0.010000\n",
            "Training Epoch: 13 [44672/50000]\tLoss: 4.4902\tLR: 0.010000\n",
            "Training Epoch: 13 [44800/50000]\tLoss: 4.4634\tLR: 0.010000\n",
            "Training Epoch: 13 [44928/50000]\tLoss: 4.4800\tLR: 0.010000\n",
            "Training Epoch: 13 [45056/50000]\tLoss: 4.4433\tLR: 0.010000\n",
            "Training Epoch: 13 [45184/50000]\tLoss: 4.4136\tLR: 0.010000\n",
            "Training Epoch: 13 [45312/50000]\tLoss: 4.4537\tLR: 0.010000\n",
            "Training Epoch: 13 [45440/50000]\tLoss: 4.4988\tLR: 0.010000\n",
            "Training Epoch: 13 [45568/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 13 [45696/50000]\tLoss: 4.4843\tLR: 0.010000\n",
            "Training Epoch: 13 [45824/50000]\tLoss: 4.4156\tLR: 0.010000\n",
            "Training Epoch: 13 [45952/50000]\tLoss: 4.4935\tLR: 0.010000\n",
            "Training Epoch: 13 [46080/50000]\tLoss: 4.4482\tLR: 0.010000\n",
            "Training Epoch: 13 [46208/50000]\tLoss: 4.4717\tLR: 0.010000\n",
            "Training Epoch: 13 [46336/50000]\tLoss: 4.3485\tLR: 0.010000\n",
            "Training Epoch: 13 [46464/50000]\tLoss: 4.4391\tLR: 0.010000\n",
            "Training Epoch: 13 [46592/50000]\tLoss: 4.4289\tLR: 0.010000\n",
            "Training Epoch: 13 [46720/50000]\tLoss: 4.4445\tLR: 0.010000\n",
            "Training Epoch: 13 [46848/50000]\tLoss: 4.5079\tLR: 0.010000\n",
            "Training Epoch: 13 [46976/50000]\tLoss: 4.4898\tLR: 0.010000\n",
            "Training Epoch: 13 [47104/50000]\tLoss: 4.5309\tLR: 0.010000\n",
            "Training Epoch: 13 [47232/50000]\tLoss: 4.4490\tLR: 0.010000\n",
            "Training Epoch: 13 [47360/50000]\tLoss: 4.4656\tLR: 0.010000\n",
            "Training Epoch: 13 [47488/50000]\tLoss: 4.4789\tLR: 0.010000\n",
            "Training Epoch: 13 [47616/50000]\tLoss: 4.4980\tLR: 0.010000\n",
            "Training Epoch: 13 [47744/50000]\tLoss: 4.4162\tLR: 0.010000\n",
            "Training Epoch: 13 [47872/50000]\tLoss: 4.4558\tLR: 0.010000\n",
            "Training Epoch: 13 [48000/50000]\tLoss: 4.4257\tLR: 0.010000\n",
            "Training Epoch: 13 [48128/50000]\tLoss: 4.4356\tLR: 0.010000\n",
            "Training Epoch: 13 [48256/50000]\tLoss: 4.4631\tLR: 0.010000\n",
            "Training Epoch: 13 [48384/50000]\tLoss: 4.4694\tLR: 0.010000\n",
            "Training Epoch: 13 [48512/50000]\tLoss: 4.4237\tLR: 0.010000\n",
            "Training Epoch: 13 [48640/50000]\tLoss: 4.4957\tLR: 0.010000\n",
            "Training Epoch: 13 [48768/50000]\tLoss: 4.4516\tLR: 0.010000\n",
            "Training Epoch: 13 [48896/50000]\tLoss: 4.4100\tLR: 0.010000\n",
            "Training Epoch: 13 [49024/50000]\tLoss: 4.4454\tLR: 0.010000\n",
            "Training Epoch: 13 [49152/50000]\tLoss: 4.4787\tLR: 0.010000\n",
            "Training Epoch: 13 [49280/50000]\tLoss: 4.5353\tLR: 0.010000\n",
            "Training Epoch: 13 [49408/50000]\tLoss: 4.4901\tLR: 0.010000\n",
            "Training Epoch: 13 [49536/50000]\tLoss: 4.4873\tLR: 0.010000\n",
            "Training Epoch: 13 [49664/50000]\tLoss: 4.5100\tLR: 0.010000\n",
            "Training Epoch: 13 [49792/50000]\tLoss: 4.4377\tLR: 0.010000\n",
            "Training Epoch: 13 [49920/50000]\tLoss: 4.5077\tLR: 0.010000\n",
            "Training Epoch: 13 [50000/50000]\tLoss: 4.4541\tLR: 0.010000\n",
            "epoch 13 training time consumed: 145.37s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 137999 GiB | 137999 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 136258 GiB | 136258 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1741 GiB |   1741 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 137999 GiB | 137999 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 136258 GiB | 136258 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1741 GiB |   1741 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 137622 GiB | 137622 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 135881 GiB | 135881 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1740 GiB |   1740 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 114731 GiB | 114731 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 112764 GiB | 112764 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   1967 GiB |   1967 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    8408 K  |    8408 K  |\n",
            "|       from large pool |       5    |     146    |    4075 K  |    4075 K  |\n",
            "|       from small pool |     516    |     682    |    4333 K  |    4332 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    8408 K  |    8408 K  |\n",
            "|       from large pool |       5    |     146    |    4075 K  |    4075 K  |\n",
            "|       from small pool |     516    |     682    |    4333 K  |    4332 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |     117    |    3133 K  |    3133 K  |\n",
            "|       from large pool |       4    |      46    |    1921 K  |    1921 K  |\n",
            "|       from small pool |      73    |      89    |    1211 K  |    1211 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0354, Accuracy: 0.0193, Time consumed:9.53s\n",
            "\n",
            "Training Epoch: 14 [128/50000]\tLoss: 4.4737\tLR: 0.010000\n",
            "Training Epoch: 14 [256/50000]\tLoss: 4.5275\tLR: 0.010000\n",
            "Training Epoch: 14 [384/50000]\tLoss: 4.4136\tLR: 0.010000\n",
            "Training Epoch: 14 [512/50000]\tLoss: 4.3959\tLR: 0.010000\n",
            "Training Epoch: 14 [640/50000]\tLoss: 4.4747\tLR: 0.010000\n",
            "Training Epoch: 14 [768/50000]\tLoss: 4.4498\tLR: 0.010000\n",
            "Training Epoch: 14 [896/50000]\tLoss: 4.4777\tLR: 0.010000\n",
            "Training Epoch: 14 [1024/50000]\tLoss: 4.4802\tLR: 0.010000\n",
            "Training Epoch: 14 [1152/50000]\tLoss: 4.5013\tLR: 0.010000\n",
            "Training Epoch: 14 [1280/50000]\tLoss: 4.4600\tLR: 0.010000\n",
            "Training Epoch: 14 [1408/50000]\tLoss: 4.4870\tLR: 0.010000\n",
            "Training Epoch: 14 [1536/50000]\tLoss: 4.5319\tLR: 0.010000\n",
            "Training Epoch: 14 [1664/50000]\tLoss: 4.4049\tLR: 0.010000\n",
            "Training Epoch: 14 [1792/50000]\tLoss: 4.4230\tLR: 0.010000\n",
            "Training Epoch: 14 [1920/50000]\tLoss: 4.5122\tLR: 0.010000\n",
            "Training Epoch: 14 [2048/50000]\tLoss: 4.4554\tLR: 0.010000\n",
            "Training Epoch: 14 [2176/50000]\tLoss: 4.4335\tLR: 0.010000\n",
            "Training Epoch: 14 [2304/50000]\tLoss: 4.4431\tLR: 0.010000\n",
            "Training Epoch: 14 [2432/50000]\tLoss: 4.4829\tLR: 0.010000\n",
            "Training Epoch: 14 [2560/50000]\tLoss: 4.5110\tLR: 0.010000\n",
            "Training Epoch: 14 [2688/50000]\tLoss: 4.3997\tLR: 0.010000\n",
            "Training Epoch: 14 [2816/50000]\tLoss: 4.4568\tLR: 0.010000\n",
            "Training Epoch: 14 [2944/50000]\tLoss: 4.4293\tLR: 0.010000\n",
            "Training Epoch: 14 [3072/50000]\tLoss: 4.5048\tLR: 0.010000\n",
            "Training Epoch: 14 [3200/50000]\tLoss: 4.5144\tLR: 0.010000\n",
            "Training Epoch: 14 [3328/50000]\tLoss: 4.4356\tLR: 0.010000\n",
            "Training Epoch: 14 [3456/50000]\tLoss: 4.4342\tLR: 0.010000\n",
            "Training Epoch: 14 [3584/50000]\tLoss: 4.4849\tLR: 0.010000\n",
            "Training Epoch: 14 [3712/50000]\tLoss: 4.4456\tLR: 0.010000\n",
            "Training Epoch: 14 [3840/50000]\tLoss: 4.4658\tLR: 0.010000\n",
            "Training Epoch: 14 [3968/50000]\tLoss: 4.4216\tLR: 0.010000\n",
            "Training Epoch: 14 [4096/50000]\tLoss: 4.4076\tLR: 0.010000\n",
            "Training Epoch: 14 [4224/50000]\tLoss: 4.4652\tLR: 0.010000\n",
            "Training Epoch: 14 [4352/50000]\tLoss: 4.4071\tLR: 0.010000\n",
            "Training Epoch: 14 [4480/50000]\tLoss: 4.4839\tLR: 0.010000\n",
            "Training Epoch: 14 [4608/50000]\tLoss: 4.3953\tLR: 0.010000\n",
            "Training Epoch: 14 [4736/50000]\tLoss: 4.4056\tLR: 0.010000\n",
            "Training Epoch: 14 [4864/50000]\tLoss: 4.4182\tLR: 0.010000\n",
            "Training Epoch: 14 [4992/50000]\tLoss: 4.4375\tLR: 0.010000\n",
            "Training Epoch: 14 [5120/50000]\tLoss: 4.3651\tLR: 0.010000\n",
            "Training Epoch: 14 [5248/50000]\tLoss: 4.4078\tLR: 0.010000\n",
            "Training Epoch: 14 [5376/50000]\tLoss: 4.4909\tLR: 0.010000\n",
            "Training Epoch: 14 [5504/50000]\tLoss: 4.4401\tLR: 0.010000\n",
            "Training Epoch: 14 [5632/50000]\tLoss: 4.4095\tLR: 0.010000\n",
            "Training Epoch: 14 [5760/50000]\tLoss: 4.4598\tLR: 0.010000\n",
            "Training Epoch: 14 [5888/50000]\tLoss: 4.4031\tLR: 0.010000\n",
            "Training Epoch: 14 [6016/50000]\tLoss: 4.4156\tLR: 0.010000\n",
            "Training Epoch: 14 [6144/50000]\tLoss: 4.4194\tLR: 0.010000\n",
            "Training Epoch: 14 [6272/50000]\tLoss: 4.3698\tLR: 0.010000\n",
            "Training Epoch: 14 [6400/50000]\tLoss: 4.4303\tLR: 0.010000\n",
            "Training Epoch: 14 [6528/50000]\tLoss: 4.4685\tLR: 0.010000\n",
            "Training Epoch: 14 [6656/50000]\tLoss: 4.4616\tLR: 0.010000\n",
            "Training Epoch: 14 [6784/50000]\tLoss: 4.3957\tLR: 0.010000\n",
            "Training Epoch: 14 [6912/50000]\tLoss: 4.3810\tLR: 0.010000\n",
            "Training Epoch: 14 [7040/50000]\tLoss: 4.4033\tLR: 0.010000\n",
            "Training Epoch: 14 [7168/50000]\tLoss: 4.3819\tLR: 0.010000\n",
            "Training Epoch: 14 [7296/50000]\tLoss: 4.4565\tLR: 0.010000\n",
            "Training Epoch: 14 [7424/50000]\tLoss: 4.4783\tLR: 0.010000\n",
            "Training Epoch: 14 [7552/50000]\tLoss: 4.4268\tLR: 0.010000\n",
            "Training Epoch: 14 [7680/50000]\tLoss: 4.4228\tLR: 0.010000\n",
            "Training Epoch: 14 [7808/50000]\tLoss: 4.4166\tLR: 0.010000\n",
            "Training Epoch: 14 [7936/50000]\tLoss: 4.4104\tLR: 0.010000\n",
            "Training Epoch: 14 [8064/50000]\tLoss: 4.4812\tLR: 0.010000\n",
            "Training Epoch: 14 [8192/50000]\tLoss: 4.4708\tLR: 0.010000\n",
            "Training Epoch: 14 [8320/50000]\tLoss: 4.4028\tLR: 0.010000\n",
            "Training Epoch: 14 [8448/50000]\tLoss: 4.4771\tLR: 0.010000\n",
            "Training Epoch: 14 [8576/50000]\tLoss: 4.4143\tLR: 0.010000\n",
            "Training Epoch: 14 [8704/50000]\tLoss: 4.4847\tLR: 0.010000\n",
            "Training Epoch: 14 [8832/50000]\tLoss: 4.4475\tLR: 0.010000\n",
            "Training Epoch: 14 [8960/50000]\tLoss: 4.4751\tLR: 0.010000\n",
            "Training Epoch: 14 [9088/50000]\tLoss: 4.4951\tLR: 0.010000\n",
            "Training Epoch: 14 [9216/50000]\tLoss: 4.4436\tLR: 0.010000\n",
            "Training Epoch: 14 [9344/50000]\tLoss: 4.4003\tLR: 0.010000\n",
            "Training Epoch: 14 [9472/50000]\tLoss: 4.4266\tLR: 0.010000\n",
            "Training Epoch: 14 [9600/50000]\tLoss: 4.4536\tLR: 0.010000\n",
            "Training Epoch: 14 [9728/50000]\tLoss: 4.4640\tLR: 0.010000\n",
            "Training Epoch: 14 [9856/50000]\tLoss: 4.4300\tLR: 0.010000\n",
            "Training Epoch: 14 [9984/50000]\tLoss: 4.4178\tLR: 0.010000\n",
            "Training Epoch: 14 [10112/50000]\tLoss: 4.4097\tLR: 0.010000\n",
            "Training Epoch: 14 [10240/50000]\tLoss: 4.4783\tLR: 0.010000\n",
            "Training Epoch: 14 [10368/50000]\tLoss: 4.4390\tLR: 0.010000\n",
            "Training Epoch: 14 [10496/50000]\tLoss: 4.4319\tLR: 0.010000\n",
            "Training Epoch: 14 [10624/50000]\tLoss: 4.4595\tLR: 0.010000\n",
            "Training Epoch: 14 [10752/50000]\tLoss: 4.4770\tLR: 0.010000\n",
            "Training Epoch: 14 [10880/50000]\tLoss: 4.4044\tLR: 0.010000\n",
            "Training Epoch: 14 [11008/50000]\tLoss: 4.4295\tLR: 0.010000\n",
            "Training Epoch: 14 [11136/50000]\tLoss: 4.4223\tLR: 0.010000\n",
            "Training Epoch: 14 [11264/50000]\tLoss: 4.4807\tLR: 0.010000\n",
            "Training Epoch: 14 [11392/50000]\tLoss: 4.4282\tLR: 0.010000\n",
            "Training Epoch: 14 [11520/50000]\tLoss: 4.4409\tLR: 0.010000\n",
            "Training Epoch: 14 [11648/50000]\tLoss: 4.3547\tLR: 0.010000\n",
            "Training Epoch: 14 [11776/50000]\tLoss: 4.3643\tLR: 0.010000\n",
            "Training Epoch: 14 [11904/50000]\tLoss: 4.4743\tLR: 0.010000\n",
            "Training Epoch: 14 [12032/50000]\tLoss: 4.4239\tLR: 0.010000\n",
            "Training Epoch: 14 [12160/50000]\tLoss: 4.5212\tLR: 0.010000\n",
            "Training Epoch: 14 [12288/50000]\tLoss: 4.4210\tLR: 0.010000\n",
            "Training Epoch: 14 [12416/50000]\tLoss: 4.4636\tLR: 0.010000\n",
            "Training Epoch: 14 [12544/50000]\tLoss: 4.4400\tLR: 0.010000\n",
            "Training Epoch: 14 [12672/50000]\tLoss: 4.4107\tLR: 0.010000\n",
            "Training Epoch: 14 [12800/50000]\tLoss: 4.4144\tLR: 0.010000\n",
            "Training Epoch: 14 [12928/50000]\tLoss: 4.5127\tLR: 0.010000\n",
            "Training Epoch: 14 [13056/50000]\tLoss: 4.4410\tLR: 0.010000\n",
            "Training Epoch: 14 [13184/50000]\tLoss: 4.4274\tLR: 0.010000\n",
            "Training Epoch: 14 [13312/50000]\tLoss: 4.4126\tLR: 0.010000\n",
            "Training Epoch: 14 [13440/50000]\tLoss: 4.4530\tLR: 0.010000\n",
            "Training Epoch: 14 [13568/50000]\tLoss: 4.4785\tLR: 0.010000\n",
            "Training Epoch: 14 [13696/50000]\tLoss: 4.6353\tLR: 0.010000\n",
            "Training Epoch: 14 [13824/50000]\tLoss: 4.3428\tLR: 0.010000\n",
            "Training Epoch: 14 [13952/50000]\tLoss: 4.4593\tLR: 0.010000\n",
            "Training Epoch: 14 [14080/50000]\tLoss: 4.3835\tLR: 0.010000\n",
            "Training Epoch: 14 [14208/50000]\tLoss: 4.3925\tLR: 0.010000\n",
            "Training Epoch: 14 [14336/50000]\tLoss: 4.5070\tLR: 0.010000\n",
            "Training Epoch: 14 [14464/50000]\tLoss: 4.4121\tLR: 0.010000\n",
            "Training Epoch: 14 [14592/50000]\tLoss: 4.4232\tLR: 0.010000\n",
            "Training Epoch: 14 [14720/50000]\tLoss: 4.4188\tLR: 0.010000\n",
            "Training Epoch: 14 [14848/50000]\tLoss: 4.4333\tLR: 0.010000\n",
            "Training Epoch: 14 [14976/50000]\tLoss: 4.4874\tLR: 0.010000\n",
            "Training Epoch: 14 [15104/50000]\tLoss: 4.2892\tLR: 0.010000\n",
            "Training Epoch: 14 [15232/50000]\tLoss: 4.4145\tLR: 0.010000\n",
            "Training Epoch: 14 [15360/50000]\tLoss: 4.3850\tLR: 0.010000\n",
            "Training Epoch: 14 [15488/50000]\tLoss: 4.4539\tLR: 0.010000\n",
            "Training Epoch: 14 [15616/50000]\tLoss: 4.4599\tLR: 0.010000\n",
            "Training Epoch: 14 [15744/50000]\tLoss: 4.4207\tLR: 0.010000\n",
            "Training Epoch: 14 [15872/50000]\tLoss: 4.4129\tLR: 0.010000\n",
            "Training Epoch: 14 [16000/50000]\tLoss: 4.4522\tLR: 0.010000\n",
            "Training Epoch: 14 [16128/50000]\tLoss: 4.4301\tLR: 0.010000\n",
            "Training Epoch: 14 [16256/50000]\tLoss: 4.4792\tLR: 0.010000\n",
            "Training Epoch: 14 [16384/50000]\tLoss: 4.4641\tLR: 0.010000\n",
            "Training Epoch: 14 [16512/50000]\tLoss: 4.3534\tLR: 0.010000\n",
            "Training Epoch: 14 [16640/50000]\tLoss: 4.4448\tLR: 0.010000\n",
            "Training Epoch: 14 [16768/50000]\tLoss: 4.4113\tLR: 0.010000\n",
            "Training Epoch: 14 [16896/50000]\tLoss: 4.3581\tLR: 0.010000\n",
            "Training Epoch: 14 [17024/50000]\tLoss: 4.3001\tLR: 0.010000\n",
            "Training Epoch: 14 [17152/50000]\tLoss: 4.4608\tLR: 0.010000\n",
            "Training Epoch: 14 [17280/50000]\tLoss: 4.4017\tLR: 0.010000\n",
            "Training Epoch: 14 [17408/50000]\tLoss: 4.3556\tLR: 0.010000\n",
            "Training Epoch: 14 [17536/50000]\tLoss: 4.5136\tLR: 0.010000\n",
            "Training Epoch: 14 [17664/50000]\tLoss: 4.4463\tLR: 0.010000\n",
            "Training Epoch: 14 [17792/50000]\tLoss: 4.3961\tLR: 0.010000\n",
            "Training Epoch: 14 [17920/50000]\tLoss: 4.4614\tLR: 0.010000\n",
            "Training Epoch: 14 [18048/50000]\tLoss: 4.3545\tLR: 0.010000\n",
            "Training Epoch: 14 [18176/50000]\tLoss: 4.4559\tLR: 0.010000\n",
            "Training Epoch: 14 [18304/50000]\tLoss: 4.4246\tLR: 0.010000\n",
            "Training Epoch: 14 [18432/50000]\tLoss: 4.3223\tLR: 0.010000\n",
            "Training Epoch: 14 [18560/50000]\tLoss: 4.4573\tLR: 0.010000\n",
            "Training Epoch: 14 [18688/50000]\tLoss: 4.3922\tLR: 0.010000\n",
            "Training Epoch: 14 [18816/50000]\tLoss: 4.2928\tLR: 0.010000\n",
            "Training Epoch: 14 [18944/50000]\tLoss: 4.4412\tLR: 0.010000\n",
            "Training Epoch: 14 [19072/50000]\tLoss: 4.4126\tLR: 0.010000\n",
            "Training Epoch: 14 [19200/50000]\tLoss: 4.3242\tLR: 0.010000\n",
            "Training Epoch: 14 [19328/50000]\tLoss: 4.3695\tLR: 0.010000\n",
            "Training Epoch: 14 [19456/50000]\tLoss: 4.2858\tLR: 0.010000\n",
            "Training Epoch: 14 [19584/50000]\tLoss: 4.3750\tLR: 0.010000\n",
            "Training Epoch: 14 [19712/50000]\tLoss: 4.4095\tLR: 0.010000\n",
            "Training Epoch: 14 [19840/50000]\tLoss: 4.3687\tLR: 0.010000\n",
            "Training Epoch: 14 [19968/50000]\tLoss: 4.4183\tLR: 0.010000\n",
            "Training Epoch: 14 [20096/50000]\tLoss: 4.3165\tLR: 0.010000\n",
            "Training Epoch: 14 [20224/50000]\tLoss: 4.3747\tLR: 0.010000\n",
            "Training Epoch: 14 [20352/50000]\tLoss: 4.4580\tLR: 0.010000\n",
            "Training Epoch: 14 [20480/50000]\tLoss: 4.3649\tLR: 0.010000\n",
            "Training Epoch: 14 [20608/50000]\tLoss: 4.3390\tLR: 0.010000\n",
            "Training Epoch: 14 [20736/50000]\tLoss: 4.3025\tLR: 0.010000\n",
            "Training Epoch: 14 [20864/50000]\tLoss: 4.4433\tLR: 0.010000\n",
            "Training Epoch: 14 [20992/50000]\tLoss: 4.3897\tLR: 0.010000\n",
            "Training Epoch: 14 [21120/50000]\tLoss: 4.4078\tLR: 0.010000\n",
            "Training Epoch: 14 [21248/50000]\tLoss: 4.4353\tLR: 0.010000\n",
            "Training Epoch: 14 [21376/50000]\tLoss: 4.5232\tLR: 0.010000\n",
            "Training Epoch: 14 [21504/50000]\tLoss: 4.3242\tLR: 0.010000\n",
            "Training Epoch: 14 [21632/50000]\tLoss: 4.4132\tLR: 0.010000\n",
            "Training Epoch: 14 [21760/50000]\tLoss: 4.3610\tLR: 0.010000\n",
            "Training Epoch: 14 [21888/50000]\tLoss: 4.3888\tLR: 0.010000\n",
            "Training Epoch: 14 [22016/50000]\tLoss: 4.4224\tLR: 0.010000\n",
            "Training Epoch: 14 [22144/50000]\tLoss: 4.3668\tLR: 0.010000\n",
            "Training Epoch: 14 [22272/50000]\tLoss: 4.3931\tLR: 0.010000\n",
            "Training Epoch: 14 [22400/50000]\tLoss: 4.3201\tLR: 0.010000\n",
            "Training Epoch: 14 [22528/50000]\tLoss: 4.4085\tLR: 0.010000\n",
            "Training Epoch: 14 [22656/50000]\tLoss: 4.3939\tLR: 0.010000\n",
            "Training Epoch: 14 [22784/50000]\tLoss: 4.2765\tLR: 0.010000\n",
            "Training Epoch: 14 [22912/50000]\tLoss: 4.3163\tLR: 0.010000\n",
            "Training Epoch: 14 [23040/50000]\tLoss: 4.4227\tLR: 0.010000\n",
            "Training Epoch: 14 [23168/50000]\tLoss: 4.3131\tLR: 0.010000\n",
            "Training Epoch: 14 [23296/50000]\tLoss: 4.3241\tLR: 0.010000\n",
            "Training Epoch: 14 [23424/50000]\tLoss: 4.3836\tLR: 0.010000\n",
            "Training Epoch: 14 [23552/50000]\tLoss: 4.3097\tLR: 0.010000\n",
            "Training Epoch: 14 [23680/50000]\tLoss: 4.3585\tLR: 0.010000\n",
            "Training Epoch: 14 [23808/50000]\tLoss: 4.4185\tLR: 0.010000\n",
            "Training Epoch: 14 [23936/50000]\tLoss: 4.5601\tLR: 0.010000\n",
            "Training Epoch: 14 [24064/50000]\tLoss: 4.2768\tLR: 0.010000\n",
            "Training Epoch: 14 [24192/50000]\tLoss: 4.3280\tLR: 0.010000\n",
            "Training Epoch: 14 [24320/50000]\tLoss: 4.3761\tLR: 0.010000\n",
            "Training Epoch: 14 [24448/50000]\tLoss: 4.3381\tLR: 0.010000\n",
            "Training Epoch: 14 [24576/50000]\tLoss: 4.4409\tLR: 0.010000\n",
            "Training Epoch: 14 [24704/50000]\tLoss: 4.3803\tLR: 0.010000\n",
            "Training Epoch: 14 [24832/50000]\tLoss: 4.4418\tLR: 0.010000\n",
            "Training Epoch: 14 [24960/50000]\tLoss: 4.3409\tLR: 0.010000\n",
            "Training Epoch: 14 [25088/50000]\tLoss: 4.3562\tLR: 0.010000\n",
            "Training Epoch: 14 [25216/50000]\tLoss: 4.3503\tLR: 0.010000\n",
            "Training Epoch: 14 [25344/50000]\tLoss: 4.3409\tLR: 0.010000\n",
            "Training Epoch: 14 [25472/50000]\tLoss: 4.3602\tLR: 0.010000\n",
            "Training Epoch: 14 [25600/50000]\tLoss: 4.3935\tLR: 0.010000\n",
            "Training Epoch: 14 [25728/50000]\tLoss: 4.3843\tLR: 0.010000\n",
            "Training Epoch: 14 [25856/50000]\tLoss: 4.3863\tLR: 0.010000\n",
            "Training Epoch: 14 [25984/50000]\tLoss: 4.4008\tLR: 0.010000\n",
            "Training Epoch: 14 [26112/50000]\tLoss: 4.3493\tLR: 0.010000\n",
            "Training Epoch: 14 [26240/50000]\tLoss: 4.3901\tLR: 0.010000\n",
            "Training Epoch: 14 [26368/50000]\tLoss: 4.4434\tLR: 0.010000\n",
            "Training Epoch: 14 [26496/50000]\tLoss: 4.3445\tLR: 0.010000\n",
            "Training Epoch: 14 [26624/50000]\tLoss: 4.3947\tLR: 0.010000\n",
            "Training Epoch: 14 [26752/50000]\tLoss: 4.3671\tLR: 0.010000\n",
            "Training Epoch: 14 [26880/50000]\tLoss: 4.3728\tLR: 0.010000\n",
            "Training Epoch: 14 [27008/50000]\tLoss: 4.3434\tLR: 0.010000\n",
            "Training Epoch: 14 [27136/50000]\tLoss: 4.3439\tLR: 0.010000\n",
            "Training Epoch: 14 [27264/50000]\tLoss: 4.3265\tLR: 0.010000\n",
            "Training Epoch: 14 [27392/50000]\tLoss: 4.4037\tLR: 0.010000\n",
            "Training Epoch: 14 [27520/50000]\tLoss: 4.4469\tLR: 0.010000\n",
            "Training Epoch: 14 [27648/50000]\tLoss: 4.3256\tLR: 0.010000\n",
            "Training Epoch: 14 [27776/50000]\tLoss: 4.3361\tLR: 0.010000\n",
            "Training Epoch: 14 [27904/50000]\tLoss: 4.3554\tLR: 0.010000\n",
            "Training Epoch: 14 [28032/50000]\tLoss: 4.3249\tLR: 0.010000\n",
            "Training Epoch: 14 [28160/50000]\tLoss: 4.3585\tLR: 0.010000\n",
            "Training Epoch: 14 [28288/50000]\tLoss: 4.4456\tLR: 0.010000\n",
            "Training Epoch: 14 [28416/50000]\tLoss: 4.2911\tLR: 0.010000\n",
            "Training Epoch: 14 [28544/50000]\tLoss: 4.3760\tLR: 0.010000\n",
            "Training Epoch: 14 [28672/50000]\tLoss: 4.3841\tLR: 0.010000\n",
            "Training Epoch: 14 [28800/50000]\tLoss: 4.3958\tLR: 0.010000\n",
            "Training Epoch: 14 [28928/50000]\tLoss: 4.3548\tLR: 0.010000\n",
            "Training Epoch: 14 [29056/50000]\tLoss: 4.3107\tLR: 0.010000\n",
            "Training Epoch: 14 [29184/50000]\tLoss: 4.4398\tLR: 0.010000\n",
            "Training Epoch: 14 [29312/50000]\tLoss: 4.3621\tLR: 0.010000\n",
            "Training Epoch: 14 [29440/50000]\tLoss: 4.4123\tLR: 0.010000\n",
            "Training Epoch: 14 [29568/50000]\tLoss: 4.3236\tLR: 0.010000\n",
            "Training Epoch: 14 [29696/50000]\tLoss: 4.3926\tLR: 0.010000\n",
            "Training Epoch: 14 [29824/50000]\tLoss: 4.2563\tLR: 0.010000\n",
            "Training Epoch: 14 [29952/50000]\tLoss: 4.4676\tLR: 0.010000\n",
            "Training Epoch: 14 [30080/50000]\tLoss: 4.3361\tLR: 0.010000\n",
            "Training Epoch: 14 [30208/50000]\tLoss: 4.3503\tLR: 0.010000\n",
            "Training Epoch: 14 [30336/50000]\tLoss: 4.3062\tLR: 0.010000\n",
            "Training Epoch: 14 [30464/50000]\tLoss: 4.3947\tLR: 0.010000\n",
            "Training Epoch: 14 [30592/50000]\tLoss: 4.2644\tLR: 0.010000\n",
            "Training Epoch: 14 [30720/50000]\tLoss: 4.3467\tLR: 0.010000\n",
            "Training Epoch: 14 [30848/50000]\tLoss: 4.3583\tLR: 0.010000\n",
            "Training Epoch: 14 [30976/50000]\tLoss: 4.3869\tLR: 0.010000\n",
            "Training Epoch: 14 [31104/50000]\tLoss: 4.3280\tLR: 0.010000\n",
            "Training Epoch: 14 [31232/50000]\tLoss: 4.3800\tLR: 0.010000\n",
            "Training Epoch: 14 [31360/50000]\tLoss: 4.3394\tLR: 0.010000\n",
            "Training Epoch: 14 [31488/50000]\tLoss: 4.4061\tLR: 0.010000\n",
            "Training Epoch: 14 [31616/50000]\tLoss: 4.3910\tLR: 0.010000\n",
            "Training Epoch: 14 [31744/50000]\tLoss: 4.4599\tLR: 0.010000\n",
            "Training Epoch: 14 [31872/50000]\tLoss: 4.3605\tLR: 0.010000\n",
            "Training Epoch: 14 [32000/50000]\tLoss: 4.4187\tLR: 0.010000\n",
            "Training Epoch: 14 [32128/50000]\tLoss: 4.4393\tLR: 0.010000\n",
            "Training Epoch: 14 [32256/50000]\tLoss: 4.3959\tLR: 0.010000\n",
            "Training Epoch: 14 [32384/50000]\tLoss: 4.3660\tLR: 0.010000\n",
            "Training Epoch: 14 [32512/50000]\tLoss: 4.3996\tLR: 0.010000\n",
            "Training Epoch: 14 [32640/50000]\tLoss: 4.2400\tLR: 0.010000\n",
            "Training Epoch: 14 [32768/50000]\tLoss: 4.4765\tLR: 0.010000\n",
            "Training Epoch: 14 [32896/50000]\tLoss: 4.3555\tLR: 0.010000\n",
            "Training Epoch: 14 [33024/50000]\tLoss: 4.4664\tLR: 0.010000\n",
            "Training Epoch: 14 [33152/50000]\tLoss: 4.3742\tLR: 0.010000\n",
            "Training Epoch: 14 [33280/50000]\tLoss: 4.4169\tLR: 0.010000\n",
            "Training Epoch: 14 [33408/50000]\tLoss: 4.2596\tLR: 0.010000\n",
            "Training Epoch: 14 [33536/50000]\tLoss: 4.4644\tLR: 0.010000\n",
            "Training Epoch: 14 [33664/50000]\tLoss: 4.2990\tLR: 0.010000\n",
            "Training Epoch: 14 [33792/50000]\tLoss: 4.3740\tLR: 0.010000\n",
            "Training Epoch: 14 [33920/50000]\tLoss: 4.3849\tLR: 0.010000\n",
            "Training Epoch: 14 [34048/50000]\tLoss: 4.3437\tLR: 0.010000\n",
            "Training Epoch: 14 [34176/50000]\tLoss: 4.4106\tLR: 0.010000\n",
            "Training Epoch: 14 [34304/50000]\tLoss: 4.3414\tLR: 0.010000\n",
            "Training Epoch: 14 [34432/50000]\tLoss: 4.3248\tLR: 0.010000\n",
            "Training Epoch: 14 [34560/50000]\tLoss: 4.3577\tLR: 0.010000\n",
            "Training Epoch: 14 [34688/50000]\tLoss: 4.4322\tLR: 0.010000\n",
            "Training Epoch: 14 [34816/50000]\tLoss: 4.3172\tLR: 0.010000\n",
            "Training Epoch: 14 [34944/50000]\tLoss: 4.4384\tLR: 0.010000\n",
            "Training Epoch: 14 [35072/50000]\tLoss: 4.3612\tLR: 0.010000\n",
            "Training Epoch: 14 [35200/50000]\tLoss: 4.1818\tLR: 0.010000\n",
            "Training Epoch: 14 [35328/50000]\tLoss: 4.3018\tLR: 0.010000\n",
            "Training Epoch: 14 [35456/50000]\tLoss: 4.3660\tLR: 0.010000\n",
            "Training Epoch: 14 [35584/50000]\tLoss: 4.3365\tLR: 0.010000\n",
            "Training Epoch: 14 [35712/50000]\tLoss: 4.3691\tLR: 0.010000\n",
            "Training Epoch: 14 [35840/50000]\tLoss: 4.3396\tLR: 0.010000\n",
            "Training Epoch: 14 [35968/50000]\tLoss: 4.3153\tLR: 0.010000\n",
            "Training Epoch: 14 [36096/50000]\tLoss: 4.3497\tLR: 0.010000\n",
            "Training Epoch: 14 [36224/50000]\tLoss: 4.3704\tLR: 0.010000\n",
            "Training Epoch: 14 [36352/50000]\tLoss: 4.3834\tLR: 0.010000\n",
            "Training Epoch: 14 [36480/50000]\tLoss: 4.3633\tLR: 0.010000\n",
            "Training Epoch: 14 [36608/50000]\tLoss: 4.4829\tLR: 0.010000\n",
            "Training Epoch: 14 [36736/50000]\tLoss: 4.3312\tLR: 0.010000\n",
            "Training Epoch: 14 [36864/50000]\tLoss: 4.2982\tLR: 0.010000\n",
            "Training Epoch: 14 [36992/50000]\tLoss: 4.3076\tLR: 0.010000\n",
            "Training Epoch: 14 [37120/50000]\tLoss: 4.3646\tLR: 0.010000\n",
            "Training Epoch: 14 [37248/50000]\tLoss: 4.3776\tLR: 0.010000\n",
            "Training Epoch: 14 [37376/50000]\tLoss: 4.4074\tLR: 0.010000\n",
            "Training Epoch: 14 [37504/50000]\tLoss: 4.3694\tLR: 0.010000\n",
            "Training Epoch: 14 [37632/50000]\tLoss: 4.4601\tLR: 0.010000\n",
            "Training Epoch: 14 [37760/50000]\tLoss: 4.3984\tLR: 0.010000\n",
            "Training Epoch: 14 [37888/50000]\tLoss: 4.3081\tLR: 0.010000\n",
            "Training Epoch: 14 [38016/50000]\tLoss: 4.3732\tLR: 0.010000\n",
            "Training Epoch: 14 [38144/50000]\tLoss: 4.2981\tLR: 0.010000\n",
            "Training Epoch: 14 [38272/50000]\tLoss: 4.3524\tLR: 0.010000\n",
            "Training Epoch: 14 [38400/50000]\tLoss: 4.2821\tLR: 0.010000\n",
            "Training Epoch: 14 [38528/50000]\tLoss: 4.3361\tLR: 0.010000\n",
            "Training Epoch: 14 [38656/50000]\tLoss: 4.3976\tLR: 0.010000\n",
            "Training Epoch: 14 [38784/50000]\tLoss: 4.4639\tLR: 0.010000\n",
            "Training Epoch: 14 [38912/50000]\tLoss: 4.3379\tLR: 0.010000\n",
            "Training Epoch: 14 [39040/50000]\tLoss: 4.3048\tLR: 0.010000\n",
            "Training Epoch: 14 [39168/50000]\tLoss: 4.4336\tLR: 0.010000\n",
            "Training Epoch: 14 [39296/50000]\tLoss: 4.2600\tLR: 0.010000\n",
            "Training Epoch: 14 [39424/50000]\tLoss: 4.3762\tLR: 0.010000\n",
            "Training Epoch: 14 [39552/50000]\tLoss: 4.3867\tLR: 0.010000\n",
            "Training Epoch: 14 [39680/50000]\tLoss: 4.3228\tLR: 0.010000\n",
            "Training Epoch: 14 [39808/50000]\tLoss: 4.3645\tLR: 0.010000\n",
            "Training Epoch: 14 [39936/50000]\tLoss: 4.3447\tLR: 0.010000\n",
            "Training Epoch: 14 [40064/50000]\tLoss: 4.3052\tLR: 0.010000\n",
            "Training Epoch: 14 [40192/50000]\tLoss: 4.4435\tLR: 0.010000\n",
            "Training Epoch: 14 [40320/50000]\tLoss: 4.3881\tLR: 0.010000\n",
            "Training Epoch: 14 [40448/50000]\tLoss: 4.4675\tLR: 0.010000\n",
            "Training Epoch: 14 [40576/50000]\tLoss: 4.3638\tLR: 0.010000\n",
            "Training Epoch: 14 [40704/50000]\tLoss: 4.3246\tLR: 0.010000\n",
            "Training Epoch: 14 [40832/50000]\tLoss: 4.3649\tLR: 0.010000\n",
            "Training Epoch: 14 [40960/50000]\tLoss: 4.3281\tLR: 0.010000\n",
            "Training Epoch: 14 [41088/50000]\tLoss: 4.3469\tLR: 0.010000\n",
            "Training Epoch: 14 [41216/50000]\tLoss: 4.4064\tLR: 0.010000\n",
            "Training Epoch: 14 [41344/50000]\tLoss: 4.3747\tLR: 0.010000\n",
            "Training Epoch: 14 [41472/50000]\tLoss: 4.2839\tLR: 0.010000\n",
            "Training Epoch: 14 [41600/50000]\tLoss: 4.2524\tLR: 0.010000\n",
            "Training Epoch: 14 [41728/50000]\tLoss: 4.1653\tLR: 0.010000\n",
            "Training Epoch: 14 [41856/50000]\tLoss: 4.2950\tLR: 0.010000\n",
            "Training Epoch: 14 [41984/50000]\tLoss: 4.4287\tLR: 0.010000\n",
            "Training Epoch: 14 [42112/50000]\tLoss: 4.3487\tLR: 0.010000\n",
            "Training Epoch: 14 [42240/50000]\tLoss: 4.3580\tLR: 0.010000\n",
            "Training Epoch: 14 [42368/50000]\tLoss: 4.4617\tLR: 0.010000\n",
            "Training Epoch: 14 [42496/50000]\tLoss: 4.2968\tLR: 0.010000\n",
            "Training Epoch: 14 [42624/50000]\tLoss: 4.3198\tLR: 0.010000\n",
            "Training Epoch: 14 [42752/50000]\tLoss: 4.3258\tLR: 0.010000\n",
            "Training Epoch: 14 [42880/50000]\tLoss: 4.2607\tLR: 0.010000\n",
            "Training Epoch: 14 [43008/50000]\tLoss: 4.3403\tLR: 0.010000\n",
            "Training Epoch: 14 [43136/50000]\tLoss: 4.3558\tLR: 0.010000\n",
            "Training Epoch: 14 [43264/50000]\tLoss: 4.2624\tLR: 0.010000\n",
            "Training Epoch: 14 [43392/50000]\tLoss: 4.4000\tLR: 0.010000\n",
            "Training Epoch: 14 [43520/50000]\tLoss: 4.2830\tLR: 0.010000\n",
            "Training Epoch: 14 [43648/50000]\tLoss: 4.3782\tLR: 0.010000\n",
            "Training Epoch: 14 [43776/50000]\tLoss: 4.2517\tLR: 0.010000\n",
            "Training Epoch: 14 [43904/50000]\tLoss: 4.3803\tLR: 0.010000\n",
            "Training Epoch: 14 [44032/50000]\tLoss: 4.3525\tLR: 0.010000\n",
            "Training Epoch: 14 [44160/50000]\tLoss: 4.3367\tLR: 0.010000\n",
            "Training Epoch: 14 [44288/50000]\tLoss: 4.3411\tLR: 0.010000\n",
            "Training Epoch: 14 [44416/50000]\tLoss: 4.3250\tLR: 0.010000\n",
            "Training Epoch: 14 [44544/50000]\tLoss: 4.2553\tLR: 0.010000\n",
            "Training Epoch: 14 [44672/50000]\tLoss: 4.4367\tLR: 0.010000\n",
            "Training Epoch: 14 [44800/50000]\tLoss: 4.2983\tLR: 0.010000\n",
            "Training Epoch: 14 [44928/50000]\tLoss: 4.2844\tLR: 0.010000\n",
            "Training Epoch: 14 [45056/50000]\tLoss: 4.2531\tLR: 0.010000\n",
            "Training Epoch: 14 [45184/50000]\tLoss: 4.3950\tLR: 0.010000\n",
            "Training Epoch: 14 [45312/50000]\tLoss: 4.3135\tLR: 0.010000\n",
            "Training Epoch: 14 [45440/50000]\tLoss: 4.3313\tLR: 0.010000\n",
            "Training Epoch: 14 [45568/50000]\tLoss: 4.3801\tLR: 0.010000\n",
            "Training Epoch: 14 [45696/50000]\tLoss: 4.4258\tLR: 0.010000\n",
            "Training Epoch: 14 [45824/50000]\tLoss: 4.2515\tLR: 0.010000\n",
            "Training Epoch: 14 [45952/50000]\tLoss: 4.3144\tLR: 0.010000\n",
            "Training Epoch: 14 [46080/50000]\tLoss: 4.3222\tLR: 0.010000\n",
            "Training Epoch: 14 [46208/50000]\tLoss: 4.3617\tLR: 0.010000\n",
            "Training Epoch: 14 [46336/50000]\tLoss: 4.3918\tLR: 0.010000\n",
            "Training Epoch: 14 [46464/50000]\tLoss: 4.2732\tLR: 0.010000\n",
            "Training Epoch: 14 [46592/50000]\tLoss: 4.3163\tLR: 0.010000\n",
            "Training Epoch: 14 [46720/50000]\tLoss: 4.2530\tLR: 0.010000\n",
            "Training Epoch: 14 [46848/50000]\tLoss: 4.4117\tLR: 0.010000\n",
            "Training Epoch: 14 [46976/50000]\tLoss: 4.2495\tLR: 0.010000\n",
            "Training Epoch: 14 [47104/50000]\tLoss: 4.3564\tLR: 0.010000\n",
            "Training Epoch: 14 [47232/50000]\tLoss: 4.4461\tLR: 0.010000\n",
            "Training Epoch: 14 [47360/50000]\tLoss: 4.3578\tLR: 0.010000\n",
            "Training Epoch: 14 [47488/50000]\tLoss: 4.3360\tLR: 0.010000\n",
            "Training Epoch: 14 [47616/50000]\tLoss: 4.2991\tLR: 0.010000\n",
            "Training Epoch: 14 [47744/50000]\tLoss: 4.4152\tLR: 0.010000\n",
            "Training Epoch: 14 [47872/50000]\tLoss: 4.3957\tLR: 0.010000\n",
            "Training Epoch: 14 [48000/50000]\tLoss: 4.2743\tLR: 0.010000\n",
            "Training Epoch: 14 [48128/50000]\tLoss: 4.2688\tLR: 0.010000\n",
            "Training Epoch: 14 [48256/50000]\tLoss: 4.3364\tLR: 0.010000\n",
            "Training Epoch: 14 [48384/50000]\tLoss: 4.3137\tLR: 0.010000\n",
            "Training Epoch: 14 [48512/50000]\tLoss: 4.2811\tLR: 0.010000\n",
            "Training Epoch: 14 [48640/50000]\tLoss: 4.3624\tLR: 0.010000\n",
            "Training Epoch: 14 [48768/50000]\tLoss: 4.2707\tLR: 0.010000\n",
            "Training Epoch: 14 [48896/50000]\tLoss: 4.3758\tLR: 0.010000\n",
            "Training Epoch: 14 [49024/50000]\tLoss: 4.2626\tLR: 0.010000\n",
            "Training Epoch: 14 [49152/50000]\tLoss: 4.2837\tLR: 0.010000\n",
            "Training Epoch: 14 [49280/50000]\tLoss: 4.3511\tLR: 0.010000\n",
            "Training Epoch: 14 [49408/50000]\tLoss: 4.2186\tLR: 0.010000\n",
            "Training Epoch: 14 [49536/50000]\tLoss: 4.3233\tLR: 0.010000\n",
            "Training Epoch: 14 [49664/50000]\tLoss: 4.3820\tLR: 0.010000\n",
            "Training Epoch: 14 [49792/50000]\tLoss: 4.2772\tLR: 0.010000\n",
            "Training Epoch: 14 [49920/50000]\tLoss: 4.3192\tLR: 0.010000\n",
            "Training Epoch: 14 [50000/50000]\tLoss: 4.2527\tLR: 0.010000\n",
            "epoch 14 training time consumed: 144.49s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 148612 GiB | 148612 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 146737 GiB | 146737 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1875 GiB |   1875 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 148612 GiB | 148612 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 146737 GiB | 146737 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   1875 GiB |   1875 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 148206 GiB | 148205 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 146331 GiB | 146331 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   1874 GiB |   1874 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 123687 GiB | 123687 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 121568 GiB | 121568 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2118 GiB |   2118 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    9055 K  |    9054 K  |\n",
            "|       from large pool |       5    |     146    |    4389 K  |    4389 K  |\n",
            "|       from small pool |     516    |     682    |    4666 K  |    4665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    9055 K  |    9054 K  |\n",
            "|       from large pool |       5    |     146    |    4389 K  |    4389 K  |\n",
            "|       from small pool |     516    |     682    |    4666 K  |    4665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |     117    |    3373 K  |    3373 K  |\n",
            "|       from large pool |       4    |      46    |    2068 K  |    2068 K  |\n",
            "|       from small pool |      75    |      89    |    1304 K  |    1304 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0342, Accuracy: 0.0339, Time consumed:9.50s\n",
            "\n",
            "Training Epoch: 15 [128/50000]\tLoss: 4.3572\tLR: 0.010000\n",
            "Training Epoch: 15 [256/50000]\tLoss: 4.2630\tLR: 0.010000\n",
            "Training Epoch: 15 [384/50000]\tLoss: 4.3602\tLR: 0.010000\n",
            "Training Epoch: 15 [512/50000]\tLoss: 4.3563\tLR: 0.010000\n",
            "Training Epoch: 15 [640/50000]\tLoss: 4.3577\tLR: 0.010000\n",
            "Training Epoch: 15 [768/50000]\tLoss: 4.2566\tLR: 0.010000\n",
            "Training Epoch: 15 [896/50000]\tLoss: 4.3653\tLR: 0.010000\n",
            "Training Epoch: 15 [1024/50000]\tLoss: 4.3566\tLR: 0.010000\n",
            "Training Epoch: 15 [1152/50000]\tLoss: 4.3427\tLR: 0.010000\n",
            "Training Epoch: 15 [1280/50000]\tLoss: 4.3635\tLR: 0.010000\n",
            "Training Epoch: 15 [1408/50000]\tLoss: 4.2144\tLR: 0.010000\n",
            "Training Epoch: 15 [1536/50000]\tLoss: 4.3438\tLR: 0.010000\n",
            "Training Epoch: 15 [1664/50000]\tLoss: 4.3064\tLR: 0.010000\n",
            "Training Epoch: 15 [1792/50000]\tLoss: 4.4572\tLR: 0.010000\n",
            "Training Epoch: 15 [1920/50000]\tLoss: 4.3178\tLR: 0.010000\n",
            "Training Epoch: 15 [2048/50000]\tLoss: 4.2144\tLR: 0.010000\n",
            "Training Epoch: 15 [2176/50000]\tLoss: 4.2929\tLR: 0.010000\n",
            "Training Epoch: 15 [2304/50000]\tLoss: 4.3773\tLR: 0.010000\n",
            "Training Epoch: 15 [2432/50000]\tLoss: 4.2726\tLR: 0.010000\n",
            "Training Epoch: 15 [2560/50000]\tLoss: 4.5192\tLR: 0.010000\n",
            "Training Epoch: 15 [2688/50000]\tLoss: 4.3523\tLR: 0.010000\n",
            "Training Epoch: 15 [2816/50000]\tLoss: 4.2942\tLR: 0.010000\n",
            "Training Epoch: 15 [2944/50000]\tLoss: 4.3313\tLR: 0.010000\n",
            "Training Epoch: 15 [3072/50000]\tLoss: 4.2328\tLR: 0.010000\n",
            "Training Epoch: 15 [3200/50000]\tLoss: 4.4184\tLR: 0.010000\n",
            "Training Epoch: 15 [3328/50000]\tLoss: 4.2970\tLR: 0.010000\n",
            "Training Epoch: 15 [3456/50000]\tLoss: 4.3237\tLR: 0.010000\n",
            "Training Epoch: 15 [3584/50000]\tLoss: 4.3473\tLR: 0.010000\n",
            "Training Epoch: 15 [3712/50000]\tLoss: 4.3954\tLR: 0.010000\n",
            "Training Epoch: 15 [3840/50000]\tLoss: 4.3606\tLR: 0.010000\n",
            "Training Epoch: 15 [3968/50000]\tLoss: 4.3671\tLR: 0.010000\n",
            "Training Epoch: 15 [4096/50000]\tLoss: 4.2506\tLR: 0.010000\n",
            "Training Epoch: 15 [4224/50000]\tLoss: 4.3635\tLR: 0.010000\n",
            "Training Epoch: 15 [4352/50000]\tLoss: 4.3695\tLR: 0.010000\n",
            "Training Epoch: 15 [4480/50000]\tLoss: 4.3730\tLR: 0.010000\n",
            "Training Epoch: 15 [4608/50000]\tLoss: 4.2404\tLR: 0.010000\n",
            "Training Epoch: 15 [4736/50000]\tLoss: 4.2975\tLR: 0.010000\n",
            "Training Epoch: 15 [4864/50000]\tLoss: 4.3008\tLR: 0.010000\n",
            "Training Epoch: 15 [4992/50000]\tLoss: 4.3406\tLR: 0.010000\n",
            "Training Epoch: 15 [5120/50000]\tLoss: 4.3919\tLR: 0.010000\n",
            "Training Epoch: 15 [5248/50000]\tLoss: 4.3415\tLR: 0.010000\n",
            "Training Epoch: 15 [5376/50000]\tLoss: 4.3856\tLR: 0.010000\n",
            "Training Epoch: 15 [5504/50000]\tLoss: 4.2522\tLR: 0.010000\n",
            "Training Epoch: 15 [5632/50000]\tLoss: 4.3500\tLR: 0.010000\n",
            "Training Epoch: 15 [5760/50000]\tLoss: 4.4292\tLR: 0.010000\n",
            "Training Epoch: 15 [5888/50000]\tLoss: 4.2940\tLR: 0.010000\n",
            "Training Epoch: 15 [6016/50000]\tLoss: 4.4075\tLR: 0.010000\n",
            "Training Epoch: 15 [6144/50000]\tLoss: 4.2821\tLR: 0.010000\n",
            "Training Epoch: 15 [6272/50000]\tLoss: 4.2417\tLR: 0.010000\n",
            "Training Epoch: 15 [6400/50000]\tLoss: 4.3295\tLR: 0.010000\n",
            "Training Epoch: 15 [6528/50000]\tLoss: 4.4194\tLR: 0.010000\n",
            "Training Epoch: 15 [6656/50000]\tLoss: 4.2167\tLR: 0.010000\n",
            "Training Epoch: 15 [6784/50000]\tLoss: 4.3200\tLR: 0.010000\n",
            "Training Epoch: 15 [6912/50000]\tLoss: 4.3161\tLR: 0.010000\n",
            "Training Epoch: 15 [7040/50000]\tLoss: 4.2216\tLR: 0.010000\n",
            "Training Epoch: 15 [7168/50000]\tLoss: 4.3142\tLR: 0.010000\n",
            "Training Epoch: 15 [7296/50000]\tLoss: 4.3117\tLR: 0.010000\n",
            "Training Epoch: 15 [7424/50000]\tLoss: 4.2778\tLR: 0.010000\n",
            "Training Epoch: 15 [7552/50000]\tLoss: 4.3176\tLR: 0.010000\n",
            "Training Epoch: 15 [7680/50000]\tLoss: 4.2783\tLR: 0.010000\n",
            "Training Epoch: 15 [7808/50000]\tLoss: 4.2302\tLR: 0.010000\n",
            "Training Epoch: 15 [7936/50000]\tLoss: 4.4131\tLR: 0.010000\n",
            "Training Epoch: 15 [8064/50000]\tLoss: 4.2930\tLR: 0.010000\n",
            "Training Epoch: 15 [8192/50000]\tLoss: 4.2621\tLR: 0.010000\n",
            "Training Epoch: 15 [8320/50000]\tLoss: 4.2948\tLR: 0.010000\n",
            "Training Epoch: 15 [8448/50000]\tLoss: 4.2891\tLR: 0.010000\n",
            "Training Epoch: 15 [8576/50000]\tLoss: 4.3265\tLR: 0.010000\n",
            "Training Epoch: 15 [8704/50000]\tLoss: 4.4222\tLR: 0.010000\n",
            "Training Epoch: 15 [8832/50000]\tLoss: 4.2349\tLR: 0.010000\n",
            "Training Epoch: 15 [8960/50000]\tLoss: 4.2464\tLR: 0.010000\n",
            "Training Epoch: 15 [9088/50000]\tLoss: 4.3857\tLR: 0.010000\n",
            "Training Epoch: 15 [9216/50000]\tLoss: 4.2688\tLR: 0.010000\n",
            "Training Epoch: 15 [9344/50000]\tLoss: 4.1775\tLR: 0.010000\n",
            "Training Epoch: 15 [9472/50000]\tLoss: 4.2578\tLR: 0.010000\n",
            "Training Epoch: 15 [9600/50000]\tLoss: 4.2081\tLR: 0.010000\n",
            "Training Epoch: 15 [9728/50000]\tLoss: 4.2105\tLR: 0.010000\n",
            "Training Epoch: 15 [9856/50000]\tLoss: 4.2561\tLR: 0.010000\n",
            "Training Epoch: 15 [9984/50000]\tLoss: 4.3167\tLR: 0.010000\n",
            "Training Epoch: 15 [10112/50000]\tLoss: 4.2844\tLR: 0.010000\n",
            "Training Epoch: 15 [10240/50000]\tLoss: 4.2729\tLR: 0.010000\n",
            "Training Epoch: 15 [10368/50000]\tLoss: 4.2691\tLR: 0.010000\n",
            "Training Epoch: 15 [10496/50000]\tLoss: 4.3338\tLR: 0.010000\n",
            "Training Epoch: 15 [10624/50000]\tLoss: 4.2481\tLR: 0.010000\n",
            "Training Epoch: 15 [10752/50000]\tLoss: 4.3272\tLR: 0.010000\n",
            "Training Epoch: 15 [10880/50000]\tLoss: 4.3617\tLR: 0.010000\n",
            "Training Epoch: 15 [11008/50000]\tLoss: 4.3647\tLR: 0.010000\n",
            "Training Epoch: 15 [11136/50000]\tLoss: 4.3490\tLR: 0.010000\n",
            "Training Epoch: 15 [11264/50000]\tLoss: 4.2792\tLR: 0.010000\n",
            "Training Epoch: 15 [11392/50000]\tLoss: 4.3794\tLR: 0.010000\n",
            "Training Epoch: 15 [11520/50000]\tLoss: 4.3968\tLR: 0.010000\n",
            "Training Epoch: 15 [11648/50000]\tLoss: 4.3646\tLR: 0.010000\n",
            "Training Epoch: 15 [11776/50000]\tLoss: 4.2428\tLR: 0.010000\n",
            "Training Epoch: 15 [11904/50000]\tLoss: 4.2928\tLR: 0.010000\n",
            "Training Epoch: 15 [12032/50000]\tLoss: 4.3757\tLR: 0.010000\n",
            "Training Epoch: 15 [12160/50000]\tLoss: 4.4582\tLR: 0.010000\n",
            "Training Epoch: 15 [12288/50000]\tLoss: 4.2596\tLR: 0.010000\n",
            "Training Epoch: 15 [12416/50000]\tLoss: 4.3245\tLR: 0.010000\n",
            "Training Epoch: 15 [12544/50000]\tLoss: 4.3667\tLR: 0.010000\n",
            "Training Epoch: 15 [12672/50000]\tLoss: 4.3678\tLR: 0.010000\n",
            "Training Epoch: 15 [12800/50000]\tLoss: 4.3651\tLR: 0.010000\n",
            "Training Epoch: 15 [12928/50000]\tLoss: 4.2445\tLR: 0.010000\n",
            "Training Epoch: 15 [13056/50000]\tLoss: 4.2176\tLR: 0.010000\n",
            "Training Epoch: 15 [13184/50000]\tLoss: 4.1931\tLR: 0.010000\n",
            "Training Epoch: 15 [13312/50000]\tLoss: 4.3181\tLR: 0.010000\n",
            "Training Epoch: 15 [13440/50000]\tLoss: 4.2572\tLR: 0.010000\n",
            "Training Epoch: 15 [13568/50000]\tLoss: 4.3501\tLR: 0.010000\n",
            "Training Epoch: 15 [13696/50000]\tLoss: 4.3450\tLR: 0.010000\n",
            "Training Epoch: 15 [13824/50000]\tLoss: 4.3039\tLR: 0.010000\n",
            "Training Epoch: 15 [13952/50000]\tLoss: 4.4002\tLR: 0.010000\n",
            "Training Epoch: 15 [14080/50000]\tLoss: 4.2642\tLR: 0.010000\n",
            "Training Epoch: 15 [14208/50000]\tLoss: 4.2422\tLR: 0.010000\n",
            "Training Epoch: 15 [14336/50000]\tLoss: 4.4496\tLR: 0.010000\n",
            "Training Epoch: 15 [14464/50000]\tLoss: 4.1867\tLR: 0.010000\n",
            "Training Epoch: 15 [14592/50000]\tLoss: 4.2060\tLR: 0.010000\n",
            "Training Epoch: 15 [14720/50000]\tLoss: 4.3514\tLR: 0.010000\n",
            "Training Epoch: 15 [14848/50000]\tLoss: 4.5022\tLR: 0.010000\n",
            "Training Epoch: 15 [14976/50000]\tLoss: 4.1807\tLR: 0.010000\n",
            "Training Epoch: 15 [15104/50000]\tLoss: 4.4141\tLR: 0.010000\n",
            "Training Epoch: 15 [15232/50000]\tLoss: 4.1648\tLR: 0.010000\n",
            "Training Epoch: 15 [15360/50000]\tLoss: 4.3032\tLR: 0.010000\n",
            "Training Epoch: 15 [15488/50000]\tLoss: 4.3468\tLR: 0.010000\n",
            "Training Epoch: 15 [15616/50000]\tLoss: 4.2999\tLR: 0.010000\n",
            "Training Epoch: 15 [15744/50000]\tLoss: 4.2984\tLR: 0.010000\n",
            "Training Epoch: 15 [15872/50000]\tLoss: 4.3439\tLR: 0.010000\n",
            "Training Epoch: 15 [16000/50000]\tLoss: 4.2445\tLR: 0.010000\n",
            "Training Epoch: 15 [16128/50000]\tLoss: 4.2546\tLR: 0.010000\n",
            "Training Epoch: 15 [16256/50000]\tLoss: 4.3396\tLR: 0.010000\n",
            "Training Epoch: 15 [16384/50000]\tLoss: 4.4009\tLR: 0.010000\n",
            "Training Epoch: 15 [16512/50000]\tLoss: 4.3737\tLR: 0.010000\n",
            "Training Epoch: 15 [16640/50000]\tLoss: 4.3387\tLR: 0.010000\n",
            "Training Epoch: 15 [16768/50000]\tLoss: 4.3250\tLR: 0.010000\n",
            "Training Epoch: 15 [16896/50000]\tLoss: 4.3530\tLR: 0.010000\n",
            "Training Epoch: 15 [17024/50000]\tLoss: 4.3268\tLR: 0.010000\n",
            "Training Epoch: 15 [17152/50000]\tLoss: 4.2577\tLR: 0.010000\n",
            "Training Epoch: 15 [17280/50000]\tLoss: 4.2585\tLR: 0.010000\n",
            "Training Epoch: 15 [17408/50000]\tLoss: 4.2685\tLR: 0.010000\n",
            "Training Epoch: 15 [17536/50000]\tLoss: 4.3781\tLR: 0.010000\n",
            "Training Epoch: 15 [17664/50000]\tLoss: 4.2493\tLR: 0.010000\n",
            "Training Epoch: 15 [17792/50000]\tLoss: 4.3554\tLR: 0.010000\n",
            "Training Epoch: 15 [17920/50000]\tLoss: 4.2658\tLR: 0.010000\n",
            "Training Epoch: 15 [18048/50000]\tLoss: 4.2883\tLR: 0.010000\n",
            "Training Epoch: 15 [18176/50000]\tLoss: 4.3262\tLR: 0.010000\n",
            "Training Epoch: 15 [18304/50000]\tLoss: 4.3402\tLR: 0.010000\n",
            "Training Epoch: 15 [18432/50000]\tLoss: 4.2584\tLR: 0.010000\n",
            "Training Epoch: 15 [18560/50000]\tLoss: 4.3374\tLR: 0.010000\n",
            "Training Epoch: 15 [18688/50000]\tLoss: 4.3522\tLR: 0.010000\n",
            "Training Epoch: 15 [18816/50000]\tLoss: 4.2321\tLR: 0.010000\n",
            "Training Epoch: 15 [18944/50000]\tLoss: 4.2487\tLR: 0.010000\n",
            "Training Epoch: 15 [19072/50000]\tLoss: 4.2526\tLR: 0.010000\n",
            "Training Epoch: 15 [19200/50000]\tLoss: 4.1726\tLR: 0.010000\n",
            "Training Epoch: 15 [19328/50000]\tLoss: 4.3973\tLR: 0.010000\n",
            "Training Epoch: 15 [19456/50000]\tLoss: 4.2425\tLR: 0.010000\n",
            "Training Epoch: 15 [19584/50000]\tLoss: 4.2516\tLR: 0.010000\n",
            "Training Epoch: 15 [19712/50000]\tLoss: 4.2651\tLR: 0.010000\n",
            "Training Epoch: 15 [19840/50000]\tLoss: 4.1755\tLR: 0.010000\n",
            "Training Epoch: 15 [19968/50000]\tLoss: 4.2532\tLR: 0.010000\n",
            "Training Epoch: 15 [20096/50000]\tLoss: 4.3406\tLR: 0.010000\n",
            "Training Epoch: 15 [20224/50000]\tLoss: 4.3028\tLR: 0.010000\n",
            "Training Epoch: 15 [20352/50000]\tLoss: 4.2874\tLR: 0.010000\n",
            "Training Epoch: 15 [20480/50000]\tLoss: 4.2631\tLR: 0.010000\n",
            "Training Epoch: 15 [20608/50000]\tLoss: 4.3287\tLR: 0.010000\n",
            "Training Epoch: 15 [20736/50000]\tLoss: 4.2933\tLR: 0.010000\n",
            "Training Epoch: 15 [20864/50000]\tLoss: 4.2958\tLR: 0.010000\n",
            "Training Epoch: 15 [20992/50000]\tLoss: 4.1688\tLR: 0.010000\n",
            "Training Epoch: 15 [21120/50000]\tLoss: 4.1993\tLR: 0.010000\n",
            "Training Epoch: 15 [21248/50000]\tLoss: 4.2600\tLR: 0.010000\n",
            "Training Epoch: 15 [21376/50000]\tLoss: 4.3457\tLR: 0.010000\n",
            "Training Epoch: 15 [21504/50000]\tLoss: 4.2089\tLR: 0.010000\n",
            "Training Epoch: 15 [21632/50000]\tLoss: 4.2194\tLR: 0.010000\n",
            "Training Epoch: 15 [21760/50000]\tLoss: 4.2088\tLR: 0.010000\n",
            "Training Epoch: 15 [21888/50000]\tLoss: 4.2209\tLR: 0.010000\n",
            "Training Epoch: 15 [22016/50000]\tLoss: 4.2451\tLR: 0.010000\n",
            "Training Epoch: 15 [22144/50000]\tLoss: 4.3409\tLR: 0.010000\n",
            "Training Epoch: 15 [22272/50000]\tLoss: 4.4138\tLR: 0.010000\n",
            "Training Epoch: 15 [22400/50000]\tLoss: 4.1568\tLR: 0.010000\n",
            "Training Epoch: 15 [22528/50000]\tLoss: 4.1826\tLR: 0.010000\n",
            "Training Epoch: 15 [22656/50000]\tLoss: 4.2433\tLR: 0.010000\n",
            "Training Epoch: 15 [22784/50000]\tLoss: 4.3239\tLR: 0.010000\n",
            "Training Epoch: 15 [22912/50000]\tLoss: 4.3114\tLR: 0.010000\n",
            "Training Epoch: 15 [23040/50000]\tLoss: 4.2908\tLR: 0.010000\n",
            "Training Epoch: 15 [23168/50000]\tLoss: 4.3542\tLR: 0.010000\n",
            "Training Epoch: 15 [23296/50000]\tLoss: 4.2357\tLR: 0.010000\n",
            "Training Epoch: 15 [23424/50000]\tLoss: 4.2055\tLR: 0.010000\n",
            "Training Epoch: 15 [23552/50000]\tLoss: 4.2847\tLR: 0.010000\n",
            "Training Epoch: 15 [23680/50000]\tLoss: 4.3365\tLR: 0.010000\n",
            "Training Epoch: 15 [23808/50000]\tLoss: 4.2871\tLR: 0.010000\n",
            "Training Epoch: 15 [23936/50000]\tLoss: 4.2328\tLR: 0.010000\n",
            "Training Epoch: 15 [24064/50000]\tLoss: 4.3088\tLR: 0.010000\n",
            "Training Epoch: 15 [24192/50000]\tLoss: 4.3612\tLR: 0.010000\n",
            "Training Epoch: 15 [24320/50000]\tLoss: 4.1667\tLR: 0.010000\n",
            "Training Epoch: 15 [24448/50000]\tLoss: 4.2684\tLR: 0.010000\n",
            "Training Epoch: 15 [24576/50000]\tLoss: 4.2653\tLR: 0.010000\n",
            "Training Epoch: 15 [24704/50000]\tLoss: 4.3709\tLR: 0.010000\n",
            "Training Epoch: 15 [24832/50000]\tLoss: 4.3113\tLR: 0.010000\n",
            "Training Epoch: 15 [24960/50000]\tLoss: 4.3187\tLR: 0.010000\n",
            "Training Epoch: 15 [25088/50000]\tLoss: 4.1951\tLR: 0.010000\n",
            "Training Epoch: 15 [25216/50000]\tLoss: 4.3830\tLR: 0.010000\n",
            "Training Epoch: 15 [25344/50000]\tLoss: 4.3059\tLR: 0.010000\n",
            "Training Epoch: 15 [25472/50000]\tLoss: 4.2603\tLR: 0.010000\n",
            "Training Epoch: 15 [25600/50000]\tLoss: 4.2103\tLR: 0.010000\n",
            "Training Epoch: 15 [25728/50000]\tLoss: 4.2198\tLR: 0.010000\n",
            "Training Epoch: 15 [25856/50000]\tLoss: 4.2426\tLR: 0.010000\n",
            "Training Epoch: 15 [25984/50000]\tLoss: 4.3209\tLR: 0.010000\n",
            "Training Epoch: 15 [26112/50000]\tLoss: 4.2551\tLR: 0.010000\n",
            "Training Epoch: 15 [26240/50000]\tLoss: 4.0961\tLR: 0.010000\n",
            "Training Epoch: 15 [26368/50000]\tLoss: 4.2606\tLR: 0.010000\n",
            "Training Epoch: 15 [26496/50000]\tLoss: 4.3725\tLR: 0.010000\n",
            "Training Epoch: 15 [26624/50000]\tLoss: 4.2728\tLR: 0.010000\n",
            "Training Epoch: 15 [26752/50000]\tLoss: 4.2430\tLR: 0.010000\n",
            "Training Epoch: 15 [26880/50000]\tLoss: 4.4349\tLR: 0.010000\n",
            "Training Epoch: 15 [27008/50000]\tLoss: 4.2022\tLR: 0.010000\n",
            "Training Epoch: 15 [27136/50000]\tLoss: 4.2548\tLR: 0.010000\n",
            "Training Epoch: 15 [27264/50000]\tLoss: 4.2423\tLR: 0.010000\n",
            "Training Epoch: 15 [27392/50000]\tLoss: 4.3930\tLR: 0.010000\n",
            "Training Epoch: 15 [27520/50000]\tLoss: 4.1895\tLR: 0.010000\n",
            "Training Epoch: 15 [27648/50000]\tLoss: 4.3437\tLR: 0.010000\n",
            "Training Epoch: 15 [27776/50000]\tLoss: 4.2998\tLR: 0.010000\n",
            "Training Epoch: 15 [27904/50000]\tLoss: 4.4278\tLR: 0.010000\n",
            "Training Epoch: 15 [28032/50000]\tLoss: 4.3523\tLR: 0.010000\n",
            "Training Epoch: 15 [28160/50000]\tLoss: 4.2880\tLR: 0.010000\n",
            "Training Epoch: 15 [28288/50000]\tLoss: 4.2362\tLR: 0.010000\n",
            "Training Epoch: 15 [28416/50000]\tLoss: 4.1820\tLR: 0.010000\n",
            "Training Epoch: 15 [28544/50000]\tLoss: 4.3106\tLR: 0.010000\n",
            "Training Epoch: 15 [28672/50000]\tLoss: 4.2429\tLR: 0.010000\n",
            "Training Epoch: 15 [28800/50000]\tLoss: 4.2992\tLR: 0.010000\n",
            "Training Epoch: 15 [28928/50000]\tLoss: 4.3385\tLR: 0.010000\n",
            "Training Epoch: 15 [29056/50000]\tLoss: 4.2797\tLR: 0.010000\n",
            "Training Epoch: 15 [29184/50000]\tLoss: 4.2656\tLR: 0.010000\n",
            "Training Epoch: 15 [29312/50000]\tLoss: 4.2199\tLR: 0.010000\n",
            "Training Epoch: 15 [29440/50000]\tLoss: 4.2172\tLR: 0.010000\n",
            "Training Epoch: 15 [29568/50000]\tLoss: 4.2416\tLR: 0.010000\n",
            "Training Epoch: 15 [29696/50000]\tLoss: 4.3759\tLR: 0.010000\n",
            "Training Epoch: 15 [29824/50000]\tLoss: 4.3729\tLR: 0.010000\n",
            "Training Epoch: 15 [29952/50000]\tLoss: 4.2739\tLR: 0.010000\n",
            "Training Epoch: 15 [30080/50000]\tLoss: 4.2710\tLR: 0.010000\n",
            "Training Epoch: 15 [30208/50000]\tLoss: 4.2608\tLR: 0.010000\n",
            "Training Epoch: 15 [30336/50000]\tLoss: 4.3032\tLR: 0.010000\n",
            "Training Epoch: 15 [30464/50000]\tLoss: 4.1448\tLR: 0.010000\n",
            "Training Epoch: 15 [30592/50000]\tLoss: 4.2902\tLR: 0.010000\n",
            "Training Epoch: 15 [30720/50000]\tLoss: 4.4188\tLR: 0.010000\n",
            "Training Epoch: 15 [30848/50000]\tLoss: 4.2933\tLR: 0.010000\n",
            "Training Epoch: 15 [30976/50000]\tLoss: 4.2191\tLR: 0.010000\n",
            "Training Epoch: 15 [31104/50000]\tLoss: 4.2098\tLR: 0.010000\n",
            "Training Epoch: 15 [31232/50000]\tLoss: 4.3276\tLR: 0.010000\n",
            "Training Epoch: 15 [31360/50000]\tLoss: 4.1513\tLR: 0.010000\n",
            "Training Epoch: 15 [31488/50000]\tLoss: 4.3240\tLR: 0.010000\n",
            "Training Epoch: 15 [31616/50000]\tLoss: 4.3378\tLR: 0.010000\n",
            "Training Epoch: 15 [31744/50000]\tLoss: 4.1323\tLR: 0.010000\n",
            "Training Epoch: 15 [31872/50000]\tLoss: 4.2618\tLR: 0.010000\n",
            "Training Epoch: 15 [32000/50000]\tLoss: 4.3162\tLR: 0.010000\n",
            "Training Epoch: 15 [32128/50000]\tLoss: 4.1737\tLR: 0.010000\n",
            "Training Epoch: 15 [32256/50000]\tLoss: 4.3979\tLR: 0.010000\n",
            "Training Epoch: 15 [32384/50000]\tLoss: 4.2659\tLR: 0.010000\n",
            "Training Epoch: 15 [32512/50000]\tLoss: 4.2445\tLR: 0.010000\n",
            "Training Epoch: 15 [32640/50000]\tLoss: 4.2103\tLR: 0.010000\n",
            "Training Epoch: 15 [32768/50000]\tLoss: 4.1516\tLR: 0.010000\n",
            "Training Epoch: 15 [32896/50000]\tLoss: 4.1981\tLR: 0.010000\n",
            "Training Epoch: 15 [33024/50000]\tLoss: 4.2467\tLR: 0.010000\n",
            "Training Epoch: 15 [33152/50000]\tLoss: 4.4093\tLR: 0.010000\n",
            "Training Epoch: 15 [33280/50000]\tLoss: 4.3863\tLR: 0.010000\n",
            "Training Epoch: 15 [33408/50000]\tLoss: 4.1987\tLR: 0.010000\n",
            "Training Epoch: 15 [33536/50000]\tLoss: 4.2300\tLR: 0.010000\n",
            "Training Epoch: 15 [33664/50000]\tLoss: 4.2481\tLR: 0.010000\n",
            "Training Epoch: 15 [33792/50000]\tLoss: 4.1494\tLR: 0.010000\n",
            "Training Epoch: 15 [33920/50000]\tLoss: 4.2882\tLR: 0.010000\n",
            "Training Epoch: 15 [34048/50000]\tLoss: 4.3710\tLR: 0.010000\n",
            "Training Epoch: 15 [34176/50000]\tLoss: 4.1921\tLR: 0.010000\n",
            "Training Epoch: 15 [34304/50000]\tLoss: 4.2856\tLR: 0.010000\n",
            "Training Epoch: 15 [34432/50000]\tLoss: 4.2785\tLR: 0.010000\n",
            "Training Epoch: 15 [34560/50000]\tLoss: 4.3097\tLR: 0.010000\n",
            "Training Epoch: 15 [34688/50000]\tLoss: 4.2308\tLR: 0.010000\n",
            "Training Epoch: 15 [34816/50000]\tLoss: 4.2382\tLR: 0.010000\n",
            "Training Epoch: 15 [34944/50000]\tLoss: 4.1651\tLR: 0.010000\n",
            "Training Epoch: 15 [35072/50000]\tLoss: 4.4306\tLR: 0.010000\n",
            "Training Epoch: 15 [35200/50000]\tLoss: 4.2929\tLR: 0.010000\n",
            "Training Epoch: 15 [35328/50000]\tLoss: 4.2113\tLR: 0.010000\n",
            "Training Epoch: 15 [35456/50000]\tLoss: 4.2404\tLR: 0.010000\n",
            "Training Epoch: 15 [35584/50000]\tLoss: 4.2629\tLR: 0.010000\n",
            "Training Epoch: 15 [35712/50000]\tLoss: 4.2836\tLR: 0.010000\n",
            "Training Epoch: 15 [35840/50000]\tLoss: 4.2620\tLR: 0.010000\n",
            "Training Epoch: 15 [35968/50000]\tLoss: 4.2094\tLR: 0.010000\n",
            "Training Epoch: 15 [36096/50000]\tLoss: 4.2499\tLR: 0.010000\n",
            "Training Epoch: 15 [36224/50000]\tLoss: 4.2545\tLR: 0.010000\n",
            "Training Epoch: 15 [36352/50000]\tLoss: 4.3379\tLR: 0.010000\n",
            "Training Epoch: 15 [36480/50000]\tLoss: 4.2217\tLR: 0.010000\n",
            "Training Epoch: 15 [36608/50000]\tLoss: 4.0389\tLR: 0.010000\n",
            "Training Epoch: 15 [36736/50000]\tLoss: 4.2204\tLR: 0.010000\n",
            "Training Epoch: 15 [36864/50000]\tLoss: 4.1502\tLR: 0.010000\n",
            "Training Epoch: 15 [36992/50000]\tLoss: 4.2554\tLR: 0.010000\n",
            "Training Epoch: 15 [37120/50000]\tLoss: 4.3491\tLR: 0.010000\n",
            "Training Epoch: 15 [37248/50000]\tLoss: 4.1906\tLR: 0.010000\n",
            "Training Epoch: 15 [37376/50000]\tLoss: 4.3329\tLR: 0.010000\n",
            "Training Epoch: 15 [37504/50000]\tLoss: 4.2340\tLR: 0.010000\n",
            "Training Epoch: 15 [37632/50000]\tLoss: 4.2348\tLR: 0.010000\n",
            "Training Epoch: 15 [37760/50000]\tLoss: 4.3288\tLR: 0.010000\n",
            "Training Epoch: 15 [37888/50000]\tLoss: 4.2008\tLR: 0.010000\n",
            "Training Epoch: 15 [38016/50000]\tLoss: 4.2588\tLR: 0.010000\n",
            "Training Epoch: 15 [38144/50000]\tLoss: 4.2359\tLR: 0.010000\n",
            "Training Epoch: 15 [38272/50000]\tLoss: 4.1891\tLR: 0.010000\n",
            "Training Epoch: 15 [38400/50000]\tLoss: 4.1952\tLR: 0.010000\n",
            "Training Epoch: 15 [38528/50000]\tLoss: 4.2922\tLR: 0.010000\n",
            "Training Epoch: 15 [38656/50000]\tLoss: 4.3161\tLR: 0.010000\n",
            "Training Epoch: 15 [38784/50000]\tLoss: 4.2261\tLR: 0.010000\n",
            "Training Epoch: 15 [38912/50000]\tLoss: 4.2830\tLR: 0.010000\n",
            "Training Epoch: 15 [39040/50000]\tLoss: 4.2298\tLR: 0.010000\n",
            "Training Epoch: 15 [39168/50000]\tLoss: 4.1195\tLR: 0.010000\n",
            "Training Epoch: 15 [39296/50000]\tLoss: 4.2868\tLR: 0.010000\n",
            "Training Epoch: 15 [39424/50000]\tLoss: 4.3874\tLR: 0.010000\n",
            "Training Epoch: 15 [39552/50000]\tLoss: 4.2220\tLR: 0.010000\n",
            "Training Epoch: 15 [39680/50000]\tLoss: 4.1815\tLR: 0.010000\n",
            "Training Epoch: 15 [39808/50000]\tLoss: 4.1873\tLR: 0.010000\n",
            "Training Epoch: 15 [39936/50000]\tLoss: 4.2697\tLR: 0.010000\n",
            "Training Epoch: 15 [40064/50000]\tLoss: 4.3839\tLR: 0.010000\n",
            "Training Epoch: 15 [40192/50000]\tLoss: 4.2275\tLR: 0.010000\n",
            "Training Epoch: 15 [40320/50000]\tLoss: 4.2395\tLR: 0.010000\n",
            "Training Epoch: 15 [40448/50000]\tLoss: 4.1621\tLR: 0.010000\n",
            "Training Epoch: 15 [40576/50000]\tLoss: 4.3578\tLR: 0.010000\n",
            "Training Epoch: 15 [40704/50000]\tLoss: 4.1358\tLR: 0.010000\n",
            "Training Epoch: 15 [40832/50000]\tLoss: 4.3391\tLR: 0.010000\n",
            "Training Epoch: 15 [40960/50000]\tLoss: 4.3003\tLR: 0.010000\n",
            "Training Epoch: 15 [41088/50000]\tLoss: 4.1196\tLR: 0.010000\n",
            "Training Epoch: 15 [41216/50000]\tLoss: 4.1892\tLR: 0.010000\n",
            "Training Epoch: 15 [41344/50000]\tLoss: 4.1126\tLR: 0.010000\n",
            "Training Epoch: 15 [41472/50000]\tLoss: 4.3680\tLR: 0.010000\n",
            "Training Epoch: 15 [41600/50000]\tLoss: 4.3043\tLR: 0.010000\n",
            "Training Epoch: 15 [41728/50000]\tLoss: 4.2697\tLR: 0.010000\n",
            "Training Epoch: 15 [41856/50000]\tLoss: 4.2664\tLR: 0.010000\n",
            "Training Epoch: 15 [41984/50000]\tLoss: 4.2442\tLR: 0.010000\n",
            "Training Epoch: 15 [42112/50000]\tLoss: 4.1629\tLR: 0.010000\n",
            "Training Epoch: 15 [42240/50000]\tLoss: 4.1199\tLR: 0.010000\n",
            "Training Epoch: 15 [42368/50000]\tLoss: 4.2705\tLR: 0.010000\n",
            "Training Epoch: 15 [42496/50000]\tLoss: 4.2785\tLR: 0.010000\n",
            "Training Epoch: 15 [42624/50000]\tLoss: 4.2080\tLR: 0.010000\n",
            "Training Epoch: 15 [42752/50000]\tLoss: 4.3875\tLR: 0.010000\n",
            "Training Epoch: 15 [42880/50000]\tLoss: 4.2086\tLR: 0.010000\n",
            "Training Epoch: 15 [43008/50000]\tLoss: 4.3020\tLR: 0.010000\n",
            "Training Epoch: 15 [43136/50000]\tLoss: 4.2888\tLR: 0.010000\n",
            "Training Epoch: 15 [43264/50000]\tLoss: 4.3452\tLR: 0.010000\n",
            "Training Epoch: 15 [43392/50000]\tLoss: 4.1915\tLR: 0.010000\n",
            "Training Epoch: 15 [43520/50000]\tLoss: 4.2380\tLR: 0.010000\n",
            "Training Epoch: 15 [43648/50000]\tLoss: 4.1812\tLR: 0.010000\n",
            "Training Epoch: 15 [43776/50000]\tLoss: 4.2510\tLR: 0.010000\n",
            "Training Epoch: 15 [43904/50000]\tLoss: 4.2316\tLR: 0.010000\n",
            "Training Epoch: 15 [44032/50000]\tLoss: 4.2930\tLR: 0.010000\n",
            "Training Epoch: 15 [44160/50000]\tLoss: 4.2807\tLR: 0.010000\n",
            "Training Epoch: 15 [44288/50000]\tLoss: 4.2141\tLR: 0.010000\n",
            "Training Epoch: 15 [44416/50000]\tLoss: 4.2492\tLR: 0.010000\n",
            "Training Epoch: 15 [44544/50000]\tLoss: 4.1351\tLR: 0.010000\n",
            "Training Epoch: 15 [44672/50000]\tLoss: 4.1623\tLR: 0.010000\n",
            "Training Epoch: 15 [44800/50000]\tLoss: 4.3731\tLR: 0.010000\n",
            "Training Epoch: 15 [44928/50000]\tLoss: 4.2577\tLR: 0.010000\n",
            "Training Epoch: 15 [45056/50000]\tLoss: 4.1957\tLR: 0.010000\n",
            "Training Epoch: 15 [45184/50000]\tLoss: 4.1255\tLR: 0.010000\n",
            "Training Epoch: 15 [45312/50000]\tLoss: 4.1925\tLR: 0.010000\n",
            "Training Epoch: 15 [45440/50000]\tLoss: 4.3443\tLR: 0.010000\n",
            "Training Epoch: 15 [45568/50000]\tLoss: 4.1637\tLR: 0.010000\n",
            "Training Epoch: 15 [45696/50000]\tLoss: 4.1538\tLR: 0.010000\n",
            "Training Epoch: 15 [45824/50000]\tLoss: 4.2665\tLR: 0.010000\n",
            "Training Epoch: 15 [45952/50000]\tLoss: 4.4257\tLR: 0.010000\n",
            "Training Epoch: 15 [46080/50000]\tLoss: 4.2187\tLR: 0.010000\n",
            "Training Epoch: 15 [46208/50000]\tLoss: 4.0999\tLR: 0.010000\n",
            "Training Epoch: 15 [46336/50000]\tLoss: 4.2236\tLR: 0.010000\n",
            "Training Epoch: 15 [46464/50000]\tLoss: 4.0964\tLR: 0.010000\n",
            "Training Epoch: 15 [46592/50000]\tLoss: 4.2630\tLR: 0.010000\n",
            "Training Epoch: 15 [46720/50000]\tLoss: 4.2903\tLR: 0.010000\n",
            "Training Epoch: 15 [46848/50000]\tLoss: 4.0579\tLR: 0.010000\n",
            "Training Epoch: 15 [46976/50000]\tLoss: 4.3108\tLR: 0.010000\n",
            "Training Epoch: 15 [47104/50000]\tLoss: 4.1583\tLR: 0.010000\n",
            "Training Epoch: 15 [47232/50000]\tLoss: 4.1975\tLR: 0.010000\n",
            "Training Epoch: 15 [47360/50000]\tLoss: 4.2189\tLR: 0.010000\n",
            "Training Epoch: 15 [47488/50000]\tLoss: 4.3145\tLR: 0.010000\n",
            "Training Epoch: 15 [47616/50000]\tLoss: 4.1278\tLR: 0.010000\n",
            "Training Epoch: 15 [47744/50000]\tLoss: 4.3402\tLR: 0.010000\n",
            "Training Epoch: 15 [47872/50000]\tLoss: 4.2600\tLR: 0.010000\n",
            "Training Epoch: 15 [48000/50000]\tLoss: 4.1528\tLR: 0.010000\n",
            "Training Epoch: 15 [48128/50000]\tLoss: 4.1666\tLR: 0.010000\n",
            "Training Epoch: 15 [48256/50000]\tLoss: 4.2453\tLR: 0.010000\n",
            "Training Epoch: 15 [48384/50000]\tLoss: 4.1674\tLR: 0.010000\n",
            "Training Epoch: 15 [48512/50000]\tLoss: 4.1983\tLR: 0.010000\n",
            "Training Epoch: 15 [48640/50000]\tLoss: 4.3258\tLR: 0.010000\n",
            "Training Epoch: 15 [48768/50000]\tLoss: 4.2831\tLR: 0.010000\n",
            "Training Epoch: 15 [48896/50000]\tLoss: 4.2212\tLR: 0.010000\n",
            "Training Epoch: 15 [49024/50000]\tLoss: 4.1587\tLR: 0.010000\n",
            "Training Epoch: 15 [49152/50000]\tLoss: 4.1157\tLR: 0.010000\n",
            "Training Epoch: 15 [49280/50000]\tLoss: 4.3136\tLR: 0.010000\n",
            "Training Epoch: 15 [49408/50000]\tLoss: 4.1013\tLR: 0.010000\n",
            "Training Epoch: 15 [49536/50000]\tLoss: 4.3018\tLR: 0.010000\n",
            "Training Epoch: 15 [49664/50000]\tLoss: 4.1204\tLR: 0.010000\n",
            "Training Epoch: 15 [49792/50000]\tLoss: 4.2710\tLR: 0.010000\n",
            "Training Epoch: 15 [49920/50000]\tLoss: 4.2047\tLR: 0.010000\n",
            "Training Epoch: 15 [50000/50000]\tLoss: 4.2524\tLR: 0.010000\n",
            "epoch 15 training time consumed: 144.71s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 159225 GiB | 159225 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 157216 GiB | 157216 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2009 GiB |   2008 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 159225 GiB | 159225 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 157216 GiB | 157216 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2009 GiB |   2008 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 158789 GiB | 158789 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 156781 GiB | 156781 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2008 GiB |   2008 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 132642 GiB | 132642 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 130372 GiB | 130372 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2269 GiB |   2269 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |    9702 K  |    9701 K  |\n",
            "|       from large pool |       5    |     146    |    4702 K  |    4702 K  |\n",
            "|       from small pool |     516    |     682    |    4999 K  |    4998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |    9702 K  |    9701 K  |\n",
            "|       from large pool |       5    |     146    |    4702 K  |    4702 K  |\n",
            "|       from small pool |     516    |     682    |    4999 K  |    4998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |     117    |    3612 K  |    3612 K  |\n",
            "|       from large pool |       4    |      46    |    2214 K  |    2214 K  |\n",
            "|       from small pool |      74    |      89    |    1397 K  |    1397 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0334, Accuracy: 0.0479, Time consumed:9.45s\n",
            "\n",
            "Training Epoch: 16 [128/50000]\tLoss: 4.1581\tLR: 0.010000\n",
            "Training Epoch: 16 [256/50000]\tLoss: 4.1927\tLR: 0.010000\n",
            "Training Epoch: 16 [384/50000]\tLoss: 4.2328\tLR: 0.010000\n",
            "Training Epoch: 16 [512/50000]\tLoss: 4.2493\tLR: 0.010000\n",
            "Training Epoch: 16 [640/50000]\tLoss: 4.2001\tLR: 0.010000\n",
            "Training Epoch: 16 [768/50000]\tLoss: 4.2049\tLR: 0.010000\n",
            "Training Epoch: 16 [896/50000]\tLoss: 4.0817\tLR: 0.010000\n",
            "Training Epoch: 16 [1024/50000]\tLoss: 4.3471\tLR: 0.010000\n",
            "Training Epoch: 16 [1152/50000]\tLoss: 4.2999\tLR: 0.010000\n",
            "Training Epoch: 16 [1280/50000]\tLoss: 4.2562\tLR: 0.010000\n",
            "Training Epoch: 16 [1408/50000]\tLoss: 4.3562\tLR: 0.010000\n",
            "Training Epoch: 16 [1536/50000]\tLoss: 4.1863\tLR: 0.010000\n",
            "Training Epoch: 16 [1664/50000]\tLoss: 4.2152\tLR: 0.010000\n",
            "Training Epoch: 16 [1792/50000]\tLoss: 4.1979\tLR: 0.010000\n",
            "Training Epoch: 16 [1920/50000]\tLoss: 4.2157\tLR: 0.010000\n",
            "Training Epoch: 16 [2048/50000]\tLoss: 4.0926\tLR: 0.010000\n",
            "Training Epoch: 16 [2176/50000]\tLoss: 4.1822\tLR: 0.010000\n",
            "Training Epoch: 16 [2304/50000]\tLoss: 4.1623\tLR: 0.010000\n",
            "Training Epoch: 16 [2432/50000]\tLoss: 4.1538\tLR: 0.010000\n",
            "Training Epoch: 16 [2560/50000]\tLoss: 4.1305\tLR: 0.010000\n",
            "Training Epoch: 16 [2688/50000]\tLoss: 4.3091\tLR: 0.010000\n",
            "Training Epoch: 16 [2816/50000]\tLoss: 4.2234\tLR: 0.010000\n",
            "Training Epoch: 16 [2944/50000]\tLoss: 4.1839\tLR: 0.010000\n",
            "Training Epoch: 16 [3072/50000]\tLoss: 4.1834\tLR: 0.010000\n",
            "Training Epoch: 16 [3200/50000]\tLoss: 4.0867\tLR: 0.010000\n",
            "Training Epoch: 16 [3328/50000]\tLoss: 4.1678\tLR: 0.010000\n",
            "Training Epoch: 16 [3456/50000]\tLoss: 4.3605\tLR: 0.010000\n",
            "Training Epoch: 16 [3584/50000]\tLoss: 4.1368\tLR: 0.010000\n",
            "Training Epoch: 16 [3712/50000]\tLoss: 4.1410\tLR: 0.010000\n",
            "Training Epoch: 16 [3840/50000]\tLoss: 4.2790\tLR: 0.010000\n",
            "Training Epoch: 16 [3968/50000]\tLoss: 4.0620\tLR: 0.010000\n",
            "Training Epoch: 16 [4096/50000]\tLoss: 4.1804\tLR: 0.010000\n",
            "Training Epoch: 16 [4224/50000]\tLoss: 4.1423\tLR: 0.010000\n",
            "Training Epoch: 16 [4352/50000]\tLoss: 4.2189\tLR: 0.010000\n",
            "Training Epoch: 16 [4480/50000]\tLoss: 4.1321\tLR: 0.010000\n",
            "Training Epoch: 16 [4608/50000]\tLoss: 4.0419\tLR: 0.010000\n",
            "Training Epoch: 16 [4736/50000]\tLoss: 4.2063\tLR: 0.010000\n",
            "Training Epoch: 16 [4864/50000]\tLoss: 4.2559\tLR: 0.010000\n",
            "Training Epoch: 16 [4992/50000]\tLoss: 4.3094\tLR: 0.010000\n",
            "Training Epoch: 16 [5120/50000]\tLoss: 4.3444\tLR: 0.010000\n",
            "Training Epoch: 16 [5248/50000]\tLoss: 4.3142\tLR: 0.010000\n",
            "Training Epoch: 16 [5376/50000]\tLoss: 4.3624\tLR: 0.010000\n",
            "Training Epoch: 16 [5504/50000]\tLoss: 4.1633\tLR: 0.010000\n",
            "Training Epoch: 16 [5632/50000]\tLoss: 4.1456\tLR: 0.010000\n",
            "Training Epoch: 16 [5760/50000]\tLoss: 4.2397\tLR: 0.010000\n",
            "Training Epoch: 16 [5888/50000]\tLoss: 4.2561\tLR: 0.010000\n",
            "Training Epoch: 16 [6016/50000]\tLoss: 4.1957\tLR: 0.010000\n",
            "Training Epoch: 16 [6144/50000]\tLoss: 4.1918\tLR: 0.010000\n",
            "Training Epoch: 16 [6272/50000]\tLoss: 4.2771\tLR: 0.010000\n",
            "Training Epoch: 16 [6400/50000]\tLoss: 4.0501\tLR: 0.010000\n",
            "Training Epoch: 16 [6528/50000]\tLoss: 4.3813\tLR: 0.010000\n",
            "Training Epoch: 16 [6656/50000]\tLoss: 4.2791\tLR: 0.010000\n",
            "Training Epoch: 16 [6784/50000]\tLoss: 4.1972\tLR: 0.010000\n",
            "Training Epoch: 16 [6912/50000]\tLoss: 4.2638\tLR: 0.010000\n",
            "Training Epoch: 16 [7040/50000]\tLoss: 4.1005\tLR: 0.010000\n",
            "Training Epoch: 16 [7168/50000]\tLoss: 4.1715\tLR: 0.010000\n",
            "Training Epoch: 16 [7296/50000]\tLoss: 4.2934\tLR: 0.010000\n",
            "Training Epoch: 16 [7424/50000]\tLoss: 4.1603\tLR: 0.010000\n",
            "Training Epoch: 16 [7552/50000]\tLoss: 4.2036\tLR: 0.010000\n",
            "Training Epoch: 16 [7680/50000]\tLoss: 4.1520\tLR: 0.010000\n",
            "Training Epoch: 16 [7808/50000]\tLoss: 4.2067\tLR: 0.010000\n",
            "Training Epoch: 16 [7936/50000]\tLoss: 4.2859\tLR: 0.010000\n",
            "Training Epoch: 16 [8064/50000]\tLoss: 4.1985\tLR: 0.010000\n",
            "Training Epoch: 16 [8192/50000]\tLoss: 4.1486\tLR: 0.010000\n",
            "Training Epoch: 16 [8320/50000]\tLoss: 4.0606\tLR: 0.010000\n",
            "Training Epoch: 16 [8448/50000]\tLoss: 4.3253\tLR: 0.010000\n",
            "Training Epoch: 16 [8576/50000]\tLoss: 4.1180\tLR: 0.010000\n",
            "Training Epoch: 16 [8704/50000]\tLoss: 4.2215\tLR: 0.010000\n",
            "Training Epoch: 16 [8832/50000]\tLoss: 4.2858\tLR: 0.010000\n",
            "Training Epoch: 16 [8960/50000]\tLoss: 4.1986\tLR: 0.010000\n",
            "Training Epoch: 16 [9088/50000]\tLoss: 4.1930\tLR: 0.010000\n",
            "Training Epoch: 16 [9216/50000]\tLoss: 4.2217\tLR: 0.010000\n",
            "Training Epoch: 16 [9344/50000]\tLoss: 4.1921\tLR: 0.010000\n",
            "Training Epoch: 16 [9472/50000]\tLoss: 4.3125\tLR: 0.010000\n",
            "Training Epoch: 16 [9600/50000]\tLoss: 4.2607\tLR: 0.010000\n",
            "Training Epoch: 16 [9728/50000]\tLoss: 4.1774\tLR: 0.010000\n",
            "Training Epoch: 16 [9856/50000]\tLoss: 4.2404\tLR: 0.010000\n",
            "Training Epoch: 16 [9984/50000]\tLoss: 4.0290\tLR: 0.010000\n",
            "Training Epoch: 16 [10112/50000]\tLoss: 4.2327\tLR: 0.010000\n",
            "Training Epoch: 16 [10240/50000]\tLoss: 4.2222\tLR: 0.010000\n",
            "Training Epoch: 16 [10368/50000]\tLoss: 4.3443\tLR: 0.010000\n",
            "Training Epoch: 16 [10496/50000]\tLoss: 4.2874\tLR: 0.010000\n",
            "Training Epoch: 16 [10624/50000]\tLoss: 4.3361\tLR: 0.010000\n",
            "Training Epoch: 16 [10752/50000]\tLoss: 4.1948\tLR: 0.010000\n",
            "Training Epoch: 16 [10880/50000]\tLoss: 4.2213\tLR: 0.010000\n",
            "Training Epoch: 16 [11008/50000]\tLoss: 4.2497\tLR: 0.010000\n",
            "Training Epoch: 16 [11136/50000]\tLoss: 4.2162\tLR: 0.010000\n",
            "Training Epoch: 16 [11264/50000]\tLoss: 3.9914\tLR: 0.010000\n",
            "Training Epoch: 16 [11392/50000]\tLoss: 4.1986\tLR: 0.010000\n",
            "Training Epoch: 16 [11520/50000]\tLoss: 4.2098\tLR: 0.010000\n",
            "Training Epoch: 16 [11648/50000]\tLoss: 4.1117\tLR: 0.010000\n",
            "Training Epoch: 16 [11776/50000]\tLoss: 4.3812\tLR: 0.010000\n",
            "Training Epoch: 16 [11904/50000]\tLoss: 4.3336\tLR: 0.010000\n",
            "Training Epoch: 16 [12032/50000]\tLoss: 4.2107\tLR: 0.010000\n",
            "Training Epoch: 16 [12160/50000]\tLoss: 4.0569\tLR: 0.010000\n",
            "Training Epoch: 16 [12288/50000]\tLoss: 4.2307\tLR: 0.010000\n",
            "Training Epoch: 16 [12416/50000]\tLoss: 4.2195\tLR: 0.010000\n",
            "Training Epoch: 16 [12544/50000]\tLoss: 4.2401\tLR: 0.010000\n",
            "Training Epoch: 16 [12672/50000]\tLoss: 4.0939\tLR: 0.010000\n",
            "Training Epoch: 16 [12800/50000]\tLoss: 4.1616\tLR: 0.010000\n",
            "Training Epoch: 16 [12928/50000]\tLoss: 4.1539\tLR: 0.010000\n",
            "Training Epoch: 16 [13056/50000]\tLoss: 4.2261\tLR: 0.010000\n",
            "Training Epoch: 16 [13184/50000]\tLoss: 4.3154\tLR: 0.010000\n",
            "Training Epoch: 16 [13312/50000]\tLoss: 4.2599\tLR: 0.010000\n",
            "Training Epoch: 16 [13440/50000]\tLoss: 4.1924\tLR: 0.010000\n",
            "Training Epoch: 16 [13568/50000]\tLoss: 4.2088\tLR: 0.010000\n",
            "Training Epoch: 16 [13696/50000]\tLoss: 4.3372\tLR: 0.010000\n",
            "Training Epoch: 16 [13824/50000]\tLoss: 4.2974\tLR: 0.010000\n",
            "Training Epoch: 16 [13952/50000]\tLoss: 4.2853\tLR: 0.010000\n",
            "Training Epoch: 16 [14080/50000]\tLoss: 4.1688\tLR: 0.010000\n",
            "Training Epoch: 16 [14208/50000]\tLoss: 4.2216\tLR: 0.010000\n",
            "Training Epoch: 16 [14336/50000]\tLoss: 4.1978\tLR: 0.010000\n",
            "Training Epoch: 16 [14464/50000]\tLoss: 4.1317\tLR: 0.010000\n",
            "Training Epoch: 16 [14592/50000]\tLoss: 4.1602\tLR: 0.010000\n",
            "Training Epoch: 16 [14720/50000]\tLoss: 4.0434\tLR: 0.010000\n",
            "Training Epoch: 16 [14848/50000]\tLoss: 4.1724\tLR: 0.010000\n",
            "Training Epoch: 16 [14976/50000]\tLoss: 4.1570\tLR: 0.010000\n",
            "Training Epoch: 16 [15104/50000]\tLoss: 4.1604\tLR: 0.010000\n",
            "Training Epoch: 16 [15232/50000]\tLoss: 4.2897\tLR: 0.010000\n",
            "Training Epoch: 16 [15360/50000]\tLoss: 4.3692\tLR: 0.010000\n",
            "Training Epoch: 16 [15488/50000]\tLoss: 4.2228\tLR: 0.010000\n",
            "Training Epoch: 16 [15616/50000]\tLoss: 4.0830\tLR: 0.010000\n",
            "Training Epoch: 16 [15744/50000]\tLoss: 4.1938\tLR: 0.010000\n",
            "Training Epoch: 16 [15872/50000]\tLoss: 4.1662\tLR: 0.010000\n",
            "Training Epoch: 16 [16000/50000]\tLoss: 4.2164\tLR: 0.010000\n",
            "Training Epoch: 16 [16128/50000]\tLoss: 4.2464\tLR: 0.010000\n",
            "Training Epoch: 16 [16256/50000]\tLoss: 4.1652\tLR: 0.010000\n",
            "Training Epoch: 16 [16384/50000]\tLoss: 4.1384\tLR: 0.010000\n",
            "Training Epoch: 16 [16512/50000]\tLoss: 4.1178\tLR: 0.010000\n",
            "Training Epoch: 16 [16640/50000]\tLoss: 4.1855\tLR: 0.010000\n",
            "Training Epoch: 16 [16768/50000]\tLoss: 4.1352\tLR: 0.010000\n",
            "Training Epoch: 16 [16896/50000]\tLoss: 4.1490\tLR: 0.010000\n",
            "Training Epoch: 16 [17024/50000]\tLoss: 4.2170\tLR: 0.010000\n",
            "Training Epoch: 16 [17152/50000]\tLoss: 4.1039\tLR: 0.010000\n",
            "Training Epoch: 16 [17280/50000]\tLoss: 4.1835\tLR: 0.010000\n",
            "Training Epoch: 16 [17408/50000]\tLoss: 4.3550\tLR: 0.010000\n",
            "Training Epoch: 16 [17536/50000]\tLoss: 4.2350\tLR: 0.010000\n",
            "Training Epoch: 16 [17664/50000]\tLoss: 4.1388\tLR: 0.010000\n",
            "Training Epoch: 16 [17792/50000]\tLoss: 4.1698\tLR: 0.010000\n",
            "Training Epoch: 16 [17920/50000]\tLoss: 4.2609\tLR: 0.010000\n",
            "Training Epoch: 16 [18048/50000]\tLoss: 4.1245\tLR: 0.010000\n",
            "Training Epoch: 16 [18176/50000]\tLoss: 4.3047\tLR: 0.010000\n",
            "Training Epoch: 16 [18304/50000]\tLoss: 4.0885\tLR: 0.010000\n",
            "Training Epoch: 16 [18432/50000]\tLoss: 4.1987\tLR: 0.010000\n",
            "Training Epoch: 16 [18560/50000]\tLoss: 4.1556\tLR: 0.010000\n",
            "Training Epoch: 16 [18688/50000]\tLoss: 4.1057\tLR: 0.010000\n",
            "Training Epoch: 16 [18816/50000]\tLoss: 4.1355\tLR: 0.010000\n",
            "Training Epoch: 16 [18944/50000]\tLoss: 4.2437\tLR: 0.010000\n",
            "Training Epoch: 16 [19072/50000]\tLoss: 4.1901\tLR: 0.010000\n",
            "Training Epoch: 16 [19200/50000]\tLoss: 4.1751\tLR: 0.010000\n",
            "Training Epoch: 16 [19328/50000]\tLoss: 4.1669\tLR: 0.010000\n",
            "Training Epoch: 16 [19456/50000]\tLoss: 4.0466\tLR: 0.010000\n",
            "Training Epoch: 16 [19584/50000]\tLoss: 4.2062\tLR: 0.010000\n",
            "Training Epoch: 16 [19712/50000]\tLoss: 4.2009\tLR: 0.010000\n",
            "Training Epoch: 16 [19840/50000]\tLoss: 4.1592\tLR: 0.010000\n",
            "Training Epoch: 16 [19968/50000]\tLoss: 4.2486\tLR: 0.010000\n",
            "Training Epoch: 16 [20096/50000]\tLoss: 4.0875\tLR: 0.010000\n",
            "Training Epoch: 16 [20224/50000]\tLoss: 4.1938\tLR: 0.010000\n",
            "Training Epoch: 16 [20352/50000]\tLoss: 4.1331\tLR: 0.010000\n",
            "Training Epoch: 16 [20480/50000]\tLoss: 4.1749\tLR: 0.010000\n",
            "Training Epoch: 16 [20608/50000]\tLoss: 4.1766\tLR: 0.010000\n",
            "Training Epoch: 16 [20736/50000]\tLoss: 4.2153\tLR: 0.010000\n",
            "Training Epoch: 16 [20864/50000]\tLoss: 4.3191\tLR: 0.010000\n",
            "Training Epoch: 16 [20992/50000]\tLoss: 4.2016\tLR: 0.010000\n",
            "Training Epoch: 16 [21120/50000]\tLoss: 4.0897\tLR: 0.010000\n",
            "Training Epoch: 16 [21248/50000]\tLoss: 4.1190\tLR: 0.010000\n",
            "Training Epoch: 16 [21376/50000]\tLoss: 4.1593\tLR: 0.010000\n",
            "Training Epoch: 16 [21504/50000]\tLoss: 4.2082\tLR: 0.010000\n",
            "Training Epoch: 16 [21632/50000]\tLoss: 4.2472\tLR: 0.010000\n",
            "Training Epoch: 16 [21760/50000]\tLoss: 4.1941\tLR: 0.010000\n",
            "Training Epoch: 16 [21888/50000]\tLoss: 4.0909\tLR: 0.010000\n",
            "Training Epoch: 16 [22016/50000]\tLoss: 4.1729\tLR: 0.010000\n",
            "Training Epoch: 16 [22144/50000]\tLoss: 4.2424\tLR: 0.010000\n",
            "Training Epoch: 16 [22272/50000]\tLoss: 4.1747\tLR: 0.010000\n",
            "Training Epoch: 16 [22400/50000]\tLoss: 4.2165\tLR: 0.010000\n",
            "Training Epoch: 16 [22528/50000]\tLoss: 4.2952\tLR: 0.010000\n",
            "Training Epoch: 16 [22656/50000]\tLoss: 4.1636\tLR: 0.010000\n",
            "Training Epoch: 16 [22784/50000]\tLoss: 4.1186\tLR: 0.010000\n",
            "Training Epoch: 16 [22912/50000]\tLoss: 4.1425\tLR: 0.010000\n",
            "Training Epoch: 16 [23040/50000]\tLoss: 4.1310\tLR: 0.010000\n",
            "Training Epoch: 16 [23168/50000]\tLoss: 4.0230\tLR: 0.010000\n",
            "Training Epoch: 16 [23296/50000]\tLoss: 4.1426\tLR: 0.010000\n",
            "Training Epoch: 16 [23424/50000]\tLoss: 4.2941\tLR: 0.010000\n",
            "Training Epoch: 16 [23552/50000]\tLoss: 4.0425\tLR: 0.010000\n",
            "Training Epoch: 16 [23680/50000]\tLoss: 4.2469\tLR: 0.010000\n",
            "Training Epoch: 16 [23808/50000]\tLoss: 4.1969\tLR: 0.010000\n",
            "Training Epoch: 16 [23936/50000]\tLoss: 4.0578\tLR: 0.010000\n",
            "Training Epoch: 16 [24064/50000]\tLoss: 4.0706\tLR: 0.010000\n",
            "Training Epoch: 16 [24192/50000]\tLoss: 4.2485\tLR: 0.010000\n",
            "Training Epoch: 16 [24320/50000]\tLoss: 4.0886\tLR: 0.010000\n",
            "Training Epoch: 16 [24448/50000]\tLoss: 4.1150\tLR: 0.010000\n",
            "Training Epoch: 16 [24576/50000]\tLoss: 4.1941\tLR: 0.010000\n",
            "Training Epoch: 16 [24704/50000]\tLoss: 4.2230\tLR: 0.010000\n",
            "Training Epoch: 16 [24832/50000]\tLoss: 4.3538\tLR: 0.010000\n",
            "Training Epoch: 16 [24960/50000]\tLoss: 3.9704\tLR: 0.010000\n",
            "Training Epoch: 16 [25088/50000]\tLoss: 4.2281\tLR: 0.010000\n",
            "Training Epoch: 16 [25216/50000]\tLoss: 4.1648\tLR: 0.010000\n",
            "Training Epoch: 16 [25344/50000]\tLoss: 4.2090\tLR: 0.010000\n",
            "Training Epoch: 16 [25472/50000]\tLoss: 4.1529\tLR: 0.010000\n",
            "Training Epoch: 16 [25600/50000]\tLoss: 4.2459\tLR: 0.010000\n",
            "Training Epoch: 16 [25728/50000]\tLoss: 4.0806\tLR: 0.010000\n",
            "Training Epoch: 16 [25856/50000]\tLoss: 4.1689\tLR: 0.010000\n",
            "Training Epoch: 16 [25984/50000]\tLoss: 4.1079\tLR: 0.010000\n",
            "Training Epoch: 16 [26112/50000]\tLoss: 4.1065\tLR: 0.010000\n",
            "Training Epoch: 16 [26240/50000]\tLoss: 4.0858\tLR: 0.010000\n",
            "Training Epoch: 16 [26368/50000]\tLoss: 4.1326\tLR: 0.010000\n",
            "Training Epoch: 16 [26496/50000]\tLoss: 3.9995\tLR: 0.010000\n",
            "Training Epoch: 16 [26624/50000]\tLoss: 4.1241\tLR: 0.010000\n",
            "Training Epoch: 16 [26752/50000]\tLoss: 4.1362\tLR: 0.010000\n",
            "Training Epoch: 16 [26880/50000]\tLoss: 4.1143\tLR: 0.010000\n",
            "Training Epoch: 16 [27008/50000]\tLoss: 4.1599\tLR: 0.010000\n",
            "Training Epoch: 16 [27136/50000]\tLoss: 4.1360\tLR: 0.010000\n",
            "Training Epoch: 16 [27264/50000]\tLoss: 4.2647\tLR: 0.010000\n",
            "Training Epoch: 16 [27392/50000]\tLoss: 4.0728\tLR: 0.010000\n",
            "Training Epoch: 16 [27520/50000]\tLoss: 4.1664\tLR: 0.010000\n",
            "Training Epoch: 16 [27648/50000]\tLoss: 4.1590\tLR: 0.010000\n",
            "Training Epoch: 16 [27776/50000]\tLoss: 4.1163\tLR: 0.010000\n",
            "Training Epoch: 16 [27904/50000]\tLoss: 4.1997\tLR: 0.010000\n",
            "Training Epoch: 16 [28032/50000]\tLoss: 4.1772\tLR: 0.010000\n",
            "Training Epoch: 16 [28160/50000]\tLoss: 4.1623\tLR: 0.010000\n",
            "Training Epoch: 16 [28288/50000]\tLoss: 4.0829\tLR: 0.010000\n",
            "Training Epoch: 16 [28416/50000]\tLoss: 4.0246\tLR: 0.010000\n",
            "Training Epoch: 16 [28544/50000]\tLoss: 4.1392\tLR: 0.010000\n",
            "Training Epoch: 16 [28672/50000]\tLoss: 4.2643\tLR: 0.010000\n",
            "Training Epoch: 16 [28800/50000]\tLoss: 4.1455\tLR: 0.010000\n",
            "Training Epoch: 16 [28928/50000]\tLoss: 4.2007\tLR: 0.010000\n",
            "Training Epoch: 16 [29056/50000]\tLoss: 4.1300\tLR: 0.010000\n",
            "Training Epoch: 16 [29184/50000]\tLoss: 4.3047\tLR: 0.010000\n",
            "Training Epoch: 16 [29312/50000]\tLoss: 4.1345\tLR: 0.010000\n",
            "Training Epoch: 16 [29440/50000]\tLoss: 4.2322\tLR: 0.010000\n",
            "Training Epoch: 16 [29568/50000]\tLoss: 4.2001\tLR: 0.010000\n",
            "Training Epoch: 16 [29696/50000]\tLoss: 4.1994\tLR: 0.010000\n",
            "Training Epoch: 16 [29824/50000]\tLoss: 4.1570\tLR: 0.010000\n",
            "Training Epoch: 16 [29952/50000]\tLoss: 3.9879\tLR: 0.010000\n",
            "Training Epoch: 16 [30080/50000]\tLoss: 4.2381\tLR: 0.010000\n",
            "Training Epoch: 16 [30208/50000]\tLoss: 4.1700\tLR: 0.010000\n",
            "Training Epoch: 16 [30336/50000]\tLoss: 4.1182\tLR: 0.010000\n",
            "Training Epoch: 16 [30464/50000]\tLoss: 4.1052\tLR: 0.010000\n",
            "Training Epoch: 16 [30592/50000]\tLoss: 4.0549\tLR: 0.010000\n",
            "Training Epoch: 16 [30720/50000]\tLoss: 4.1793\tLR: 0.010000\n",
            "Training Epoch: 16 [30848/50000]\tLoss: 3.9490\tLR: 0.010000\n",
            "Training Epoch: 16 [30976/50000]\tLoss: 4.1686\tLR: 0.010000\n",
            "Training Epoch: 16 [31104/50000]\tLoss: 4.0952\tLR: 0.010000\n",
            "Training Epoch: 16 [31232/50000]\tLoss: 4.0996\tLR: 0.010000\n",
            "Training Epoch: 16 [31360/50000]\tLoss: 4.2310\tLR: 0.010000\n",
            "Training Epoch: 16 [31488/50000]\tLoss: 4.1859\tLR: 0.010000\n",
            "Training Epoch: 16 [31616/50000]\tLoss: 4.2261\tLR: 0.010000\n",
            "Training Epoch: 16 [31744/50000]\tLoss: 4.3156\tLR: 0.010000\n",
            "Training Epoch: 16 [31872/50000]\tLoss: 4.2856\tLR: 0.010000\n",
            "Training Epoch: 16 [32000/50000]\tLoss: 4.1569\tLR: 0.010000\n",
            "Training Epoch: 16 [32128/50000]\tLoss: 4.1488\tLR: 0.010000\n",
            "Training Epoch: 16 [32256/50000]\tLoss: 4.1656\tLR: 0.010000\n",
            "Training Epoch: 16 [32384/50000]\tLoss: 4.1003\tLR: 0.010000\n",
            "Training Epoch: 16 [32512/50000]\tLoss: 4.1824\tLR: 0.010000\n",
            "Training Epoch: 16 [32640/50000]\tLoss: 4.0926\tLR: 0.010000\n",
            "Training Epoch: 16 [32768/50000]\tLoss: 4.0234\tLR: 0.010000\n",
            "Training Epoch: 16 [32896/50000]\tLoss: 4.1725\tLR: 0.010000\n",
            "Training Epoch: 16 [33024/50000]\tLoss: 4.0425\tLR: 0.010000\n",
            "Training Epoch: 16 [33152/50000]\tLoss: 4.0993\tLR: 0.010000\n",
            "Training Epoch: 16 [33280/50000]\tLoss: 4.0669\tLR: 0.010000\n",
            "Training Epoch: 16 [33408/50000]\tLoss: 4.1691\tLR: 0.010000\n",
            "Training Epoch: 16 [33536/50000]\tLoss: 4.1884\tLR: 0.010000\n",
            "Training Epoch: 16 [33664/50000]\tLoss: 4.1054\tLR: 0.010000\n",
            "Training Epoch: 16 [33792/50000]\tLoss: 4.2504\tLR: 0.010000\n",
            "Training Epoch: 16 [33920/50000]\tLoss: 4.2496\tLR: 0.010000\n",
            "Training Epoch: 16 [34048/50000]\tLoss: 4.0622\tLR: 0.010000\n",
            "Training Epoch: 16 [34176/50000]\tLoss: 4.1727\tLR: 0.010000\n",
            "Training Epoch: 16 [34304/50000]\tLoss: 4.2728\tLR: 0.010000\n",
            "Training Epoch: 16 [34432/50000]\tLoss: 4.1888\tLR: 0.010000\n",
            "Training Epoch: 16 [34560/50000]\tLoss: 4.0692\tLR: 0.010000\n",
            "Training Epoch: 16 [34688/50000]\tLoss: 4.1602\tLR: 0.010000\n",
            "Training Epoch: 16 [34816/50000]\tLoss: 4.1468\tLR: 0.010000\n",
            "Training Epoch: 16 [34944/50000]\tLoss: 4.1761\tLR: 0.010000\n",
            "Training Epoch: 16 [35072/50000]\tLoss: 4.2362\tLR: 0.010000\n",
            "Training Epoch: 16 [35200/50000]\tLoss: 4.0869\tLR: 0.010000\n",
            "Training Epoch: 16 [35328/50000]\tLoss: 4.1655\tLR: 0.010000\n",
            "Training Epoch: 16 [35456/50000]\tLoss: 4.2164\tLR: 0.010000\n",
            "Training Epoch: 16 [35584/50000]\tLoss: 4.2120\tLR: 0.010000\n",
            "Training Epoch: 16 [35712/50000]\tLoss: 4.2430\tLR: 0.010000\n",
            "Training Epoch: 16 [35840/50000]\tLoss: 4.0702\tLR: 0.010000\n",
            "Training Epoch: 16 [35968/50000]\tLoss: 4.1860\tLR: 0.010000\n",
            "Training Epoch: 16 [36096/50000]\tLoss: 4.2196\tLR: 0.010000\n",
            "Training Epoch: 16 [36224/50000]\tLoss: 4.2450\tLR: 0.010000\n",
            "Training Epoch: 16 [36352/50000]\tLoss: 4.1911\tLR: 0.010000\n",
            "Training Epoch: 16 [36480/50000]\tLoss: 4.3040\tLR: 0.010000\n",
            "Training Epoch: 16 [36608/50000]\tLoss: 4.1448\tLR: 0.010000\n",
            "Training Epoch: 16 [36736/50000]\tLoss: 4.1234\tLR: 0.010000\n",
            "Training Epoch: 16 [36864/50000]\tLoss: 4.1864\tLR: 0.010000\n",
            "Training Epoch: 16 [36992/50000]\tLoss: 4.1721\tLR: 0.010000\n",
            "Training Epoch: 16 [37120/50000]\tLoss: 4.0306\tLR: 0.010000\n",
            "Training Epoch: 16 [37248/50000]\tLoss: 4.1476\tLR: 0.010000\n",
            "Training Epoch: 16 [37376/50000]\tLoss: 4.2129\tLR: 0.010000\n",
            "Training Epoch: 16 [37504/50000]\tLoss: 4.3006\tLR: 0.010000\n",
            "Training Epoch: 16 [37632/50000]\tLoss: 4.1412\tLR: 0.010000\n",
            "Training Epoch: 16 [37760/50000]\tLoss: 4.2350\tLR: 0.010000\n",
            "Training Epoch: 16 [37888/50000]\tLoss: 4.1668\tLR: 0.010000\n",
            "Training Epoch: 16 [38016/50000]\tLoss: 4.1162\tLR: 0.010000\n",
            "Training Epoch: 16 [38144/50000]\tLoss: 4.0493\tLR: 0.010000\n",
            "Training Epoch: 16 [38272/50000]\tLoss: 4.2323\tLR: 0.010000\n",
            "Training Epoch: 16 [38400/50000]\tLoss: 4.0716\tLR: 0.010000\n",
            "Training Epoch: 16 [38528/50000]\tLoss: 4.1342\tLR: 0.010000\n",
            "Training Epoch: 16 [38656/50000]\tLoss: 4.0867\tLR: 0.010000\n",
            "Training Epoch: 16 [38784/50000]\tLoss: 4.1893\tLR: 0.010000\n",
            "Training Epoch: 16 [38912/50000]\tLoss: 4.2679\tLR: 0.010000\n",
            "Training Epoch: 16 [39040/50000]\tLoss: 4.1310\tLR: 0.010000\n",
            "Training Epoch: 16 [39168/50000]\tLoss: 4.0589\tLR: 0.010000\n",
            "Training Epoch: 16 [39296/50000]\tLoss: 4.2154\tLR: 0.010000\n",
            "Training Epoch: 16 [39424/50000]\tLoss: 4.0857\tLR: 0.010000\n",
            "Training Epoch: 16 [39552/50000]\tLoss: 4.0782\tLR: 0.010000\n",
            "Training Epoch: 16 [39680/50000]\tLoss: 4.1769\tLR: 0.010000\n",
            "Training Epoch: 16 [39808/50000]\tLoss: 4.2549\tLR: 0.010000\n",
            "Training Epoch: 16 [39936/50000]\tLoss: 4.2105\tLR: 0.010000\n",
            "Training Epoch: 16 [40064/50000]\tLoss: 4.1586\tLR: 0.010000\n",
            "Training Epoch: 16 [40192/50000]\tLoss: 4.1854\tLR: 0.010000\n",
            "Training Epoch: 16 [40320/50000]\tLoss: 4.1196\tLR: 0.010000\n",
            "Training Epoch: 16 [40448/50000]\tLoss: 4.0226\tLR: 0.010000\n",
            "Training Epoch: 16 [40576/50000]\tLoss: 4.2840\tLR: 0.010000\n",
            "Training Epoch: 16 [40704/50000]\tLoss: 4.0845\tLR: 0.010000\n",
            "Training Epoch: 16 [40832/50000]\tLoss: 4.2617\tLR: 0.010000\n",
            "Training Epoch: 16 [40960/50000]\tLoss: 4.1455\tLR: 0.010000\n",
            "Training Epoch: 16 [41088/50000]\tLoss: 4.0846\tLR: 0.010000\n",
            "Training Epoch: 16 [41216/50000]\tLoss: 4.0613\tLR: 0.010000\n",
            "Training Epoch: 16 [41344/50000]\tLoss: 4.2362\tLR: 0.010000\n",
            "Training Epoch: 16 [41472/50000]\tLoss: 4.2117\tLR: 0.010000\n",
            "Training Epoch: 16 [41600/50000]\tLoss: 4.0672\tLR: 0.010000\n",
            "Training Epoch: 16 [41728/50000]\tLoss: 4.1331\tLR: 0.010000\n",
            "Training Epoch: 16 [41856/50000]\tLoss: 4.0236\tLR: 0.010000\n",
            "Training Epoch: 16 [41984/50000]\tLoss: 4.0738\tLR: 0.010000\n",
            "Training Epoch: 16 [42112/50000]\tLoss: 4.2580\tLR: 0.010000\n",
            "Training Epoch: 16 [42240/50000]\tLoss: 4.1786\tLR: 0.010000\n",
            "Training Epoch: 16 [42368/50000]\tLoss: 4.0251\tLR: 0.010000\n",
            "Training Epoch: 16 [42496/50000]\tLoss: 4.1368\tLR: 0.010000\n",
            "Training Epoch: 16 [42624/50000]\tLoss: 4.1499\tLR: 0.010000\n",
            "Training Epoch: 16 [42752/50000]\tLoss: 4.1957\tLR: 0.010000\n",
            "Training Epoch: 16 [42880/50000]\tLoss: 4.2143\tLR: 0.010000\n",
            "Training Epoch: 16 [43008/50000]\tLoss: 4.0670\tLR: 0.010000\n",
            "Training Epoch: 16 [43136/50000]\tLoss: 4.0554\tLR: 0.010000\n",
            "Training Epoch: 16 [43264/50000]\tLoss: 4.0373\tLR: 0.010000\n",
            "Training Epoch: 16 [43392/50000]\tLoss: 4.2027\tLR: 0.010000\n",
            "Training Epoch: 16 [43520/50000]\tLoss: 4.1066\tLR: 0.010000\n",
            "Training Epoch: 16 [43648/50000]\tLoss: 4.2413\tLR: 0.010000\n",
            "Training Epoch: 16 [43776/50000]\tLoss: 4.1138\tLR: 0.010000\n",
            "Training Epoch: 16 [43904/50000]\tLoss: 4.3275\tLR: 0.010000\n",
            "Training Epoch: 16 [44032/50000]\tLoss: 4.1442\tLR: 0.010000\n",
            "Training Epoch: 16 [44160/50000]\tLoss: 4.0840\tLR: 0.010000\n",
            "Training Epoch: 16 [44288/50000]\tLoss: 4.1484\tLR: 0.010000\n",
            "Training Epoch: 16 [44416/50000]\tLoss: 4.0748\tLR: 0.010000\n",
            "Training Epoch: 16 [44544/50000]\tLoss: 4.0942\tLR: 0.010000\n",
            "Training Epoch: 16 [44672/50000]\tLoss: 4.0049\tLR: 0.010000\n",
            "Training Epoch: 16 [44800/50000]\tLoss: 4.0162\tLR: 0.010000\n",
            "Training Epoch: 16 [44928/50000]\tLoss: 4.1657\tLR: 0.010000\n",
            "Training Epoch: 16 [45056/50000]\tLoss: 4.0703\tLR: 0.010000\n",
            "Training Epoch: 16 [45184/50000]\tLoss: 4.1129\tLR: 0.010000\n",
            "Training Epoch: 16 [45312/50000]\tLoss: 4.1843\tLR: 0.010000\n",
            "Training Epoch: 16 [45440/50000]\tLoss: 4.1775\tLR: 0.010000\n",
            "Training Epoch: 16 [45568/50000]\tLoss: 3.8917\tLR: 0.010000\n",
            "Training Epoch: 16 [45696/50000]\tLoss: 4.2018\tLR: 0.010000\n",
            "Training Epoch: 16 [45824/50000]\tLoss: 4.1559\tLR: 0.010000\n",
            "Training Epoch: 16 [45952/50000]\tLoss: 4.1403\tLR: 0.010000\n",
            "Training Epoch: 16 [46080/50000]\tLoss: 4.0598\tLR: 0.010000\n",
            "Training Epoch: 16 [46208/50000]\tLoss: 4.1871\tLR: 0.010000\n",
            "Training Epoch: 16 [46336/50000]\tLoss: 4.1333\tLR: 0.010000\n",
            "Training Epoch: 16 [46464/50000]\tLoss: 4.1306\tLR: 0.010000\n",
            "Training Epoch: 16 [46592/50000]\tLoss: 4.0071\tLR: 0.010000\n",
            "Training Epoch: 16 [46720/50000]\tLoss: 4.3510\tLR: 0.010000\n",
            "Training Epoch: 16 [46848/50000]\tLoss: 4.2868\tLR: 0.010000\n",
            "Training Epoch: 16 [46976/50000]\tLoss: 4.1198\tLR: 0.010000\n",
            "Training Epoch: 16 [47104/50000]\tLoss: 4.0176\tLR: 0.010000\n",
            "Training Epoch: 16 [47232/50000]\tLoss: 4.2193\tLR: 0.010000\n",
            "Training Epoch: 16 [47360/50000]\tLoss: 4.2164\tLR: 0.010000\n",
            "Training Epoch: 16 [47488/50000]\tLoss: 4.0675\tLR: 0.010000\n",
            "Training Epoch: 16 [47616/50000]\tLoss: 4.1622\tLR: 0.010000\n",
            "Training Epoch: 16 [47744/50000]\tLoss: 4.1549\tLR: 0.010000\n",
            "Training Epoch: 16 [47872/50000]\tLoss: 4.1931\tLR: 0.010000\n",
            "Training Epoch: 16 [48000/50000]\tLoss: 4.0851\tLR: 0.010000\n",
            "Training Epoch: 16 [48128/50000]\tLoss: 4.2972\tLR: 0.010000\n",
            "Training Epoch: 16 [48256/50000]\tLoss: 4.1581\tLR: 0.010000\n",
            "Training Epoch: 16 [48384/50000]\tLoss: 4.0905\tLR: 0.010000\n",
            "Training Epoch: 16 [48512/50000]\tLoss: 4.0464\tLR: 0.010000\n",
            "Training Epoch: 16 [48640/50000]\tLoss: 3.9160\tLR: 0.010000\n",
            "Training Epoch: 16 [48768/50000]\tLoss: 4.1715\tLR: 0.010000\n",
            "Training Epoch: 16 [48896/50000]\tLoss: 4.1235\tLR: 0.010000\n",
            "Training Epoch: 16 [49024/50000]\tLoss: 4.1948\tLR: 0.010000\n",
            "Training Epoch: 16 [49152/50000]\tLoss: 4.2138\tLR: 0.010000\n",
            "Training Epoch: 16 [49280/50000]\tLoss: 4.0953\tLR: 0.010000\n",
            "Training Epoch: 16 [49408/50000]\tLoss: 4.1712\tLR: 0.010000\n",
            "Training Epoch: 16 [49536/50000]\tLoss: 4.0964\tLR: 0.010000\n",
            "Training Epoch: 16 [49664/50000]\tLoss: 4.0785\tLR: 0.010000\n",
            "Training Epoch: 16 [49792/50000]\tLoss: 4.0649\tLR: 0.010000\n",
            "Training Epoch: 16 [49920/50000]\tLoss: 4.0361\tLR: 0.010000\n",
            "Training Epoch: 16 [50000/50000]\tLoss: 4.1239\tLR: 0.010000\n",
            "epoch 16 training time consumed: 144.71s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 169838 GiB | 169838 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 167695 GiB | 167695 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2142 GiB |   2142 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 169838 GiB | 169838 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 167695 GiB | 167695 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2142 GiB |   2142 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 169373 GiB | 169373 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 167230 GiB | 167230 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2142 GiB |   2142 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 141598 GiB | 141597 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 139177 GiB | 139176 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2420 GiB |   2420 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |   10348 K  |   10348 K  |\n",
            "|       from large pool |       5    |     146    |    5016 K  |    5016 K  |\n",
            "|       from small pool |     516    |     682    |    5332 K  |    5332 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |   10348 K  |   10348 K  |\n",
            "|       from large pool |       5    |     146    |    5016 K  |    5016 K  |\n",
            "|       from small pool |     516    |     682    |    5332 K  |    5332 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |     117    |    3852 K  |    3852 K  |\n",
            "|       from large pool |       4    |      46    |    2361 K  |    2361 K  |\n",
            "|       from small pool |      76    |      89    |    1490 K  |    1490 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0327, Accuracy: 0.0605, Time consumed:9.45s\n",
            "\n",
            "Training Epoch: 17 [128/50000]\tLoss: 4.1912\tLR: 0.010000\n",
            "Training Epoch: 17 [256/50000]\tLoss: 3.9839\tLR: 0.010000\n",
            "Training Epoch: 17 [384/50000]\tLoss: 4.2321\tLR: 0.010000\n",
            "Training Epoch: 17 [512/50000]\tLoss: 4.2204\tLR: 0.010000\n",
            "Training Epoch: 17 [640/50000]\tLoss: 4.0002\tLR: 0.010000\n",
            "Training Epoch: 17 [768/50000]\tLoss: 4.0763\tLR: 0.010000\n",
            "Training Epoch: 17 [896/50000]\tLoss: 4.1379\tLR: 0.010000\n",
            "Training Epoch: 17 [1024/50000]\tLoss: 4.1684\tLR: 0.010000\n",
            "Training Epoch: 17 [1152/50000]\tLoss: 4.1236\tLR: 0.010000\n",
            "Training Epoch: 17 [1280/50000]\tLoss: 4.0200\tLR: 0.010000\n",
            "Training Epoch: 17 [1408/50000]\tLoss: 4.0892\tLR: 0.010000\n",
            "Training Epoch: 17 [1536/50000]\tLoss: 3.9837\tLR: 0.010000\n",
            "Training Epoch: 17 [1664/50000]\tLoss: 4.1392\tLR: 0.010000\n",
            "Training Epoch: 17 [1792/50000]\tLoss: 3.9582\tLR: 0.010000\n",
            "Training Epoch: 17 [1920/50000]\tLoss: 4.2373\tLR: 0.010000\n",
            "Training Epoch: 17 [2048/50000]\tLoss: 4.1063\tLR: 0.010000\n",
            "Training Epoch: 17 [2176/50000]\tLoss: 4.2793\tLR: 0.010000\n",
            "Training Epoch: 17 [2304/50000]\tLoss: 4.1161\tLR: 0.010000\n",
            "Training Epoch: 17 [2432/50000]\tLoss: 4.2104\tLR: 0.010000\n",
            "Training Epoch: 17 [2560/50000]\tLoss: 3.8919\tLR: 0.010000\n",
            "Training Epoch: 17 [2688/50000]\tLoss: 4.0584\tLR: 0.010000\n",
            "Training Epoch: 17 [2816/50000]\tLoss: 4.1401\tLR: 0.010000\n",
            "Training Epoch: 17 [2944/50000]\tLoss: 4.1700\tLR: 0.010000\n",
            "Training Epoch: 17 [3072/50000]\tLoss: 4.2704\tLR: 0.010000\n",
            "Training Epoch: 17 [3200/50000]\tLoss: 4.1070\tLR: 0.010000\n",
            "Training Epoch: 17 [3328/50000]\tLoss: 4.0906\tLR: 0.010000\n",
            "Training Epoch: 17 [3456/50000]\tLoss: 4.1243\tLR: 0.010000\n",
            "Training Epoch: 17 [3584/50000]\tLoss: 4.0717\tLR: 0.010000\n",
            "Training Epoch: 17 [3712/50000]\tLoss: 4.2001\tLR: 0.010000\n",
            "Training Epoch: 17 [3840/50000]\tLoss: 4.0706\tLR: 0.010000\n",
            "Training Epoch: 17 [3968/50000]\tLoss: 4.1152\tLR: 0.010000\n",
            "Training Epoch: 17 [4096/50000]\tLoss: 4.1245\tLR: 0.010000\n",
            "Training Epoch: 17 [4224/50000]\tLoss: 4.1784\tLR: 0.010000\n",
            "Training Epoch: 17 [4352/50000]\tLoss: 4.1902\tLR: 0.010000\n",
            "Training Epoch: 17 [4480/50000]\tLoss: 4.1175\tLR: 0.010000\n",
            "Training Epoch: 17 [4608/50000]\tLoss: 4.1792\tLR: 0.010000\n",
            "Training Epoch: 17 [4736/50000]\tLoss: 4.2290\tLR: 0.010000\n",
            "Training Epoch: 17 [4864/50000]\tLoss: 4.0880\tLR: 0.010000\n",
            "Training Epoch: 17 [4992/50000]\tLoss: 4.1422\tLR: 0.010000\n",
            "Training Epoch: 17 [5120/50000]\tLoss: 4.1660\tLR: 0.010000\n",
            "Training Epoch: 17 [5248/50000]\tLoss: 4.1149\tLR: 0.010000\n",
            "Training Epoch: 17 [5376/50000]\tLoss: 4.0779\tLR: 0.010000\n",
            "Training Epoch: 17 [5504/50000]\tLoss: 4.1693\tLR: 0.010000\n",
            "Training Epoch: 17 [5632/50000]\tLoss: 4.0982\tLR: 0.010000\n",
            "Training Epoch: 17 [5760/50000]\tLoss: 4.0556\tLR: 0.010000\n",
            "Training Epoch: 17 [5888/50000]\tLoss: 4.1752\tLR: 0.010000\n",
            "Training Epoch: 17 [6016/50000]\tLoss: 4.0764\tLR: 0.010000\n",
            "Training Epoch: 17 [6144/50000]\tLoss: 4.1742\tLR: 0.010000\n",
            "Training Epoch: 17 [6272/50000]\tLoss: 4.0143\tLR: 0.010000\n",
            "Training Epoch: 17 [6400/50000]\tLoss: 4.0887\tLR: 0.010000\n",
            "Training Epoch: 17 [6528/50000]\tLoss: 4.1597\tLR: 0.010000\n",
            "Training Epoch: 17 [6656/50000]\tLoss: 4.1469\tLR: 0.010000\n",
            "Training Epoch: 17 [6784/50000]\tLoss: 4.1543\tLR: 0.010000\n",
            "Training Epoch: 17 [6912/50000]\tLoss: 4.1683\tLR: 0.010000\n",
            "Training Epoch: 17 [7040/50000]\tLoss: 4.1210\tLR: 0.010000\n",
            "Training Epoch: 17 [7168/50000]\tLoss: 4.2708\tLR: 0.010000\n",
            "Training Epoch: 17 [7296/50000]\tLoss: 4.0513\tLR: 0.010000\n",
            "Training Epoch: 17 [7424/50000]\tLoss: 4.2335\tLR: 0.010000\n",
            "Training Epoch: 17 [7552/50000]\tLoss: 4.1134\tLR: 0.010000\n",
            "Training Epoch: 17 [7680/50000]\tLoss: 4.2880\tLR: 0.010000\n",
            "Training Epoch: 17 [7808/50000]\tLoss: 4.1543\tLR: 0.010000\n",
            "Training Epoch: 17 [7936/50000]\tLoss: 4.2255\tLR: 0.010000\n",
            "Training Epoch: 17 [8064/50000]\tLoss: 4.1211\tLR: 0.010000\n",
            "Training Epoch: 17 [8192/50000]\tLoss: 4.2321\tLR: 0.010000\n",
            "Training Epoch: 17 [8320/50000]\tLoss: 4.1152\tLR: 0.010000\n",
            "Training Epoch: 17 [8448/50000]\tLoss: 4.0247\tLR: 0.010000\n",
            "Training Epoch: 17 [8576/50000]\tLoss: 4.0689\tLR: 0.010000\n",
            "Training Epoch: 17 [8704/50000]\tLoss: 4.0321\tLR: 0.010000\n",
            "Training Epoch: 17 [8832/50000]\tLoss: 4.0728\tLR: 0.010000\n",
            "Training Epoch: 17 [8960/50000]\tLoss: 4.1186\tLR: 0.010000\n",
            "Training Epoch: 17 [9088/50000]\tLoss: 4.1412\tLR: 0.010000\n",
            "Training Epoch: 17 [9216/50000]\tLoss: 4.1064\tLR: 0.010000\n",
            "Training Epoch: 17 [9344/50000]\tLoss: 4.2065\tLR: 0.010000\n",
            "Training Epoch: 17 [9472/50000]\tLoss: 4.2780\tLR: 0.010000\n",
            "Training Epoch: 17 [9600/50000]\tLoss: 4.1189\tLR: 0.010000\n",
            "Training Epoch: 17 [9728/50000]\tLoss: 4.2365\tLR: 0.010000\n",
            "Training Epoch: 17 [9856/50000]\tLoss: 4.0215\tLR: 0.010000\n",
            "Training Epoch: 17 [9984/50000]\tLoss: 4.1640\tLR: 0.010000\n",
            "Training Epoch: 17 [10112/50000]\tLoss: 4.0718\tLR: 0.010000\n",
            "Training Epoch: 17 [10240/50000]\tLoss: 4.0972\tLR: 0.010000\n",
            "Training Epoch: 17 [10368/50000]\tLoss: 4.0535\tLR: 0.010000\n",
            "Training Epoch: 17 [10496/50000]\tLoss: 3.9871\tLR: 0.010000\n",
            "Training Epoch: 17 [10624/50000]\tLoss: 4.1638\tLR: 0.010000\n",
            "Training Epoch: 17 [10752/50000]\tLoss: 4.1331\tLR: 0.010000\n",
            "Training Epoch: 17 [10880/50000]\tLoss: 3.9588\tLR: 0.010000\n",
            "Training Epoch: 17 [11008/50000]\tLoss: 3.9913\tLR: 0.010000\n",
            "Training Epoch: 17 [11136/50000]\tLoss: 4.3155\tLR: 0.010000\n",
            "Training Epoch: 17 [11264/50000]\tLoss: 3.9638\tLR: 0.010000\n",
            "Training Epoch: 17 [11392/50000]\tLoss: 4.0884\tLR: 0.010000\n",
            "Training Epoch: 17 [11520/50000]\tLoss: 4.0925\tLR: 0.010000\n",
            "Training Epoch: 17 [11648/50000]\tLoss: 4.0608\tLR: 0.010000\n",
            "Training Epoch: 17 [11776/50000]\tLoss: 4.1223\tLR: 0.010000\n",
            "Training Epoch: 17 [11904/50000]\tLoss: 4.0115\tLR: 0.010000\n",
            "Training Epoch: 17 [12032/50000]\tLoss: 4.4005\tLR: 0.010000\n",
            "Training Epoch: 17 [12160/50000]\tLoss: 4.1586\tLR: 0.010000\n",
            "Training Epoch: 17 [12288/50000]\tLoss: 4.0677\tLR: 0.010000\n",
            "Training Epoch: 17 [12416/50000]\tLoss: 4.1072\tLR: 0.010000\n",
            "Training Epoch: 17 [12544/50000]\tLoss: 4.2284\tLR: 0.010000\n",
            "Training Epoch: 17 [12672/50000]\tLoss: 4.1390\tLR: 0.010000\n",
            "Training Epoch: 17 [12800/50000]\tLoss: 4.2570\tLR: 0.010000\n",
            "Training Epoch: 17 [12928/50000]\tLoss: 4.2072\tLR: 0.010000\n",
            "Training Epoch: 17 [13056/50000]\tLoss: 4.0663\tLR: 0.010000\n",
            "Training Epoch: 17 [13184/50000]\tLoss: 4.1167\tLR: 0.010000\n",
            "Training Epoch: 17 [13312/50000]\tLoss: 4.1904\tLR: 0.010000\n",
            "Training Epoch: 17 [13440/50000]\tLoss: 4.1436\tLR: 0.010000\n",
            "Training Epoch: 17 [13568/50000]\tLoss: 4.1156\tLR: 0.010000\n",
            "Training Epoch: 17 [13696/50000]\tLoss: 4.2002\tLR: 0.010000\n",
            "Training Epoch: 17 [13824/50000]\tLoss: 4.1576\tLR: 0.010000\n",
            "Training Epoch: 17 [13952/50000]\tLoss: 4.1307\tLR: 0.010000\n",
            "Training Epoch: 17 [14080/50000]\tLoss: 4.2311\tLR: 0.010000\n",
            "Training Epoch: 17 [14208/50000]\tLoss: 4.1753\tLR: 0.010000\n",
            "Training Epoch: 17 [14336/50000]\tLoss: 4.2001\tLR: 0.010000\n",
            "Training Epoch: 17 [14464/50000]\tLoss: 4.1796\tLR: 0.010000\n",
            "Training Epoch: 17 [14592/50000]\tLoss: 4.1499\tLR: 0.010000\n",
            "Training Epoch: 17 [14720/50000]\tLoss: 4.1805\tLR: 0.010000\n",
            "Training Epoch: 17 [14848/50000]\tLoss: 3.9831\tLR: 0.010000\n",
            "Training Epoch: 17 [14976/50000]\tLoss: 3.9462\tLR: 0.010000\n",
            "Training Epoch: 17 [15104/50000]\tLoss: 4.1033\tLR: 0.010000\n",
            "Training Epoch: 17 [15232/50000]\tLoss: 4.1842\tLR: 0.010000\n",
            "Training Epoch: 17 [15360/50000]\tLoss: 4.0325\tLR: 0.010000\n",
            "Training Epoch: 17 [15488/50000]\tLoss: 3.9907\tLR: 0.010000\n",
            "Training Epoch: 17 [15616/50000]\tLoss: 4.1617\tLR: 0.010000\n",
            "Training Epoch: 17 [15744/50000]\tLoss: 4.1354\tLR: 0.010000\n",
            "Training Epoch: 17 [15872/50000]\tLoss: 4.0485\tLR: 0.010000\n",
            "Training Epoch: 17 [16000/50000]\tLoss: 4.1847\tLR: 0.010000\n",
            "Training Epoch: 17 [16128/50000]\tLoss: 4.0874\tLR: 0.010000\n",
            "Training Epoch: 17 [16256/50000]\tLoss: 3.9905\tLR: 0.010000\n",
            "Training Epoch: 17 [16384/50000]\tLoss: 4.0825\tLR: 0.010000\n",
            "Training Epoch: 17 [16512/50000]\tLoss: 4.0581\tLR: 0.010000\n",
            "Training Epoch: 17 [16640/50000]\tLoss: 3.8976\tLR: 0.010000\n",
            "Training Epoch: 17 [16768/50000]\tLoss: 4.1685\tLR: 0.010000\n",
            "Training Epoch: 17 [16896/50000]\tLoss: 3.9935\tLR: 0.010000\n",
            "Training Epoch: 17 [17024/50000]\tLoss: 4.0026\tLR: 0.010000\n",
            "Training Epoch: 17 [17152/50000]\tLoss: 4.1010\tLR: 0.010000\n",
            "Training Epoch: 17 [17280/50000]\tLoss: 4.3143\tLR: 0.010000\n",
            "Training Epoch: 17 [17408/50000]\tLoss: 4.1652\tLR: 0.010000\n",
            "Training Epoch: 17 [17536/50000]\tLoss: 4.2424\tLR: 0.010000\n",
            "Training Epoch: 17 [17664/50000]\tLoss: 4.0884\tLR: 0.010000\n",
            "Training Epoch: 17 [17792/50000]\tLoss: 4.2048\tLR: 0.010000\n",
            "Training Epoch: 17 [17920/50000]\tLoss: 4.1761\tLR: 0.010000\n",
            "Training Epoch: 17 [18048/50000]\tLoss: 4.1012\tLR: 0.010000\n",
            "Training Epoch: 17 [18176/50000]\tLoss: 4.1500\tLR: 0.010000\n",
            "Training Epoch: 17 [18304/50000]\tLoss: 4.0904\tLR: 0.010000\n",
            "Training Epoch: 17 [18432/50000]\tLoss: 4.1635\tLR: 0.010000\n",
            "Training Epoch: 17 [18560/50000]\tLoss: 4.0963\tLR: 0.010000\n",
            "Training Epoch: 17 [18688/50000]\tLoss: 4.1547\tLR: 0.010000\n",
            "Training Epoch: 17 [18816/50000]\tLoss: 4.2249\tLR: 0.010000\n",
            "Training Epoch: 17 [18944/50000]\tLoss: 4.0740\tLR: 0.010000\n",
            "Training Epoch: 17 [19072/50000]\tLoss: 4.2330\tLR: 0.010000\n",
            "Training Epoch: 17 [19200/50000]\tLoss: 4.0892\tLR: 0.010000\n",
            "Training Epoch: 17 [19328/50000]\tLoss: 4.1366\tLR: 0.010000\n",
            "Training Epoch: 17 [19456/50000]\tLoss: 4.1931\tLR: 0.010000\n",
            "Training Epoch: 17 [19584/50000]\tLoss: 4.1796\tLR: 0.010000\n",
            "Training Epoch: 17 [19712/50000]\tLoss: 4.0193\tLR: 0.010000\n",
            "Training Epoch: 17 [19840/50000]\tLoss: 4.1620\tLR: 0.010000\n",
            "Training Epoch: 17 [19968/50000]\tLoss: 4.0287\tLR: 0.010000\n",
            "Training Epoch: 17 [20096/50000]\tLoss: 3.9442\tLR: 0.010000\n",
            "Training Epoch: 17 [20224/50000]\tLoss: 4.1170\tLR: 0.010000\n",
            "Training Epoch: 17 [20352/50000]\tLoss: 4.0821\tLR: 0.010000\n",
            "Training Epoch: 17 [20480/50000]\tLoss: 4.0405\tLR: 0.010000\n",
            "Training Epoch: 17 [20608/50000]\tLoss: 4.1172\tLR: 0.010000\n",
            "Training Epoch: 17 [20736/50000]\tLoss: 4.0990\tLR: 0.010000\n",
            "Training Epoch: 17 [20864/50000]\tLoss: 4.0691\tLR: 0.010000\n",
            "Training Epoch: 17 [20992/50000]\tLoss: 4.0952\tLR: 0.010000\n",
            "Training Epoch: 17 [21120/50000]\tLoss: 3.9848\tLR: 0.010000\n",
            "Training Epoch: 17 [21248/50000]\tLoss: 4.0606\tLR: 0.010000\n",
            "Training Epoch: 17 [21376/50000]\tLoss: 4.0227\tLR: 0.010000\n",
            "Training Epoch: 17 [21504/50000]\tLoss: 4.0938\tLR: 0.010000\n",
            "Training Epoch: 17 [21632/50000]\tLoss: 4.1124\tLR: 0.010000\n",
            "Training Epoch: 17 [21760/50000]\tLoss: 3.9218\tLR: 0.010000\n",
            "Training Epoch: 17 [21888/50000]\tLoss: 4.1035\tLR: 0.010000\n",
            "Training Epoch: 17 [22016/50000]\tLoss: 4.1410\tLR: 0.010000\n",
            "Training Epoch: 17 [22144/50000]\tLoss: 3.9906\tLR: 0.010000\n",
            "Training Epoch: 17 [22272/50000]\tLoss: 4.0426\tLR: 0.010000\n",
            "Training Epoch: 17 [22400/50000]\tLoss: 4.1361\tLR: 0.010000\n",
            "Training Epoch: 17 [22528/50000]\tLoss: 4.2415\tLR: 0.010000\n",
            "Training Epoch: 17 [22656/50000]\tLoss: 4.0809\tLR: 0.010000\n",
            "Training Epoch: 17 [22784/50000]\tLoss: 4.1876\tLR: 0.010000\n",
            "Training Epoch: 17 [22912/50000]\tLoss: 4.0599\tLR: 0.010000\n",
            "Training Epoch: 17 [23040/50000]\tLoss: 4.1394\tLR: 0.010000\n",
            "Training Epoch: 17 [23168/50000]\tLoss: 4.1791\tLR: 0.010000\n",
            "Training Epoch: 17 [23296/50000]\tLoss: 4.0132\tLR: 0.010000\n",
            "Training Epoch: 17 [23424/50000]\tLoss: 4.0733\tLR: 0.010000\n",
            "Training Epoch: 17 [23552/50000]\tLoss: 4.1061\tLR: 0.010000\n",
            "Training Epoch: 17 [23680/50000]\tLoss: 4.0315\tLR: 0.010000\n",
            "Training Epoch: 17 [23808/50000]\tLoss: 3.8231\tLR: 0.010000\n",
            "Training Epoch: 17 [23936/50000]\tLoss: 3.9202\tLR: 0.010000\n",
            "Training Epoch: 17 [24064/50000]\tLoss: 4.0900\tLR: 0.010000\n",
            "Training Epoch: 17 [24192/50000]\tLoss: 4.1493\tLR: 0.010000\n",
            "Training Epoch: 17 [24320/50000]\tLoss: 4.1075\tLR: 0.010000\n",
            "Training Epoch: 17 [24448/50000]\tLoss: 4.1063\tLR: 0.010000\n",
            "Training Epoch: 17 [24576/50000]\tLoss: 4.1225\tLR: 0.010000\n",
            "Training Epoch: 17 [24704/50000]\tLoss: 4.0128\tLR: 0.010000\n",
            "Training Epoch: 17 [24832/50000]\tLoss: 4.0295\tLR: 0.010000\n",
            "Training Epoch: 17 [24960/50000]\tLoss: 4.3095\tLR: 0.010000\n",
            "Training Epoch: 17 [25088/50000]\tLoss: 3.8387\tLR: 0.010000\n",
            "Training Epoch: 17 [25216/50000]\tLoss: 4.1533\tLR: 0.010000\n",
            "Training Epoch: 17 [25344/50000]\tLoss: 4.2596\tLR: 0.010000\n",
            "Training Epoch: 17 [25472/50000]\tLoss: 4.1069\tLR: 0.010000\n",
            "Training Epoch: 17 [25600/50000]\tLoss: 4.1338\tLR: 0.010000\n",
            "Training Epoch: 17 [25728/50000]\tLoss: 4.1223\tLR: 0.010000\n",
            "Training Epoch: 17 [25856/50000]\tLoss: 4.1513\tLR: 0.010000\n",
            "Training Epoch: 17 [25984/50000]\tLoss: 4.0581\tLR: 0.010000\n",
            "Training Epoch: 17 [26112/50000]\tLoss: 4.0838\tLR: 0.010000\n",
            "Training Epoch: 17 [26240/50000]\tLoss: 4.0458\tLR: 0.010000\n",
            "Training Epoch: 17 [26368/50000]\tLoss: 4.1889\tLR: 0.010000\n",
            "Training Epoch: 17 [26496/50000]\tLoss: 4.1586\tLR: 0.010000\n",
            "Training Epoch: 17 [26624/50000]\tLoss: 4.1232\tLR: 0.010000\n",
            "Training Epoch: 17 [26752/50000]\tLoss: 4.1146\tLR: 0.010000\n",
            "Training Epoch: 17 [26880/50000]\tLoss: 3.9732\tLR: 0.010000\n",
            "Training Epoch: 17 [27008/50000]\tLoss: 4.1911\tLR: 0.010000\n",
            "Training Epoch: 17 [27136/50000]\tLoss: 4.0726\tLR: 0.010000\n",
            "Training Epoch: 17 [27264/50000]\tLoss: 3.8629\tLR: 0.010000\n",
            "Training Epoch: 17 [27392/50000]\tLoss: 3.9891\tLR: 0.010000\n",
            "Training Epoch: 17 [27520/50000]\tLoss: 4.1988\tLR: 0.010000\n",
            "Training Epoch: 17 [27648/50000]\tLoss: 4.0455\tLR: 0.010000\n",
            "Training Epoch: 17 [27776/50000]\tLoss: 4.1735\tLR: 0.010000\n",
            "Training Epoch: 17 [27904/50000]\tLoss: 4.0291\tLR: 0.010000\n",
            "Training Epoch: 17 [28032/50000]\tLoss: 4.0867\tLR: 0.010000\n",
            "Training Epoch: 17 [28160/50000]\tLoss: 4.0378\tLR: 0.010000\n",
            "Training Epoch: 17 [28288/50000]\tLoss: 4.0819\tLR: 0.010000\n",
            "Training Epoch: 17 [28416/50000]\tLoss: 4.1355\tLR: 0.010000\n",
            "Training Epoch: 17 [28544/50000]\tLoss: 4.2113\tLR: 0.010000\n",
            "Training Epoch: 17 [28672/50000]\tLoss: 3.9708\tLR: 0.010000\n",
            "Training Epoch: 17 [28800/50000]\tLoss: 4.0176\tLR: 0.010000\n",
            "Training Epoch: 17 [28928/50000]\tLoss: 4.1690\tLR: 0.010000\n",
            "Training Epoch: 17 [29056/50000]\tLoss: 4.1489\tLR: 0.010000\n",
            "Training Epoch: 17 [29184/50000]\tLoss: 4.0377\tLR: 0.010000\n",
            "Training Epoch: 17 [29312/50000]\tLoss: 4.2319\tLR: 0.010000\n",
            "Training Epoch: 17 [29440/50000]\tLoss: 4.3258\tLR: 0.010000\n",
            "Training Epoch: 17 [29568/50000]\tLoss: 4.0802\tLR: 0.010000\n",
            "Training Epoch: 17 [29696/50000]\tLoss: 4.1704\tLR: 0.010000\n",
            "Training Epoch: 17 [29824/50000]\tLoss: 4.0778\tLR: 0.010000\n",
            "Training Epoch: 17 [29952/50000]\tLoss: 4.0923\tLR: 0.010000\n",
            "Training Epoch: 17 [30080/50000]\tLoss: 3.9058\tLR: 0.010000\n",
            "Training Epoch: 17 [30208/50000]\tLoss: 4.0188\tLR: 0.010000\n",
            "Training Epoch: 17 [30336/50000]\tLoss: 4.1587\tLR: 0.010000\n",
            "Training Epoch: 17 [30464/50000]\tLoss: 4.0427\tLR: 0.010000\n",
            "Training Epoch: 17 [30592/50000]\tLoss: 4.0430\tLR: 0.010000\n",
            "Training Epoch: 17 [30720/50000]\tLoss: 3.9820\tLR: 0.010000\n",
            "Training Epoch: 17 [30848/50000]\tLoss: 4.0636\tLR: 0.010000\n",
            "Training Epoch: 17 [30976/50000]\tLoss: 4.0701\tLR: 0.010000\n",
            "Training Epoch: 17 [31104/50000]\tLoss: 4.0220\tLR: 0.010000\n",
            "Training Epoch: 17 [31232/50000]\tLoss: 4.0252\tLR: 0.010000\n",
            "Training Epoch: 17 [31360/50000]\tLoss: 4.1709\tLR: 0.010000\n",
            "Training Epoch: 17 [31488/50000]\tLoss: 4.1556\tLR: 0.010000\n",
            "Training Epoch: 17 [31616/50000]\tLoss: 4.0696\tLR: 0.010000\n",
            "Training Epoch: 17 [31744/50000]\tLoss: 4.0867\tLR: 0.010000\n",
            "Training Epoch: 17 [31872/50000]\tLoss: 4.0746\tLR: 0.010000\n",
            "Training Epoch: 17 [32000/50000]\tLoss: 4.0852\tLR: 0.010000\n",
            "Training Epoch: 17 [32128/50000]\tLoss: 4.1632\tLR: 0.010000\n",
            "Training Epoch: 17 [32256/50000]\tLoss: 4.0557\tLR: 0.010000\n",
            "Training Epoch: 17 [32384/50000]\tLoss: 4.1194\tLR: 0.010000\n",
            "Training Epoch: 17 [32512/50000]\tLoss: 4.0406\tLR: 0.010000\n",
            "Training Epoch: 17 [32640/50000]\tLoss: 4.0934\tLR: 0.010000\n",
            "Training Epoch: 17 [32768/50000]\tLoss: 4.1478\tLR: 0.010000\n",
            "Training Epoch: 17 [32896/50000]\tLoss: 4.1177\tLR: 0.010000\n",
            "Training Epoch: 17 [33024/50000]\tLoss: 3.9134\tLR: 0.010000\n",
            "Training Epoch: 17 [33152/50000]\tLoss: 4.0167\tLR: 0.010000\n",
            "Training Epoch: 17 [33280/50000]\tLoss: 4.0330\tLR: 0.010000\n",
            "Training Epoch: 17 [33408/50000]\tLoss: 3.9792\tLR: 0.010000\n",
            "Training Epoch: 17 [33536/50000]\tLoss: 4.1830\tLR: 0.010000\n",
            "Training Epoch: 17 [33664/50000]\tLoss: 3.9361\tLR: 0.010000\n",
            "Training Epoch: 17 [33792/50000]\tLoss: 4.0897\tLR: 0.010000\n",
            "Training Epoch: 17 [33920/50000]\tLoss: 3.9367\tLR: 0.010000\n",
            "Training Epoch: 17 [34048/50000]\tLoss: 3.9735\tLR: 0.010000\n",
            "Training Epoch: 17 [34176/50000]\tLoss: 4.1458\tLR: 0.010000\n",
            "Training Epoch: 17 [34304/50000]\tLoss: 4.0318\tLR: 0.010000\n",
            "Training Epoch: 17 [34432/50000]\tLoss: 4.0772\tLR: 0.010000\n",
            "Training Epoch: 17 [34560/50000]\tLoss: 4.1209\tLR: 0.010000\n",
            "Training Epoch: 17 [34688/50000]\tLoss: 4.0523\tLR: 0.010000\n",
            "Training Epoch: 17 [34816/50000]\tLoss: 4.1764\tLR: 0.010000\n",
            "Training Epoch: 17 [34944/50000]\tLoss: 4.1202\tLR: 0.010000\n",
            "Training Epoch: 17 [35072/50000]\tLoss: 4.0918\tLR: 0.010000\n",
            "Training Epoch: 17 [35200/50000]\tLoss: 4.1164\tLR: 0.010000\n",
            "Training Epoch: 17 [35328/50000]\tLoss: 4.0795\tLR: 0.010000\n",
            "Training Epoch: 17 [35456/50000]\tLoss: 4.0926\tLR: 0.010000\n",
            "Training Epoch: 17 [35584/50000]\tLoss: 3.9549\tLR: 0.010000\n",
            "Training Epoch: 17 [35712/50000]\tLoss: 4.0339\tLR: 0.010000\n",
            "Training Epoch: 17 [35840/50000]\tLoss: 4.0488\tLR: 0.010000\n",
            "Training Epoch: 17 [35968/50000]\tLoss: 4.1877\tLR: 0.010000\n",
            "Training Epoch: 17 [36096/50000]\tLoss: 3.9318\tLR: 0.010000\n",
            "Training Epoch: 17 [36224/50000]\tLoss: 4.2605\tLR: 0.010000\n",
            "Training Epoch: 17 [36352/50000]\tLoss: 4.1680\tLR: 0.010000\n",
            "Training Epoch: 17 [36480/50000]\tLoss: 4.0287\tLR: 0.010000\n",
            "Training Epoch: 17 [36608/50000]\tLoss: 4.0127\tLR: 0.010000\n",
            "Training Epoch: 17 [36736/50000]\tLoss: 4.1178\tLR: 0.010000\n",
            "Training Epoch: 17 [36864/50000]\tLoss: 3.9922\tLR: 0.010000\n",
            "Training Epoch: 17 [36992/50000]\tLoss: 3.9409\tLR: 0.010000\n",
            "Training Epoch: 17 [37120/50000]\tLoss: 3.9967\tLR: 0.010000\n",
            "Training Epoch: 17 [37248/50000]\tLoss: 4.2330\tLR: 0.010000\n",
            "Training Epoch: 17 [37376/50000]\tLoss: 4.1490\tLR: 0.010000\n",
            "Training Epoch: 17 [37504/50000]\tLoss: 3.9958\tLR: 0.010000\n",
            "Training Epoch: 17 [37632/50000]\tLoss: 4.1723\tLR: 0.010000\n",
            "Training Epoch: 17 [37760/50000]\tLoss: 4.0762\tLR: 0.010000\n",
            "Training Epoch: 17 [37888/50000]\tLoss: 4.0907\tLR: 0.010000\n",
            "Training Epoch: 17 [38016/50000]\tLoss: 4.1312\tLR: 0.010000\n",
            "Training Epoch: 17 [38144/50000]\tLoss: 4.1099\tLR: 0.010000\n",
            "Training Epoch: 17 [38272/50000]\tLoss: 4.0976\tLR: 0.010000\n",
            "Training Epoch: 17 [38400/50000]\tLoss: 4.0586\tLR: 0.010000\n",
            "Training Epoch: 17 [38528/50000]\tLoss: 4.0886\tLR: 0.010000\n",
            "Training Epoch: 17 [38656/50000]\tLoss: 4.0321\tLR: 0.010000\n",
            "Training Epoch: 17 [38784/50000]\tLoss: 4.1556\tLR: 0.010000\n",
            "Training Epoch: 17 [38912/50000]\tLoss: 4.1564\tLR: 0.010000\n",
            "Training Epoch: 17 [39040/50000]\tLoss: 4.1248\tLR: 0.010000\n",
            "Training Epoch: 17 [39168/50000]\tLoss: 3.9811\tLR: 0.010000\n",
            "Training Epoch: 17 [39296/50000]\tLoss: 4.0321\tLR: 0.010000\n",
            "Training Epoch: 17 [39424/50000]\tLoss: 4.1139\tLR: 0.010000\n",
            "Training Epoch: 17 [39552/50000]\tLoss: 4.1637\tLR: 0.010000\n",
            "Training Epoch: 17 [39680/50000]\tLoss: 4.0767\tLR: 0.010000\n",
            "Training Epoch: 17 [39808/50000]\tLoss: 4.1290\tLR: 0.010000\n",
            "Training Epoch: 17 [39936/50000]\tLoss: 4.0330\tLR: 0.010000\n",
            "Training Epoch: 17 [40064/50000]\tLoss: 3.9808\tLR: 0.010000\n",
            "Training Epoch: 17 [40192/50000]\tLoss: 3.9998\tLR: 0.010000\n",
            "Training Epoch: 17 [40320/50000]\tLoss: 4.1860\tLR: 0.010000\n",
            "Training Epoch: 17 [40448/50000]\tLoss: 4.1793\tLR: 0.010000\n",
            "Training Epoch: 17 [40576/50000]\tLoss: 4.2044\tLR: 0.010000\n",
            "Training Epoch: 17 [40704/50000]\tLoss: 4.1414\tLR: 0.010000\n",
            "Training Epoch: 17 [40832/50000]\tLoss: 4.1306\tLR: 0.010000\n",
            "Training Epoch: 17 [40960/50000]\tLoss: 4.0755\tLR: 0.010000\n",
            "Training Epoch: 17 [41088/50000]\tLoss: 4.0741\tLR: 0.010000\n",
            "Training Epoch: 17 [41216/50000]\tLoss: 3.9819\tLR: 0.010000\n",
            "Training Epoch: 17 [41344/50000]\tLoss: 4.0601\tLR: 0.010000\n",
            "Training Epoch: 17 [41472/50000]\tLoss: 4.0074\tLR: 0.010000\n",
            "Training Epoch: 17 [41600/50000]\tLoss: 4.1081\tLR: 0.010000\n",
            "Training Epoch: 17 [41728/50000]\tLoss: 4.0923\tLR: 0.010000\n",
            "Training Epoch: 17 [41856/50000]\tLoss: 4.1103\tLR: 0.010000\n",
            "Training Epoch: 17 [41984/50000]\tLoss: 4.0275\tLR: 0.010000\n",
            "Training Epoch: 17 [42112/50000]\tLoss: 4.0543\tLR: 0.010000\n",
            "Training Epoch: 17 [42240/50000]\tLoss: 4.0602\tLR: 0.010000\n",
            "Training Epoch: 17 [42368/50000]\tLoss: 4.1578\tLR: 0.010000\n",
            "Training Epoch: 17 [42496/50000]\tLoss: 3.9802\tLR: 0.010000\n",
            "Training Epoch: 17 [42624/50000]\tLoss: 4.2261\tLR: 0.010000\n",
            "Training Epoch: 17 [42752/50000]\tLoss: 3.9290\tLR: 0.010000\n",
            "Training Epoch: 17 [42880/50000]\tLoss: 4.2137\tLR: 0.010000\n",
            "Training Epoch: 17 [43008/50000]\tLoss: 4.1968\tLR: 0.010000\n",
            "Training Epoch: 17 [43136/50000]\tLoss: 4.0018\tLR: 0.010000\n",
            "Training Epoch: 17 [43264/50000]\tLoss: 4.1039\tLR: 0.010000\n",
            "Training Epoch: 17 [43392/50000]\tLoss: 3.9918\tLR: 0.010000\n",
            "Training Epoch: 17 [43520/50000]\tLoss: 4.0303\tLR: 0.010000\n",
            "Training Epoch: 17 [43648/50000]\tLoss: 4.0519\tLR: 0.010000\n",
            "Training Epoch: 17 [43776/50000]\tLoss: 4.2221\tLR: 0.010000\n",
            "Training Epoch: 17 [43904/50000]\tLoss: 4.0690\tLR: 0.010000\n",
            "Training Epoch: 17 [44032/50000]\tLoss: 4.0131\tLR: 0.010000\n",
            "Training Epoch: 17 [44160/50000]\tLoss: 4.1356\tLR: 0.010000\n",
            "Training Epoch: 17 [44288/50000]\tLoss: 4.0287\tLR: 0.010000\n",
            "Training Epoch: 17 [44416/50000]\tLoss: 3.8678\tLR: 0.010000\n",
            "Training Epoch: 17 [44544/50000]\tLoss: 4.2010\tLR: 0.010000\n",
            "Training Epoch: 17 [44672/50000]\tLoss: 3.9928\tLR: 0.010000\n",
            "Training Epoch: 17 [44800/50000]\tLoss: 4.1804\tLR: 0.010000\n",
            "Training Epoch: 17 [44928/50000]\tLoss: 4.0727\tLR: 0.010000\n",
            "Training Epoch: 17 [45056/50000]\tLoss: 4.1427\tLR: 0.010000\n",
            "Training Epoch: 17 [45184/50000]\tLoss: 4.1445\tLR: 0.010000\n",
            "Training Epoch: 17 [45312/50000]\tLoss: 3.9686\tLR: 0.010000\n",
            "Training Epoch: 17 [45440/50000]\tLoss: 4.0925\tLR: 0.010000\n",
            "Training Epoch: 17 [45568/50000]\tLoss: 4.0188\tLR: 0.010000\n",
            "Training Epoch: 17 [45696/50000]\tLoss: 4.0539\tLR: 0.010000\n",
            "Training Epoch: 17 [45824/50000]\tLoss: 4.0685\tLR: 0.010000\n",
            "Training Epoch: 17 [45952/50000]\tLoss: 4.0918\tLR: 0.010000\n",
            "Training Epoch: 17 [46080/50000]\tLoss: 4.2086\tLR: 0.010000\n",
            "Training Epoch: 17 [46208/50000]\tLoss: 4.0744\tLR: 0.010000\n",
            "Training Epoch: 17 [46336/50000]\tLoss: 4.0907\tLR: 0.010000\n",
            "Training Epoch: 17 [46464/50000]\tLoss: 3.9906\tLR: 0.010000\n",
            "Training Epoch: 17 [46592/50000]\tLoss: 3.9791\tLR: 0.010000\n",
            "Training Epoch: 17 [46720/50000]\tLoss: 3.9482\tLR: 0.010000\n",
            "Training Epoch: 17 [46848/50000]\tLoss: 4.1187\tLR: 0.010000\n",
            "Training Epoch: 17 [46976/50000]\tLoss: 4.1091\tLR: 0.010000\n",
            "Training Epoch: 17 [47104/50000]\tLoss: 4.1348\tLR: 0.010000\n",
            "Training Epoch: 17 [47232/50000]\tLoss: 4.0985\tLR: 0.010000\n",
            "Training Epoch: 17 [47360/50000]\tLoss: 4.0157\tLR: 0.010000\n",
            "Training Epoch: 17 [47488/50000]\tLoss: 4.0411\tLR: 0.010000\n",
            "Training Epoch: 17 [47616/50000]\tLoss: 3.9169\tLR: 0.010000\n",
            "Training Epoch: 17 [47744/50000]\tLoss: 4.1824\tLR: 0.010000\n",
            "Training Epoch: 17 [47872/50000]\tLoss: 4.0325\tLR: 0.010000\n",
            "Training Epoch: 17 [48000/50000]\tLoss: 3.9599\tLR: 0.010000\n",
            "Training Epoch: 17 [48128/50000]\tLoss: 3.9801\tLR: 0.010000\n",
            "Training Epoch: 17 [48256/50000]\tLoss: 3.9270\tLR: 0.010000\n",
            "Training Epoch: 17 [48384/50000]\tLoss: 3.9954\tLR: 0.010000\n",
            "Training Epoch: 17 [48512/50000]\tLoss: 4.1639\tLR: 0.010000\n",
            "Training Epoch: 17 [48640/50000]\tLoss: 3.9551\tLR: 0.010000\n",
            "Training Epoch: 17 [48768/50000]\tLoss: 4.0675\tLR: 0.010000\n",
            "Training Epoch: 17 [48896/50000]\tLoss: 4.0564\tLR: 0.010000\n",
            "Training Epoch: 17 [49024/50000]\tLoss: 4.2286\tLR: 0.010000\n",
            "Training Epoch: 17 [49152/50000]\tLoss: 4.0991\tLR: 0.010000\n",
            "Training Epoch: 17 [49280/50000]\tLoss: 3.9828\tLR: 0.010000\n",
            "Training Epoch: 17 [49408/50000]\tLoss: 4.2912\tLR: 0.010000\n",
            "Training Epoch: 17 [49536/50000]\tLoss: 4.0651\tLR: 0.010000\n",
            "Training Epoch: 17 [49664/50000]\tLoss: 4.0568\tLR: 0.010000\n",
            "Training Epoch: 17 [49792/50000]\tLoss: 3.8846\tLR: 0.010000\n",
            "Training Epoch: 17 [49920/50000]\tLoss: 3.9639\tLR: 0.010000\n",
            "Training Epoch: 17 [50000/50000]\tLoss: 3.9995\tLR: 0.010000\n",
            "epoch 17 training time consumed: 144.90s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 180451 GiB | 180450 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 178174 GiB | 178174 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2276 GiB |   2276 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 180451 GiB | 180450 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 178174 GiB | 178174 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2276 GiB |   2276 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 179957 GiB | 179956 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 177680 GiB | 177680 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2276 GiB |   2276 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 150553 GiB | 150553 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 147981 GiB | 147981 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2572 GiB |   2572 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |   10995 K  |   10994 K  |\n",
            "|       from large pool |       5    |     146    |    5329 K  |    5329 K  |\n",
            "|       from small pool |     516    |     682    |    5665 K  |    5665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |   10995 K  |   10994 K  |\n",
            "|       from large pool |       5    |     146    |    5329 K  |    5329 K  |\n",
            "|       from small pool |     516    |     682    |    5665 K  |    5665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |     117    |    4092 K  |    4092 K  |\n",
            "|       from large pool |       4    |      46    |    2507 K  |    2507 K  |\n",
            "|       from small pool |      73    |      89    |    1584 K  |    1584 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0322, Accuracy: 0.0690, Time consumed:9.36s\n",
            "\n",
            "Training Epoch: 18 [128/50000]\tLoss: 4.0555\tLR: 0.010000\n",
            "Training Epoch: 18 [256/50000]\tLoss: 4.0014\tLR: 0.010000\n",
            "Training Epoch: 18 [384/50000]\tLoss: 3.9132\tLR: 0.010000\n",
            "Training Epoch: 18 [512/50000]\tLoss: 4.1724\tLR: 0.010000\n",
            "Training Epoch: 18 [640/50000]\tLoss: 4.1179\tLR: 0.010000\n",
            "Training Epoch: 18 [768/50000]\tLoss: 4.2119\tLR: 0.010000\n",
            "Training Epoch: 18 [896/50000]\tLoss: 3.9530\tLR: 0.010000\n",
            "Training Epoch: 18 [1024/50000]\tLoss: 4.0278\tLR: 0.010000\n",
            "Training Epoch: 18 [1152/50000]\tLoss: 3.9753\tLR: 0.010000\n",
            "Training Epoch: 18 [1280/50000]\tLoss: 4.1444\tLR: 0.010000\n",
            "Training Epoch: 18 [1408/50000]\tLoss: 4.0463\tLR: 0.010000\n",
            "Training Epoch: 18 [1536/50000]\tLoss: 4.0013\tLR: 0.010000\n",
            "Training Epoch: 18 [1664/50000]\tLoss: 3.9194\tLR: 0.010000\n",
            "Training Epoch: 18 [1792/50000]\tLoss: 4.1211\tLR: 0.010000\n",
            "Training Epoch: 18 [1920/50000]\tLoss: 4.0021\tLR: 0.010000\n",
            "Training Epoch: 18 [2048/50000]\tLoss: 3.9489\tLR: 0.010000\n",
            "Training Epoch: 18 [2176/50000]\tLoss: 4.0437\tLR: 0.010000\n",
            "Training Epoch: 18 [2304/50000]\tLoss: 4.0674\tLR: 0.010000\n",
            "Training Epoch: 18 [2432/50000]\tLoss: 3.9515\tLR: 0.010000\n",
            "Training Epoch: 18 [2560/50000]\tLoss: 4.0336\tLR: 0.010000\n",
            "Training Epoch: 18 [2688/50000]\tLoss: 4.0373\tLR: 0.010000\n",
            "Training Epoch: 18 [2816/50000]\tLoss: 3.9976\tLR: 0.010000\n",
            "Training Epoch: 18 [2944/50000]\tLoss: 4.0227\tLR: 0.010000\n",
            "Training Epoch: 18 [3072/50000]\tLoss: 4.0671\tLR: 0.010000\n",
            "Training Epoch: 18 [3200/50000]\tLoss: 4.2135\tLR: 0.010000\n",
            "Training Epoch: 18 [3328/50000]\tLoss: 4.1362\tLR: 0.010000\n",
            "Training Epoch: 18 [3456/50000]\tLoss: 4.0931\tLR: 0.010000\n",
            "Training Epoch: 18 [3584/50000]\tLoss: 4.0795\tLR: 0.010000\n",
            "Training Epoch: 18 [3712/50000]\tLoss: 4.0998\tLR: 0.010000\n",
            "Training Epoch: 18 [3840/50000]\tLoss: 3.8966\tLR: 0.010000\n",
            "Training Epoch: 18 [3968/50000]\tLoss: 4.0246\tLR: 0.010000\n",
            "Training Epoch: 18 [4096/50000]\tLoss: 4.1614\tLR: 0.010000\n",
            "Training Epoch: 18 [4224/50000]\tLoss: 4.1389\tLR: 0.010000\n",
            "Training Epoch: 18 [4352/50000]\tLoss: 4.0975\tLR: 0.010000\n",
            "Training Epoch: 18 [4480/50000]\tLoss: 3.8467\tLR: 0.010000\n",
            "Training Epoch: 18 [4608/50000]\tLoss: 4.0174\tLR: 0.010000\n",
            "Training Epoch: 18 [4736/50000]\tLoss: 4.1039\tLR: 0.010000\n",
            "Training Epoch: 18 [4864/50000]\tLoss: 4.0729\tLR: 0.010000\n",
            "Training Epoch: 18 [4992/50000]\tLoss: 3.9891\tLR: 0.010000\n",
            "Training Epoch: 18 [5120/50000]\tLoss: 3.8194\tLR: 0.010000\n",
            "Training Epoch: 18 [5248/50000]\tLoss: 3.8864\tLR: 0.010000\n",
            "Training Epoch: 18 [5376/50000]\tLoss: 4.0306\tLR: 0.010000\n",
            "Training Epoch: 18 [5504/50000]\tLoss: 4.1638\tLR: 0.010000\n",
            "Training Epoch: 18 [5632/50000]\tLoss: 4.0483\tLR: 0.010000\n",
            "Training Epoch: 18 [5760/50000]\tLoss: 4.0839\tLR: 0.010000\n",
            "Training Epoch: 18 [5888/50000]\tLoss: 4.0129\tLR: 0.010000\n",
            "Training Epoch: 18 [6016/50000]\tLoss: 3.9442\tLR: 0.010000\n",
            "Training Epoch: 18 [6144/50000]\tLoss: 4.1236\tLR: 0.010000\n",
            "Training Epoch: 18 [6272/50000]\tLoss: 3.9219\tLR: 0.010000\n",
            "Training Epoch: 18 [6400/50000]\tLoss: 3.9780\tLR: 0.010000\n",
            "Training Epoch: 18 [6528/50000]\tLoss: 3.9518\tLR: 0.010000\n",
            "Training Epoch: 18 [6656/50000]\tLoss: 3.9425\tLR: 0.010000\n",
            "Training Epoch: 18 [6784/50000]\tLoss: 4.0281\tLR: 0.010000\n",
            "Training Epoch: 18 [6912/50000]\tLoss: 4.0496\tLR: 0.010000\n",
            "Training Epoch: 18 [7040/50000]\tLoss: 4.1869\tLR: 0.010000\n",
            "Training Epoch: 18 [7168/50000]\tLoss: 4.0805\tLR: 0.010000\n",
            "Training Epoch: 18 [7296/50000]\tLoss: 4.0044\tLR: 0.010000\n",
            "Training Epoch: 18 [7424/50000]\tLoss: 3.8813\tLR: 0.010000\n",
            "Training Epoch: 18 [7552/50000]\tLoss: 3.9563\tLR: 0.010000\n",
            "Training Epoch: 18 [7680/50000]\tLoss: 3.9578\tLR: 0.010000\n",
            "Training Epoch: 18 [7808/50000]\tLoss: 4.1495\tLR: 0.010000\n",
            "Training Epoch: 18 [7936/50000]\tLoss: 4.0957\tLR: 0.010000\n",
            "Training Epoch: 18 [8064/50000]\tLoss: 4.0867\tLR: 0.010000\n",
            "Training Epoch: 18 [8192/50000]\tLoss: 3.9003\tLR: 0.010000\n",
            "Training Epoch: 18 [8320/50000]\tLoss: 4.0190\tLR: 0.010000\n",
            "Training Epoch: 18 [8448/50000]\tLoss: 4.1262\tLR: 0.010000\n",
            "Training Epoch: 18 [8576/50000]\tLoss: 3.8685\tLR: 0.010000\n",
            "Training Epoch: 18 [8704/50000]\tLoss: 3.9916\tLR: 0.010000\n",
            "Training Epoch: 18 [8832/50000]\tLoss: 3.9784\tLR: 0.010000\n",
            "Training Epoch: 18 [8960/50000]\tLoss: 4.1060\tLR: 0.010000\n",
            "Training Epoch: 18 [9088/50000]\tLoss: 3.9059\tLR: 0.010000\n",
            "Training Epoch: 18 [9216/50000]\tLoss: 3.9925\tLR: 0.010000\n",
            "Training Epoch: 18 [9344/50000]\tLoss: 3.9520\tLR: 0.010000\n",
            "Training Epoch: 18 [9472/50000]\tLoss: 3.9919\tLR: 0.010000\n",
            "Training Epoch: 18 [9600/50000]\tLoss: 3.8408\tLR: 0.010000\n",
            "Training Epoch: 18 [9728/50000]\tLoss: 4.0874\tLR: 0.010000\n",
            "Training Epoch: 18 [9856/50000]\tLoss: 4.1762\tLR: 0.010000\n",
            "Training Epoch: 18 [9984/50000]\tLoss: 4.0640\tLR: 0.010000\n",
            "Training Epoch: 18 [10112/50000]\tLoss: 3.9333\tLR: 0.010000\n",
            "Training Epoch: 18 [10240/50000]\tLoss: 4.0830\tLR: 0.010000\n",
            "Training Epoch: 18 [10368/50000]\tLoss: 4.0640\tLR: 0.010000\n",
            "Training Epoch: 18 [10496/50000]\tLoss: 4.0604\tLR: 0.010000\n",
            "Training Epoch: 18 [10624/50000]\tLoss: 4.0106\tLR: 0.010000\n",
            "Training Epoch: 18 [10752/50000]\tLoss: 3.9785\tLR: 0.010000\n",
            "Training Epoch: 18 [10880/50000]\tLoss: 3.9307\tLR: 0.010000\n",
            "Training Epoch: 18 [11008/50000]\tLoss: 4.0555\tLR: 0.010000\n",
            "Training Epoch: 18 [11136/50000]\tLoss: 4.1072\tLR: 0.010000\n",
            "Training Epoch: 18 [11264/50000]\tLoss: 3.8195\tLR: 0.010000\n",
            "Training Epoch: 18 [11392/50000]\tLoss: 4.0195\tLR: 0.010000\n",
            "Training Epoch: 18 [11520/50000]\tLoss: 3.9173\tLR: 0.010000\n",
            "Training Epoch: 18 [11648/50000]\tLoss: 3.9398\tLR: 0.010000\n",
            "Training Epoch: 18 [11776/50000]\tLoss: 4.2683\tLR: 0.010000\n",
            "Training Epoch: 18 [11904/50000]\tLoss: 4.1526\tLR: 0.010000\n",
            "Training Epoch: 18 [12032/50000]\tLoss: 4.1900\tLR: 0.010000\n",
            "Training Epoch: 18 [12160/50000]\tLoss: 4.0136\tLR: 0.010000\n",
            "Training Epoch: 18 [12288/50000]\tLoss: 4.0105\tLR: 0.010000\n",
            "Training Epoch: 18 [12416/50000]\tLoss: 3.8388\tLR: 0.010000\n",
            "Training Epoch: 18 [12544/50000]\tLoss: 4.1070\tLR: 0.010000\n",
            "Training Epoch: 18 [12672/50000]\tLoss: 3.9736\tLR: 0.010000\n",
            "Training Epoch: 18 [12800/50000]\tLoss: 4.1782\tLR: 0.010000\n",
            "Training Epoch: 18 [12928/50000]\tLoss: 4.1563\tLR: 0.010000\n",
            "Training Epoch: 18 [13056/50000]\tLoss: 4.0388\tLR: 0.010000\n",
            "Training Epoch: 18 [13184/50000]\tLoss: 3.8757\tLR: 0.010000\n",
            "Training Epoch: 18 [13312/50000]\tLoss: 3.9868\tLR: 0.010000\n",
            "Training Epoch: 18 [13440/50000]\tLoss: 4.0337\tLR: 0.010000\n",
            "Training Epoch: 18 [13568/50000]\tLoss: 4.1669\tLR: 0.010000\n",
            "Training Epoch: 18 [13696/50000]\tLoss: 3.9365\tLR: 0.010000\n",
            "Training Epoch: 18 [13824/50000]\tLoss: 4.1278\tLR: 0.010000\n",
            "Training Epoch: 18 [13952/50000]\tLoss: 3.9205\tLR: 0.010000\n",
            "Training Epoch: 18 [14080/50000]\tLoss: 4.0311\tLR: 0.010000\n",
            "Training Epoch: 18 [14208/50000]\tLoss: 3.9731\tLR: 0.010000\n",
            "Training Epoch: 18 [14336/50000]\tLoss: 4.0100\tLR: 0.010000\n",
            "Training Epoch: 18 [14464/50000]\tLoss: 3.7653\tLR: 0.010000\n",
            "Training Epoch: 18 [14592/50000]\tLoss: 4.0654\tLR: 0.010000\n",
            "Training Epoch: 18 [14720/50000]\tLoss: 4.0078\tLR: 0.010000\n",
            "Training Epoch: 18 [14848/50000]\tLoss: 3.7639\tLR: 0.010000\n",
            "Training Epoch: 18 [14976/50000]\tLoss: 4.0455\tLR: 0.010000\n",
            "Training Epoch: 18 [15104/50000]\tLoss: 3.9074\tLR: 0.010000\n",
            "Training Epoch: 18 [15232/50000]\tLoss: 3.9125\tLR: 0.010000\n",
            "Training Epoch: 18 [15360/50000]\tLoss: 4.0258\tLR: 0.010000\n",
            "Training Epoch: 18 [15488/50000]\tLoss: 4.0768\tLR: 0.010000\n",
            "Training Epoch: 18 [15616/50000]\tLoss: 3.9607\tLR: 0.010000\n",
            "Training Epoch: 18 [15744/50000]\tLoss: 4.0243\tLR: 0.010000\n",
            "Training Epoch: 18 [15872/50000]\tLoss: 4.1804\tLR: 0.010000\n",
            "Training Epoch: 18 [16000/50000]\tLoss: 4.0082\tLR: 0.010000\n",
            "Training Epoch: 18 [16128/50000]\tLoss: 4.0518\tLR: 0.010000\n",
            "Training Epoch: 18 [16256/50000]\tLoss: 3.9264\tLR: 0.010000\n",
            "Training Epoch: 18 [16384/50000]\tLoss: 4.0340\tLR: 0.010000\n",
            "Training Epoch: 18 [16512/50000]\tLoss: 4.0609\tLR: 0.010000\n",
            "Training Epoch: 18 [16640/50000]\tLoss: 3.9207\tLR: 0.010000\n",
            "Training Epoch: 18 [16768/50000]\tLoss: 4.0230\tLR: 0.010000\n",
            "Training Epoch: 18 [16896/50000]\tLoss: 3.9129\tLR: 0.010000\n",
            "Training Epoch: 18 [17024/50000]\tLoss: 3.9600\tLR: 0.010000\n",
            "Training Epoch: 18 [17152/50000]\tLoss: 3.9358\tLR: 0.010000\n",
            "Training Epoch: 18 [17280/50000]\tLoss: 4.0316\tLR: 0.010000\n",
            "Training Epoch: 18 [17408/50000]\tLoss: 4.1611\tLR: 0.010000\n",
            "Training Epoch: 18 [17536/50000]\tLoss: 4.0229\tLR: 0.010000\n",
            "Training Epoch: 18 [17664/50000]\tLoss: 4.0318\tLR: 0.010000\n",
            "Training Epoch: 18 [17792/50000]\tLoss: 4.1554\tLR: 0.010000\n",
            "Training Epoch: 18 [17920/50000]\tLoss: 4.0271\tLR: 0.010000\n",
            "Training Epoch: 18 [18048/50000]\tLoss: 3.8799\tLR: 0.010000\n",
            "Training Epoch: 18 [18176/50000]\tLoss: 4.0209\tLR: 0.010000\n",
            "Training Epoch: 18 [18304/50000]\tLoss: 3.8722\tLR: 0.010000\n",
            "Training Epoch: 18 [18432/50000]\tLoss: 4.1173\tLR: 0.010000\n",
            "Training Epoch: 18 [18560/50000]\tLoss: 3.8767\tLR: 0.010000\n",
            "Training Epoch: 18 [18688/50000]\tLoss: 3.9496\tLR: 0.010000\n",
            "Training Epoch: 18 [18816/50000]\tLoss: 4.0808\tLR: 0.010000\n",
            "Training Epoch: 18 [18944/50000]\tLoss: 4.0003\tLR: 0.010000\n",
            "Training Epoch: 18 [19072/50000]\tLoss: 4.0103\tLR: 0.010000\n",
            "Training Epoch: 18 [19200/50000]\tLoss: 4.0292\tLR: 0.010000\n",
            "Training Epoch: 18 [19328/50000]\tLoss: 3.9633\tLR: 0.010000\n",
            "Training Epoch: 18 [19456/50000]\tLoss: 4.0987\tLR: 0.010000\n",
            "Training Epoch: 18 [19584/50000]\tLoss: 3.9137\tLR: 0.010000\n",
            "Training Epoch: 18 [19712/50000]\tLoss: 3.9795\tLR: 0.010000\n",
            "Training Epoch: 18 [19840/50000]\tLoss: 4.0653\tLR: 0.010000\n",
            "Training Epoch: 18 [19968/50000]\tLoss: 4.0504\tLR: 0.010000\n",
            "Training Epoch: 18 [20096/50000]\tLoss: 3.9992\tLR: 0.010000\n",
            "Training Epoch: 18 [20224/50000]\tLoss: 3.9672\tLR: 0.010000\n",
            "Training Epoch: 18 [20352/50000]\tLoss: 4.0903\tLR: 0.010000\n",
            "Training Epoch: 18 [20480/50000]\tLoss: 4.0139\tLR: 0.010000\n",
            "Training Epoch: 18 [20608/50000]\tLoss: 4.1138\tLR: 0.010000\n",
            "Training Epoch: 18 [20736/50000]\tLoss: 4.1908\tLR: 0.010000\n",
            "Training Epoch: 18 [20864/50000]\tLoss: 3.9421\tLR: 0.010000\n",
            "Training Epoch: 18 [20992/50000]\tLoss: 3.9464\tLR: 0.010000\n",
            "Training Epoch: 18 [21120/50000]\tLoss: 3.9791\tLR: 0.010000\n",
            "Training Epoch: 18 [21248/50000]\tLoss: 3.9166\tLR: 0.010000\n",
            "Training Epoch: 18 [21376/50000]\tLoss: 3.9818\tLR: 0.010000\n",
            "Training Epoch: 18 [21504/50000]\tLoss: 3.9425\tLR: 0.010000\n",
            "Training Epoch: 18 [21632/50000]\tLoss: 4.1950\tLR: 0.010000\n",
            "Training Epoch: 18 [21760/50000]\tLoss: 3.9869\tLR: 0.010000\n",
            "Training Epoch: 18 [21888/50000]\tLoss: 3.9925\tLR: 0.010000\n",
            "Training Epoch: 18 [22016/50000]\tLoss: 3.8697\tLR: 0.010000\n",
            "Training Epoch: 18 [22144/50000]\tLoss: 4.1944\tLR: 0.010000\n",
            "Training Epoch: 18 [22272/50000]\tLoss: 4.0105\tLR: 0.010000\n",
            "Training Epoch: 18 [22400/50000]\tLoss: 3.9522\tLR: 0.010000\n",
            "Training Epoch: 18 [22528/50000]\tLoss: 4.1019\tLR: 0.010000\n",
            "Training Epoch: 18 [22656/50000]\tLoss: 4.2124\tLR: 0.010000\n",
            "Training Epoch: 18 [22784/50000]\tLoss: 4.1285\tLR: 0.010000\n",
            "Training Epoch: 18 [22912/50000]\tLoss: 3.9797\tLR: 0.010000\n",
            "Training Epoch: 18 [23040/50000]\tLoss: 4.0369\tLR: 0.010000\n",
            "Training Epoch: 18 [23168/50000]\tLoss: 4.1403\tLR: 0.010000\n",
            "Training Epoch: 18 [23296/50000]\tLoss: 3.9101\tLR: 0.010000\n",
            "Training Epoch: 18 [23424/50000]\tLoss: 4.1316\tLR: 0.010000\n",
            "Training Epoch: 18 [23552/50000]\tLoss: 4.0182\tLR: 0.010000\n",
            "Training Epoch: 18 [23680/50000]\tLoss: 4.0897\tLR: 0.010000\n",
            "Training Epoch: 18 [23808/50000]\tLoss: 3.9208\tLR: 0.010000\n",
            "Training Epoch: 18 [23936/50000]\tLoss: 4.0686\tLR: 0.010000\n",
            "Training Epoch: 18 [24064/50000]\tLoss: 3.9123\tLR: 0.010000\n",
            "Training Epoch: 18 [24192/50000]\tLoss: 4.0567\tLR: 0.010000\n",
            "Training Epoch: 18 [24320/50000]\tLoss: 3.8259\tLR: 0.010000\n",
            "Training Epoch: 18 [24448/50000]\tLoss: 4.0057\tLR: 0.010000\n",
            "Training Epoch: 18 [24576/50000]\tLoss: 3.8182\tLR: 0.010000\n",
            "Training Epoch: 18 [24704/50000]\tLoss: 4.0878\tLR: 0.010000\n",
            "Training Epoch: 18 [24832/50000]\tLoss: 3.9900\tLR: 0.010000\n",
            "Training Epoch: 18 [24960/50000]\tLoss: 3.9413\tLR: 0.010000\n",
            "Training Epoch: 18 [25088/50000]\tLoss: 4.0872\tLR: 0.010000\n",
            "Training Epoch: 18 [25216/50000]\tLoss: 3.9782\tLR: 0.010000\n",
            "Training Epoch: 18 [25344/50000]\tLoss: 3.8586\tLR: 0.010000\n",
            "Training Epoch: 18 [25472/50000]\tLoss: 3.9355\tLR: 0.010000\n",
            "Training Epoch: 18 [25600/50000]\tLoss: 4.1835\tLR: 0.010000\n",
            "Training Epoch: 18 [25728/50000]\tLoss: 4.0251\tLR: 0.010000\n",
            "Training Epoch: 18 [25856/50000]\tLoss: 4.0497\tLR: 0.010000\n",
            "Training Epoch: 18 [25984/50000]\tLoss: 4.0095\tLR: 0.010000\n",
            "Training Epoch: 18 [26112/50000]\tLoss: 3.9054\tLR: 0.010000\n",
            "Training Epoch: 18 [26240/50000]\tLoss: 4.2179\tLR: 0.010000\n",
            "Training Epoch: 18 [26368/50000]\tLoss: 3.8976\tLR: 0.010000\n",
            "Training Epoch: 18 [26496/50000]\tLoss: 3.9350\tLR: 0.010000\n",
            "Training Epoch: 18 [26624/50000]\tLoss: 3.9541\tLR: 0.010000\n",
            "Training Epoch: 18 [26752/50000]\tLoss: 4.0059\tLR: 0.010000\n",
            "Training Epoch: 18 [26880/50000]\tLoss: 3.9336\tLR: 0.010000\n",
            "Training Epoch: 18 [27008/50000]\tLoss: 4.0479\tLR: 0.010000\n",
            "Training Epoch: 18 [27136/50000]\tLoss: 3.9226\tLR: 0.010000\n",
            "Training Epoch: 18 [27264/50000]\tLoss: 3.9498\tLR: 0.010000\n",
            "Training Epoch: 18 [27392/50000]\tLoss: 3.8931\tLR: 0.010000\n",
            "Training Epoch: 18 [27520/50000]\tLoss: 3.9732\tLR: 0.010000\n",
            "Training Epoch: 18 [27648/50000]\tLoss: 4.0432\tLR: 0.010000\n",
            "Training Epoch: 18 [27776/50000]\tLoss: 4.0752\tLR: 0.010000\n",
            "Training Epoch: 18 [27904/50000]\tLoss: 3.9325\tLR: 0.010000\n",
            "Training Epoch: 18 [28032/50000]\tLoss: 3.8692\tLR: 0.010000\n",
            "Training Epoch: 18 [28160/50000]\tLoss: 4.0897\tLR: 0.010000\n",
            "Training Epoch: 18 [28288/50000]\tLoss: 4.0205\tLR: 0.010000\n",
            "Training Epoch: 18 [28416/50000]\tLoss: 3.8400\tLR: 0.010000\n",
            "Training Epoch: 18 [28544/50000]\tLoss: 3.8819\tLR: 0.010000\n",
            "Training Epoch: 18 [28672/50000]\tLoss: 4.0792\tLR: 0.010000\n",
            "Training Epoch: 18 [28800/50000]\tLoss: 4.0794\tLR: 0.010000\n",
            "Training Epoch: 18 [28928/50000]\tLoss: 3.9976\tLR: 0.010000\n",
            "Training Epoch: 18 [29056/50000]\tLoss: 3.9919\tLR: 0.010000\n",
            "Training Epoch: 18 [29184/50000]\tLoss: 4.0468\tLR: 0.010000\n",
            "Training Epoch: 18 [29312/50000]\tLoss: 4.1296\tLR: 0.010000\n",
            "Training Epoch: 18 [29440/50000]\tLoss: 3.9547\tLR: 0.010000\n",
            "Training Epoch: 18 [29568/50000]\tLoss: 4.0133\tLR: 0.010000\n",
            "Training Epoch: 18 [29696/50000]\tLoss: 4.0789\tLR: 0.010000\n",
            "Training Epoch: 18 [29824/50000]\tLoss: 4.0694\tLR: 0.010000\n",
            "Training Epoch: 18 [29952/50000]\tLoss: 3.9097\tLR: 0.010000\n",
            "Training Epoch: 18 [30080/50000]\tLoss: 3.9408\tLR: 0.010000\n",
            "Training Epoch: 18 [30208/50000]\tLoss: 3.8503\tLR: 0.010000\n",
            "Training Epoch: 18 [30336/50000]\tLoss: 3.9802\tLR: 0.010000\n",
            "Training Epoch: 18 [30464/50000]\tLoss: 4.0355\tLR: 0.010000\n",
            "Training Epoch: 18 [30592/50000]\tLoss: 3.9220\tLR: 0.010000\n",
            "Training Epoch: 18 [30720/50000]\tLoss: 4.0719\tLR: 0.010000\n",
            "Training Epoch: 18 [30848/50000]\tLoss: 4.0693\tLR: 0.010000\n",
            "Training Epoch: 18 [30976/50000]\tLoss: 3.9189\tLR: 0.010000\n",
            "Training Epoch: 18 [31104/50000]\tLoss: 4.0075\tLR: 0.010000\n",
            "Training Epoch: 18 [31232/50000]\tLoss: 4.1411\tLR: 0.010000\n",
            "Training Epoch: 18 [31360/50000]\tLoss: 4.0948\tLR: 0.010000\n",
            "Training Epoch: 18 [31488/50000]\tLoss: 3.9203\tLR: 0.010000\n",
            "Training Epoch: 18 [31616/50000]\tLoss: 4.1104\tLR: 0.010000\n",
            "Training Epoch: 18 [31744/50000]\tLoss: 4.0565\tLR: 0.010000\n",
            "Training Epoch: 18 [31872/50000]\tLoss: 3.9931\tLR: 0.010000\n",
            "Training Epoch: 18 [32000/50000]\tLoss: 4.0657\tLR: 0.010000\n",
            "Training Epoch: 18 [32128/50000]\tLoss: 3.9715\tLR: 0.010000\n",
            "Training Epoch: 18 [32256/50000]\tLoss: 4.0789\tLR: 0.010000\n",
            "Training Epoch: 18 [32384/50000]\tLoss: 4.0525\tLR: 0.010000\n",
            "Training Epoch: 18 [32512/50000]\tLoss: 4.0224\tLR: 0.010000\n",
            "Training Epoch: 18 [32640/50000]\tLoss: 3.9995\tLR: 0.010000\n",
            "Training Epoch: 18 [32768/50000]\tLoss: 3.8518\tLR: 0.010000\n",
            "Training Epoch: 18 [32896/50000]\tLoss: 4.0705\tLR: 0.010000\n",
            "Training Epoch: 18 [33024/50000]\tLoss: 4.0500\tLR: 0.010000\n",
            "Training Epoch: 18 [33152/50000]\tLoss: 4.0319\tLR: 0.010000\n",
            "Training Epoch: 18 [33280/50000]\tLoss: 4.0108\tLR: 0.010000\n",
            "Training Epoch: 18 [33408/50000]\tLoss: 3.9168\tLR: 0.010000\n",
            "Training Epoch: 18 [33536/50000]\tLoss: 3.9119\tLR: 0.010000\n",
            "Training Epoch: 18 [33664/50000]\tLoss: 4.0344\tLR: 0.010000\n",
            "Training Epoch: 18 [33792/50000]\tLoss: 4.1778\tLR: 0.010000\n",
            "Training Epoch: 18 [33920/50000]\tLoss: 4.0615\tLR: 0.010000\n",
            "Training Epoch: 18 [34048/50000]\tLoss: 4.0066\tLR: 0.010000\n",
            "Training Epoch: 18 [34176/50000]\tLoss: 3.9331\tLR: 0.010000\n",
            "Training Epoch: 18 [34304/50000]\tLoss: 3.9636\tLR: 0.010000\n",
            "Training Epoch: 18 [34432/50000]\tLoss: 4.0985\tLR: 0.010000\n",
            "Training Epoch: 18 [34560/50000]\tLoss: 4.0484\tLR: 0.010000\n",
            "Training Epoch: 18 [34688/50000]\tLoss: 3.7894\tLR: 0.010000\n",
            "Training Epoch: 18 [34816/50000]\tLoss: 4.1515\tLR: 0.010000\n",
            "Training Epoch: 18 [34944/50000]\tLoss: 3.9625\tLR: 0.010000\n",
            "Training Epoch: 18 [35072/50000]\tLoss: 3.9107\tLR: 0.010000\n",
            "Training Epoch: 18 [35200/50000]\tLoss: 4.0648\tLR: 0.010000\n",
            "Training Epoch: 18 [35328/50000]\tLoss: 3.9649\tLR: 0.010000\n",
            "Training Epoch: 18 [35456/50000]\tLoss: 3.9847\tLR: 0.010000\n",
            "Training Epoch: 18 [35584/50000]\tLoss: 4.0742\tLR: 0.010000\n",
            "Training Epoch: 18 [35712/50000]\tLoss: 3.7533\tLR: 0.010000\n",
            "Training Epoch: 18 [35840/50000]\tLoss: 3.9786\tLR: 0.010000\n",
            "Training Epoch: 18 [35968/50000]\tLoss: 3.9437\tLR: 0.010000\n",
            "Training Epoch: 18 [36096/50000]\tLoss: 4.0190\tLR: 0.010000\n",
            "Training Epoch: 18 [36224/50000]\tLoss: 3.9040\tLR: 0.010000\n",
            "Training Epoch: 18 [36352/50000]\tLoss: 3.9317\tLR: 0.010000\n",
            "Training Epoch: 18 [36480/50000]\tLoss: 4.1282\tLR: 0.010000\n",
            "Training Epoch: 18 [36608/50000]\tLoss: 4.1169\tLR: 0.010000\n",
            "Training Epoch: 18 [36736/50000]\tLoss: 4.0341\tLR: 0.010000\n",
            "Training Epoch: 18 [36864/50000]\tLoss: 4.0298\tLR: 0.010000\n",
            "Training Epoch: 18 [36992/50000]\tLoss: 3.8963\tLR: 0.010000\n",
            "Training Epoch: 18 [37120/50000]\tLoss: 3.9613\tLR: 0.010000\n",
            "Training Epoch: 18 [37248/50000]\tLoss: 3.9952\tLR: 0.010000\n",
            "Training Epoch: 18 [37376/50000]\tLoss: 3.9848\tLR: 0.010000\n",
            "Training Epoch: 18 [37504/50000]\tLoss: 4.0622\tLR: 0.010000\n",
            "Training Epoch: 18 [37632/50000]\tLoss: 3.8416\tLR: 0.010000\n",
            "Training Epoch: 18 [37760/50000]\tLoss: 4.0014\tLR: 0.010000\n",
            "Training Epoch: 18 [37888/50000]\tLoss: 4.0205\tLR: 0.010000\n",
            "Training Epoch: 18 [38016/50000]\tLoss: 3.9125\tLR: 0.010000\n",
            "Training Epoch: 18 [38144/50000]\tLoss: 4.0558\tLR: 0.010000\n",
            "Training Epoch: 18 [38272/50000]\tLoss: 3.8653\tLR: 0.010000\n",
            "Training Epoch: 18 [38400/50000]\tLoss: 4.0770\tLR: 0.010000\n",
            "Training Epoch: 18 [38528/50000]\tLoss: 4.0605\tLR: 0.010000\n",
            "Training Epoch: 18 [38656/50000]\tLoss: 3.8970\tLR: 0.010000\n",
            "Training Epoch: 18 [38784/50000]\tLoss: 4.0128\tLR: 0.010000\n",
            "Training Epoch: 18 [38912/50000]\tLoss: 4.0634\tLR: 0.010000\n",
            "Training Epoch: 18 [39040/50000]\tLoss: 3.9596\tLR: 0.010000\n",
            "Training Epoch: 18 [39168/50000]\tLoss: 4.0903\tLR: 0.010000\n",
            "Training Epoch: 18 [39296/50000]\tLoss: 3.8995\tLR: 0.010000\n",
            "Training Epoch: 18 [39424/50000]\tLoss: 3.9710\tLR: 0.010000\n",
            "Training Epoch: 18 [39552/50000]\tLoss: 3.9349\tLR: 0.010000\n",
            "Training Epoch: 18 [39680/50000]\tLoss: 3.9466\tLR: 0.010000\n",
            "Training Epoch: 18 [39808/50000]\tLoss: 3.9880\tLR: 0.010000\n",
            "Training Epoch: 18 [39936/50000]\tLoss: 4.1212\tLR: 0.010000\n",
            "Training Epoch: 18 [40064/50000]\tLoss: 3.9881\tLR: 0.010000\n",
            "Training Epoch: 18 [40192/50000]\tLoss: 4.0561\tLR: 0.010000\n",
            "Training Epoch: 18 [40320/50000]\tLoss: 3.9707\tLR: 0.010000\n",
            "Training Epoch: 18 [40448/50000]\tLoss: 3.8841\tLR: 0.010000\n",
            "Training Epoch: 18 [40576/50000]\tLoss: 4.1288\tLR: 0.010000\n",
            "Training Epoch: 18 [40704/50000]\tLoss: 3.8329\tLR: 0.010000\n",
            "Training Epoch: 18 [40832/50000]\tLoss: 4.0460\tLR: 0.010000\n",
            "Training Epoch: 18 [40960/50000]\tLoss: 3.9681\tLR: 0.010000\n",
            "Training Epoch: 18 [41088/50000]\tLoss: 3.7548\tLR: 0.010000\n",
            "Training Epoch: 18 [41216/50000]\tLoss: 3.7222\tLR: 0.010000\n",
            "Training Epoch: 18 [41344/50000]\tLoss: 4.0772\tLR: 0.010000\n",
            "Training Epoch: 18 [41472/50000]\tLoss: 3.8383\tLR: 0.010000\n",
            "Training Epoch: 18 [41600/50000]\tLoss: 4.0275\tLR: 0.010000\n",
            "Training Epoch: 18 [41728/50000]\tLoss: 4.0331\tLR: 0.010000\n",
            "Training Epoch: 18 [41856/50000]\tLoss: 3.9746\tLR: 0.010000\n",
            "Training Epoch: 18 [41984/50000]\tLoss: 3.9864\tLR: 0.010000\n",
            "Training Epoch: 18 [42112/50000]\tLoss: 3.9309\tLR: 0.010000\n",
            "Training Epoch: 18 [42240/50000]\tLoss: 4.0322\tLR: 0.010000\n",
            "Training Epoch: 18 [42368/50000]\tLoss: 4.0493\tLR: 0.010000\n",
            "Training Epoch: 18 [42496/50000]\tLoss: 3.7645\tLR: 0.010000\n",
            "Training Epoch: 18 [42624/50000]\tLoss: 3.9344\tLR: 0.010000\n",
            "Training Epoch: 18 [42752/50000]\tLoss: 4.0196\tLR: 0.010000\n",
            "Training Epoch: 18 [42880/50000]\tLoss: 3.8994\tLR: 0.010000\n",
            "Training Epoch: 18 [43008/50000]\tLoss: 4.0789\tLR: 0.010000\n",
            "Training Epoch: 18 [43136/50000]\tLoss: 4.0276\tLR: 0.010000\n",
            "Training Epoch: 18 [43264/50000]\tLoss: 4.0195\tLR: 0.010000\n",
            "Training Epoch: 18 [43392/50000]\tLoss: 3.8311\tLR: 0.010000\n",
            "Training Epoch: 18 [43520/50000]\tLoss: 3.9548\tLR: 0.010000\n",
            "Training Epoch: 18 [43648/50000]\tLoss: 4.0436\tLR: 0.010000\n",
            "Training Epoch: 18 [43776/50000]\tLoss: 3.9207\tLR: 0.010000\n",
            "Training Epoch: 18 [43904/50000]\tLoss: 3.9290\tLR: 0.010000\n",
            "Training Epoch: 18 [44032/50000]\tLoss: 3.9843\tLR: 0.010000\n",
            "Training Epoch: 18 [44160/50000]\tLoss: 4.0020\tLR: 0.010000\n",
            "Training Epoch: 18 [44288/50000]\tLoss: 3.9425\tLR: 0.010000\n",
            "Training Epoch: 18 [44416/50000]\tLoss: 3.8516\tLR: 0.010000\n",
            "Training Epoch: 18 [44544/50000]\tLoss: 3.9848\tLR: 0.010000\n",
            "Training Epoch: 18 [44672/50000]\tLoss: 3.9833\tLR: 0.010000\n",
            "Training Epoch: 18 [44800/50000]\tLoss: 4.0008\tLR: 0.010000\n",
            "Training Epoch: 18 [44928/50000]\tLoss: 4.1603\tLR: 0.010000\n",
            "Training Epoch: 18 [45056/50000]\tLoss: 3.9760\tLR: 0.010000\n",
            "Training Epoch: 18 [45184/50000]\tLoss: 4.0897\tLR: 0.010000\n",
            "Training Epoch: 18 [45312/50000]\tLoss: 4.0652\tLR: 0.010000\n",
            "Training Epoch: 18 [45440/50000]\tLoss: 3.9084\tLR: 0.010000\n",
            "Training Epoch: 18 [45568/50000]\tLoss: 3.9252\tLR: 0.010000\n",
            "Training Epoch: 18 [45696/50000]\tLoss: 3.8645\tLR: 0.010000\n",
            "Training Epoch: 18 [45824/50000]\tLoss: 4.0575\tLR: 0.010000\n",
            "Training Epoch: 18 [45952/50000]\tLoss: 4.0205\tLR: 0.010000\n",
            "Training Epoch: 18 [46080/50000]\tLoss: 3.9622\tLR: 0.010000\n",
            "Training Epoch: 18 [46208/50000]\tLoss: 3.9082\tLR: 0.010000\n",
            "Training Epoch: 18 [46336/50000]\tLoss: 3.9149\tLR: 0.010000\n",
            "Training Epoch: 18 [46464/50000]\tLoss: 4.0038\tLR: 0.010000\n",
            "Training Epoch: 18 [46592/50000]\tLoss: 3.9093\tLR: 0.010000\n",
            "Training Epoch: 18 [46720/50000]\tLoss: 3.9684\tLR: 0.010000\n",
            "Training Epoch: 18 [46848/50000]\tLoss: 3.8822\tLR: 0.010000\n",
            "Training Epoch: 18 [46976/50000]\tLoss: 3.9956\tLR: 0.010000\n",
            "Training Epoch: 18 [47104/50000]\tLoss: 3.8982\tLR: 0.010000\n",
            "Training Epoch: 18 [47232/50000]\tLoss: 4.0649\tLR: 0.010000\n",
            "Training Epoch: 18 [47360/50000]\tLoss: 4.0123\tLR: 0.010000\n",
            "Training Epoch: 18 [47488/50000]\tLoss: 4.1042\tLR: 0.010000\n",
            "Training Epoch: 18 [47616/50000]\tLoss: 4.0193\tLR: 0.010000\n",
            "Training Epoch: 18 [47744/50000]\tLoss: 3.9681\tLR: 0.010000\n",
            "Training Epoch: 18 [47872/50000]\tLoss: 3.8313\tLR: 0.010000\n",
            "Training Epoch: 18 [48000/50000]\tLoss: 3.8083\tLR: 0.010000\n",
            "Training Epoch: 18 [48128/50000]\tLoss: 3.9733\tLR: 0.010000\n",
            "Training Epoch: 18 [48256/50000]\tLoss: 4.0342\tLR: 0.010000\n",
            "Training Epoch: 18 [48384/50000]\tLoss: 4.1585\tLR: 0.010000\n",
            "Training Epoch: 18 [48512/50000]\tLoss: 3.9673\tLR: 0.010000\n",
            "Training Epoch: 18 [48640/50000]\tLoss: 3.9869\tLR: 0.010000\n",
            "Training Epoch: 18 [48768/50000]\tLoss: 4.0116\tLR: 0.010000\n",
            "Training Epoch: 18 [48896/50000]\tLoss: 4.2511\tLR: 0.010000\n",
            "Training Epoch: 18 [49024/50000]\tLoss: 3.9535\tLR: 0.010000\n",
            "Training Epoch: 18 [49152/50000]\tLoss: 3.8372\tLR: 0.010000\n",
            "Training Epoch: 18 [49280/50000]\tLoss: 3.9551\tLR: 0.010000\n",
            "Training Epoch: 18 [49408/50000]\tLoss: 3.9086\tLR: 0.010000\n",
            "Training Epoch: 18 [49536/50000]\tLoss: 3.9791\tLR: 0.010000\n",
            "Training Epoch: 18 [49664/50000]\tLoss: 3.8243\tLR: 0.010000\n",
            "Training Epoch: 18 [49792/50000]\tLoss: 3.9875\tLR: 0.010000\n",
            "Training Epoch: 18 [49920/50000]\tLoss: 4.0467\tLR: 0.010000\n",
            "Training Epoch: 18 [50000/50000]\tLoss: 4.1874\tLR: 0.010000\n",
            "epoch 18 training time consumed: 145.81s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 191064 GiB | 191063 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 188653 GiB | 188653 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2410 GiB |   2410 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 191064 GiB | 191063 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 188653 GiB | 188653 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2410 GiB |   2410 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 190540 GiB | 190540 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 188130 GiB | 188130 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2410 GiB |   2410 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 159508 GiB | 159508 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 156785 GiB | 156785 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2723 GiB |   2723 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |   11642 K  |   11641 K  |\n",
            "|       from large pool |       5    |     146    |    5643 K  |    5643 K  |\n",
            "|       from small pool |     516    |     682    |    5998 K  |    5998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |   11642 K  |   11641 K  |\n",
            "|       from large pool |       5    |     146    |    5643 K  |    5643 K  |\n",
            "|       from small pool |     516    |     682    |    5998 K  |    5998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |     117    |    4331 K  |    4331 K  |\n",
            "|       from large pool |       4    |      46    |    2654 K  |    2654 K  |\n",
            "|       from small pool |      75    |      89    |    1677 K  |    1677 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0314, Accuracy: 0.0846, Time consumed:9.35s\n",
            "\n",
            "Training Epoch: 19 [128/50000]\tLoss: 4.0277\tLR: 0.010000\n",
            "Training Epoch: 19 [256/50000]\tLoss: 3.8339\tLR: 0.010000\n",
            "Training Epoch: 19 [384/50000]\tLoss: 4.0356\tLR: 0.010000\n",
            "Training Epoch: 19 [512/50000]\tLoss: 3.9029\tLR: 0.010000\n",
            "Training Epoch: 19 [640/50000]\tLoss: 3.8877\tLR: 0.010000\n",
            "Training Epoch: 19 [768/50000]\tLoss: 4.0660\tLR: 0.010000\n",
            "Training Epoch: 19 [896/50000]\tLoss: 4.0632\tLR: 0.010000\n",
            "Training Epoch: 19 [1024/50000]\tLoss: 4.0147\tLR: 0.010000\n",
            "Training Epoch: 19 [1152/50000]\tLoss: 3.8865\tLR: 0.010000\n",
            "Training Epoch: 19 [1280/50000]\tLoss: 4.1013\tLR: 0.010000\n",
            "Training Epoch: 19 [1408/50000]\tLoss: 3.9178\tLR: 0.010000\n",
            "Training Epoch: 19 [1536/50000]\tLoss: 3.8933\tLR: 0.010000\n",
            "Training Epoch: 19 [1664/50000]\tLoss: 3.9913\tLR: 0.010000\n",
            "Training Epoch: 19 [1792/50000]\tLoss: 3.9084\tLR: 0.010000\n",
            "Training Epoch: 19 [1920/50000]\tLoss: 3.9821\tLR: 0.010000\n",
            "Training Epoch: 19 [2048/50000]\tLoss: 3.9637\tLR: 0.010000\n",
            "Training Epoch: 19 [2176/50000]\tLoss: 4.0156\tLR: 0.010000\n",
            "Training Epoch: 19 [2304/50000]\tLoss: 3.8572\tLR: 0.010000\n",
            "Training Epoch: 19 [2432/50000]\tLoss: 4.0170\tLR: 0.010000\n",
            "Training Epoch: 19 [2560/50000]\tLoss: 3.8263\tLR: 0.010000\n",
            "Training Epoch: 19 [2688/50000]\tLoss: 3.7728\tLR: 0.010000\n",
            "Training Epoch: 19 [2816/50000]\tLoss: 3.9109\tLR: 0.010000\n",
            "Training Epoch: 19 [2944/50000]\tLoss: 3.9178\tLR: 0.010000\n",
            "Training Epoch: 19 [3072/50000]\tLoss: 3.9874\tLR: 0.010000\n",
            "Training Epoch: 19 [3200/50000]\tLoss: 3.8832\tLR: 0.010000\n",
            "Training Epoch: 19 [3328/50000]\tLoss: 3.9526\tLR: 0.010000\n",
            "Training Epoch: 19 [3456/50000]\tLoss: 4.1065\tLR: 0.010000\n",
            "Training Epoch: 19 [3584/50000]\tLoss: 4.1019\tLR: 0.010000\n",
            "Training Epoch: 19 [3712/50000]\tLoss: 3.9684\tLR: 0.010000\n",
            "Training Epoch: 19 [3840/50000]\tLoss: 4.0031\tLR: 0.010000\n",
            "Training Epoch: 19 [3968/50000]\tLoss: 3.8640\tLR: 0.010000\n",
            "Training Epoch: 19 [4096/50000]\tLoss: 4.0189\tLR: 0.010000\n",
            "Training Epoch: 19 [4224/50000]\tLoss: 3.9268\tLR: 0.010000\n",
            "Training Epoch: 19 [4352/50000]\tLoss: 3.9197\tLR: 0.010000\n",
            "Training Epoch: 19 [4480/50000]\tLoss: 3.8258\tLR: 0.010000\n",
            "Training Epoch: 19 [4608/50000]\tLoss: 4.0352\tLR: 0.010000\n",
            "Training Epoch: 19 [4736/50000]\tLoss: 3.8973\tLR: 0.010000\n",
            "Training Epoch: 19 [4864/50000]\tLoss: 3.9272\tLR: 0.010000\n",
            "Training Epoch: 19 [4992/50000]\tLoss: 4.0976\tLR: 0.010000\n",
            "Training Epoch: 19 [5120/50000]\tLoss: 4.0495\tLR: 0.010000\n",
            "Training Epoch: 19 [5248/50000]\tLoss: 4.0097\tLR: 0.010000\n",
            "Training Epoch: 19 [5376/50000]\tLoss: 3.9852\tLR: 0.010000\n",
            "Training Epoch: 19 [5504/50000]\tLoss: 3.7867\tLR: 0.010000\n",
            "Training Epoch: 19 [5632/50000]\tLoss: 3.8507\tLR: 0.010000\n",
            "Training Epoch: 19 [5760/50000]\tLoss: 3.8013\tLR: 0.010000\n",
            "Training Epoch: 19 [5888/50000]\tLoss: 3.8993\tLR: 0.010000\n",
            "Training Epoch: 19 [6016/50000]\tLoss: 3.8592\tLR: 0.010000\n",
            "Training Epoch: 19 [6144/50000]\tLoss: 3.9284\tLR: 0.010000\n",
            "Training Epoch: 19 [6272/50000]\tLoss: 4.0230\tLR: 0.010000\n",
            "Training Epoch: 19 [6400/50000]\tLoss: 3.9877\tLR: 0.010000\n",
            "Training Epoch: 19 [6528/50000]\tLoss: 4.1147\tLR: 0.010000\n",
            "Training Epoch: 19 [6656/50000]\tLoss: 3.9159\tLR: 0.010000\n",
            "Training Epoch: 19 [6784/50000]\tLoss: 4.1011\tLR: 0.010000\n",
            "Training Epoch: 19 [6912/50000]\tLoss: 3.8793\tLR: 0.010000\n",
            "Training Epoch: 19 [7040/50000]\tLoss: 4.0494\tLR: 0.010000\n",
            "Training Epoch: 19 [7168/50000]\tLoss: 3.8794\tLR: 0.010000\n",
            "Training Epoch: 19 [7296/50000]\tLoss: 4.1211\tLR: 0.010000\n",
            "Training Epoch: 19 [7424/50000]\tLoss: 3.8677\tLR: 0.010000\n",
            "Training Epoch: 19 [7552/50000]\tLoss: 4.0963\tLR: 0.010000\n",
            "Training Epoch: 19 [7680/50000]\tLoss: 4.2261\tLR: 0.010000\n",
            "Training Epoch: 19 [7808/50000]\tLoss: 4.0776\tLR: 0.010000\n",
            "Training Epoch: 19 [7936/50000]\tLoss: 4.1696\tLR: 0.010000\n",
            "Training Epoch: 19 [8064/50000]\tLoss: 3.8278\tLR: 0.010000\n",
            "Training Epoch: 19 [8192/50000]\tLoss: 3.8435\tLR: 0.010000\n",
            "Training Epoch: 19 [8320/50000]\tLoss: 3.9593\tLR: 0.010000\n",
            "Training Epoch: 19 [8448/50000]\tLoss: 3.7904\tLR: 0.010000\n",
            "Training Epoch: 19 [8576/50000]\tLoss: 3.9848\tLR: 0.010000\n",
            "Training Epoch: 19 [8704/50000]\tLoss: 3.7743\tLR: 0.010000\n",
            "Training Epoch: 19 [8832/50000]\tLoss: 3.9545\tLR: 0.010000\n",
            "Training Epoch: 19 [8960/50000]\tLoss: 3.9888\tLR: 0.010000\n",
            "Training Epoch: 19 [9088/50000]\tLoss: 4.0302\tLR: 0.010000\n",
            "Training Epoch: 19 [9216/50000]\tLoss: 3.9672\tLR: 0.010000\n",
            "Training Epoch: 19 [9344/50000]\tLoss: 4.1570\tLR: 0.010000\n",
            "Training Epoch: 19 [9472/50000]\tLoss: 3.9560\tLR: 0.010000\n",
            "Training Epoch: 19 [9600/50000]\tLoss: 4.0363\tLR: 0.010000\n",
            "Training Epoch: 19 [9728/50000]\tLoss: 3.9981\tLR: 0.010000\n",
            "Training Epoch: 19 [9856/50000]\tLoss: 3.8148\tLR: 0.010000\n",
            "Training Epoch: 19 [9984/50000]\tLoss: 3.9062\tLR: 0.010000\n",
            "Training Epoch: 19 [10112/50000]\tLoss: 4.0598\tLR: 0.010000\n",
            "Training Epoch: 19 [10240/50000]\tLoss: 4.1070\tLR: 0.010000\n",
            "Training Epoch: 19 [10368/50000]\tLoss: 3.8185\tLR: 0.010000\n",
            "Training Epoch: 19 [10496/50000]\tLoss: 3.8613\tLR: 0.010000\n",
            "Training Epoch: 19 [10624/50000]\tLoss: 3.8684\tLR: 0.010000\n",
            "Training Epoch: 19 [10752/50000]\tLoss: 3.9658\tLR: 0.010000\n",
            "Training Epoch: 19 [10880/50000]\tLoss: 4.0500\tLR: 0.010000\n",
            "Training Epoch: 19 [11008/50000]\tLoss: 4.1103\tLR: 0.010000\n",
            "Training Epoch: 19 [11136/50000]\tLoss: 3.8678\tLR: 0.010000\n",
            "Training Epoch: 19 [11264/50000]\tLoss: 3.9140\tLR: 0.010000\n",
            "Training Epoch: 19 [11392/50000]\tLoss: 3.9546\tLR: 0.010000\n",
            "Training Epoch: 19 [11520/50000]\tLoss: 4.0560\tLR: 0.010000\n",
            "Training Epoch: 19 [11648/50000]\tLoss: 4.2248\tLR: 0.010000\n",
            "Training Epoch: 19 [11776/50000]\tLoss: 3.9463\tLR: 0.010000\n",
            "Training Epoch: 19 [11904/50000]\tLoss: 3.9402\tLR: 0.010000\n",
            "Training Epoch: 19 [12032/50000]\tLoss: 4.0460\tLR: 0.010000\n",
            "Training Epoch: 19 [12160/50000]\tLoss: 4.0210\tLR: 0.010000\n",
            "Training Epoch: 19 [12288/50000]\tLoss: 3.8424\tLR: 0.010000\n",
            "Training Epoch: 19 [12416/50000]\tLoss: 4.1186\tLR: 0.010000\n",
            "Training Epoch: 19 [12544/50000]\tLoss: 3.8989\tLR: 0.010000\n",
            "Training Epoch: 19 [12672/50000]\tLoss: 3.9051\tLR: 0.010000\n",
            "Training Epoch: 19 [12800/50000]\tLoss: 3.8354\tLR: 0.010000\n",
            "Training Epoch: 19 [12928/50000]\tLoss: 4.0148\tLR: 0.010000\n",
            "Training Epoch: 19 [13056/50000]\tLoss: 4.0882\tLR: 0.010000\n",
            "Training Epoch: 19 [13184/50000]\tLoss: 3.8541\tLR: 0.010000\n",
            "Training Epoch: 19 [13312/50000]\tLoss: 3.9092\tLR: 0.010000\n",
            "Training Epoch: 19 [13440/50000]\tLoss: 3.8996\tLR: 0.010000\n",
            "Training Epoch: 19 [13568/50000]\tLoss: 3.9122\tLR: 0.010000\n",
            "Training Epoch: 19 [13696/50000]\tLoss: 4.1183\tLR: 0.010000\n",
            "Training Epoch: 19 [13824/50000]\tLoss: 3.9822\tLR: 0.010000\n",
            "Training Epoch: 19 [13952/50000]\tLoss: 3.9724\tLR: 0.010000\n",
            "Training Epoch: 19 [14080/50000]\tLoss: 3.8653\tLR: 0.010000\n",
            "Training Epoch: 19 [14208/50000]\tLoss: 3.8342\tLR: 0.010000\n",
            "Training Epoch: 19 [14336/50000]\tLoss: 4.2068\tLR: 0.010000\n",
            "Training Epoch: 19 [14464/50000]\tLoss: 4.1011\tLR: 0.010000\n",
            "Training Epoch: 19 [14592/50000]\tLoss: 3.9108\tLR: 0.010000\n",
            "Training Epoch: 19 [14720/50000]\tLoss: 4.1540\tLR: 0.010000\n",
            "Training Epoch: 19 [14848/50000]\tLoss: 3.9722\tLR: 0.010000\n",
            "Training Epoch: 19 [14976/50000]\tLoss: 3.9870\tLR: 0.010000\n",
            "Training Epoch: 19 [15104/50000]\tLoss: 4.0158\tLR: 0.010000\n",
            "Training Epoch: 19 [15232/50000]\tLoss: 4.0532\tLR: 0.010000\n",
            "Training Epoch: 19 [15360/50000]\tLoss: 4.0752\tLR: 0.010000\n",
            "Training Epoch: 19 [15488/50000]\tLoss: 4.0274\tLR: 0.010000\n",
            "Training Epoch: 19 [15616/50000]\tLoss: 3.9934\tLR: 0.010000\n",
            "Training Epoch: 19 [15744/50000]\tLoss: 3.9919\tLR: 0.010000\n",
            "Training Epoch: 19 [15872/50000]\tLoss: 3.8900\tLR: 0.010000\n",
            "Training Epoch: 19 [16000/50000]\tLoss: 3.9182\tLR: 0.010000\n",
            "Training Epoch: 19 [16128/50000]\tLoss: 3.9427\tLR: 0.010000\n",
            "Training Epoch: 19 [16256/50000]\tLoss: 3.7608\tLR: 0.010000\n",
            "Training Epoch: 19 [16384/50000]\tLoss: 3.8421\tLR: 0.010000\n",
            "Training Epoch: 19 [16512/50000]\tLoss: 3.9578\tLR: 0.010000\n",
            "Training Epoch: 19 [16640/50000]\tLoss: 3.9642\tLR: 0.010000\n",
            "Training Epoch: 19 [16768/50000]\tLoss: 3.9514\tLR: 0.010000\n",
            "Training Epoch: 19 [16896/50000]\tLoss: 3.8169\tLR: 0.010000\n",
            "Training Epoch: 19 [17024/50000]\tLoss: 3.9240\tLR: 0.010000\n",
            "Training Epoch: 19 [17152/50000]\tLoss: 4.0864\tLR: 0.010000\n",
            "Training Epoch: 19 [17280/50000]\tLoss: 4.1245\tLR: 0.010000\n",
            "Training Epoch: 19 [17408/50000]\tLoss: 4.0967\tLR: 0.010000\n",
            "Training Epoch: 19 [17536/50000]\tLoss: 3.7786\tLR: 0.010000\n",
            "Training Epoch: 19 [17664/50000]\tLoss: 3.8840\tLR: 0.010000\n",
            "Training Epoch: 19 [17792/50000]\tLoss: 3.8713\tLR: 0.010000\n",
            "Training Epoch: 19 [17920/50000]\tLoss: 3.9889\tLR: 0.010000\n",
            "Training Epoch: 19 [18048/50000]\tLoss: 3.9231\tLR: 0.010000\n",
            "Training Epoch: 19 [18176/50000]\tLoss: 4.0040\tLR: 0.010000\n",
            "Training Epoch: 19 [18304/50000]\tLoss: 3.9417\tLR: 0.010000\n",
            "Training Epoch: 19 [18432/50000]\tLoss: 3.9116\tLR: 0.010000\n",
            "Training Epoch: 19 [18560/50000]\tLoss: 3.6426\tLR: 0.010000\n",
            "Training Epoch: 19 [18688/50000]\tLoss: 3.9873\tLR: 0.010000\n",
            "Training Epoch: 19 [18816/50000]\tLoss: 3.9304\tLR: 0.010000\n",
            "Training Epoch: 19 [18944/50000]\tLoss: 3.9606\tLR: 0.010000\n",
            "Training Epoch: 19 [19072/50000]\tLoss: 3.8778\tLR: 0.010000\n",
            "Training Epoch: 19 [19200/50000]\tLoss: 4.0700\tLR: 0.010000\n",
            "Training Epoch: 19 [19328/50000]\tLoss: 4.0206\tLR: 0.010000\n",
            "Training Epoch: 19 [19456/50000]\tLoss: 3.9297\tLR: 0.010000\n",
            "Training Epoch: 19 [19584/50000]\tLoss: 4.0068\tLR: 0.010000\n",
            "Training Epoch: 19 [19712/50000]\tLoss: 3.8873\tLR: 0.010000\n",
            "Training Epoch: 19 [19840/50000]\tLoss: 4.0892\tLR: 0.010000\n",
            "Training Epoch: 19 [19968/50000]\tLoss: 4.0299\tLR: 0.010000\n",
            "Training Epoch: 19 [20096/50000]\tLoss: 4.0655\tLR: 0.010000\n",
            "Training Epoch: 19 [20224/50000]\tLoss: 3.8312\tLR: 0.010000\n",
            "Training Epoch: 19 [20352/50000]\tLoss: 3.8694\tLR: 0.010000\n",
            "Training Epoch: 19 [20480/50000]\tLoss: 3.8118\tLR: 0.010000\n",
            "Training Epoch: 19 [20608/50000]\tLoss: 3.8470\tLR: 0.010000\n",
            "Training Epoch: 19 [20736/50000]\tLoss: 3.9781\tLR: 0.010000\n",
            "Training Epoch: 19 [20864/50000]\tLoss: 3.8744\tLR: 0.010000\n",
            "Training Epoch: 19 [20992/50000]\tLoss: 3.6481\tLR: 0.010000\n",
            "Training Epoch: 19 [21120/50000]\tLoss: 4.1743\tLR: 0.010000\n",
            "Training Epoch: 19 [21248/50000]\tLoss: 4.0295\tLR: 0.010000\n",
            "Training Epoch: 19 [21376/50000]\tLoss: 3.9789\tLR: 0.010000\n",
            "Training Epoch: 19 [21504/50000]\tLoss: 4.0016\tLR: 0.010000\n",
            "Training Epoch: 19 [21632/50000]\tLoss: 3.9495\tLR: 0.010000\n",
            "Training Epoch: 19 [21760/50000]\tLoss: 3.8824\tLR: 0.010000\n",
            "Training Epoch: 19 [21888/50000]\tLoss: 3.9711\tLR: 0.010000\n",
            "Training Epoch: 19 [22016/50000]\tLoss: 3.9776\tLR: 0.010000\n",
            "Training Epoch: 19 [22144/50000]\tLoss: 3.9597\tLR: 0.010000\n",
            "Training Epoch: 19 [22272/50000]\tLoss: 4.0498\tLR: 0.010000\n",
            "Training Epoch: 19 [22400/50000]\tLoss: 3.9996\tLR: 0.010000\n",
            "Training Epoch: 19 [22528/50000]\tLoss: 3.9014\tLR: 0.010000\n",
            "Training Epoch: 19 [22656/50000]\tLoss: 4.0436\tLR: 0.010000\n",
            "Training Epoch: 19 [22784/50000]\tLoss: 4.0055\tLR: 0.010000\n",
            "Training Epoch: 19 [22912/50000]\tLoss: 3.9328\tLR: 0.010000\n",
            "Training Epoch: 19 [23040/50000]\tLoss: 4.0476\tLR: 0.010000\n",
            "Training Epoch: 19 [23168/50000]\tLoss: 3.8596\tLR: 0.010000\n",
            "Training Epoch: 19 [23296/50000]\tLoss: 3.9036\tLR: 0.010000\n",
            "Training Epoch: 19 [23424/50000]\tLoss: 4.0699\tLR: 0.010000\n",
            "Training Epoch: 19 [23552/50000]\tLoss: 3.9859\tLR: 0.010000\n",
            "Training Epoch: 19 [23680/50000]\tLoss: 4.0212\tLR: 0.010000\n",
            "Training Epoch: 19 [23808/50000]\tLoss: 3.9534\tLR: 0.010000\n",
            "Training Epoch: 19 [23936/50000]\tLoss: 3.8902\tLR: 0.010000\n",
            "Training Epoch: 19 [24064/50000]\tLoss: 3.8842\tLR: 0.010000\n",
            "Training Epoch: 19 [24192/50000]\tLoss: 3.9706\tLR: 0.010000\n",
            "Training Epoch: 19 [24320/50000]\tLoss: 3.9910\tLR: 0.010000\n",
            "Training Epoch: 19 [24448/50000]\tLoss: 3.8702\tLR: 0.010000\n",
            "Training Epoch: 19 [24576/50000]\tLoss: 3.9711\tLR: 0.010000\n",
            "Training Epoch: 19 [24704/50000]\tLoss: 3.9761\tLR: 0.010000\n",
            "Training Epoch: 19 [24832/50000]\tLoss: 3.7509\tLR: 0.010000\n",
            "Training Epoch: 19 [24960/50000]\tLoss: 3.8947\tLR: 0.010000\n",
            "Training Epoch: 19 [25088/50000]\tLoss: 3.8148\tLR: 0.010000\n",
            "Training Epoch: 19 [25216/50000]\tLoss: 3.8182\tLR: 0.010000\n",
            "Training Epoch: 19 [25344/50000]\tLoss: 3.7600\tLR: 0.010000\n",
            "Training Epoch: 19 [25472/50000]\tLoss: 3.8391\tLR: 0.010000\n",
            "Training Epoch: 19 [25600/50000]\tLoss: 3.9196\tLR: 0.010000\n",
            "Training Epoch: 19 [25728/50000]\tLoss: 4.0186\tLR: 0.010000\n",
            "Training Epoch: 19 [25856/50000]\tLoss: 4.2671\tLR: 0.010000\n",
            "Training Epoch: 19 [25984/50000]\tLoss: 4.0494\tLR: 0.010000\n",
            "Training Epoch: 19 [26112/50000]\tLoss: 3.8356\tLR: 0.010000\n",
            "Training Epoch: 19 [26240/50000]\tLoss: 4.1094\tLR: 0.010000\n",
            "Training Epoch: 19 [26368/50000]\tLoss: 3.9270\tLR: 0.010000\n",
            "Training Epoch: 19 [26496/50000]\tLoss: 3.8472\tLR: 0.010000\n",
            "Training Epoch: 19 [26624/50000]\tLoss: 3.9380\tLR: 0.010000\n",
            "Training Epoch: 19 [26752/50000]\tLoss: 3.8792\tLR: 0.010000\n",
            "Training Epoch: 19 [26880/50000]\tLoss: 3.9182\tLR: 0.010000\n",
            "Training Epoch: 19 [27008/50000]\tLoss: 3.9290\tLR: 0.010000\n",
            "Training Epoch: 19 [27136/50000]\tLoss: 3.8006\tLR: 0.010000\n",
            "Training Epoch: 19 [27264/50000]\tLoss: 3.9237\tLR: 0.010000\n",
            "Training Epoch: 19 [27392/50000]\tLoss: 4.1196\tLR: 0.010000\n",
            "Training Epoch: 19 [27520/50000]\tLoss: 3.8233\tLR: 0.010000\n",
            "Training Epoch: 19 [27648/50000]\tLoss: 4.1177\tLR: 0.010000\n",
            "Training Epoch: 19 [27776/50000]\tLoss: 3.9113\tLR: 0.010000\n",
            "Training Epoch: 19 [27904/50000]\tLoss: 3.9459\tLR: 0.010000\n",
            "Training Epoch: 19 [28032/50000]\tLoss: 3.9339\tLR: 0.010000\n",
            "Training Epoch: 19 [28160/50000]\tLoss: 3.7722\tLR: 0.010000\n",
            "Training Epoch: 19 [28288/50000]\tLoss: 3.9684\tLR: 0.010000\n",
            "Training Epoch: 19 [28416/50000]\tLoss: 4.0559\tLR: 0.010000\n",
            "Training Epoch: 19 [28544/50000]\tLoss: 3.8127\tLR: 0.010000\n",
            "Training Epoch: 19 [28672/50000]\tLoss: 4.1043\tLR: 0.010000\n",
            "Training Epoch: 19 [28800/50000]\tLoss: 3.9781\tLR: 0.010000\n",
            "Training Epoch: 19 [28928/50000]\tLoss: 4.2036\tLR: 0.010000\n",
            "Training Epoch: 19 [29056/50000]\tLoss: 3.9873\tLR: 0.010000\n",
            "Training Epoch: 19 [29184/50000]\tLoss: 4.0100\tLR: 0.010000\n",
            "Training Epoch: 19 [29312/50000]\tLoss: 4.0031\tLR: 0.010000\n",
            "Training Epoch: 19 [29440/50000]\tLoss: 3.8968\tLR: 0.010000\n",
            "Training Epoch: 19 [29568/50000]\tLoss: 3.9069\tLR: 0.010000\n",
            "Training Epoch: 19 [29696/50000]\tLoss: 3.8983\tLR: 0.010000\n",
            "Training Epoch: 19 [29824/50000]\tLoss: 3.9599\tLR: 0.010000\n",
            "Training Epoch: 19 [29952/50000]\tLoss: 3.9486\tLR: 0.010000\n",
            "Training Epoch: 19 [30080/50000]\tLoss: 3.9510\tLR: 0.010000\n",
            "Training Epoch: 19 [30208/50000]\tLoss: 3.8196\tLR: 0.010000\n",
            "Training Epoch: 19 [30336/50000]\tLoss: 4.0991\tLR: 0.010000\n",
            "Training Epoch: 19 [30464/50000]\tLoss: 3.8154\tLR: 0.010000\n",
            "Training Epoch: 19 [30592/50000]\tLoss: 3.9155\tLR: 0.010000\n",
            "Training Epoch: 19 [30720/50000]\tLoss: 3.8942\tLR: 0.010000\n",
            "Training Epoch: 19 [30848/50000]\tLoss: 3.9993\tLR: 0.010000\n",
            "Training Epoch: 19 [30976/50000]\tLoss: 3.8615\tLR: 0.010000\n",
            "Training Epoch: 19 [31104/50000]\tLoss: 4.0163\tLR: 0.010000\n",
            "Training Epoch: 19 [31232/50000]\tLoss: 3.9501\tLR: 0.010000\n",
            "Training Epoch: 19 [31360/50000]\tLoss: 4.1114\tLR: 0.010000\n",
            "Training Epoch: 19 [31488/50000]\tLoss: 3.8972\tLR: 0.010000\n",
            "Training Epoch: 19 [31616/50000]\tLoss: 3.8764\tLR: 0.010000\n",
            "Training Epoch: 19 [31744/50000]\tLoss: 3.9373\tLR: 0.010000\n",
            "Training Epoch: 19 [31872/50000]\tLoss: 4.1558\tLR: 0.010000\n",
            "Training Epoch: 19 [32000/50000]\tLoss: 3.8536\tLR: 0.010000\n",
            "Training Epoch: 19 [32128/50000]\tLoss: 3.8409\tLR: 0.010000\n",
            "Training Epoch: 19 [32256/50000]\tLoss: 3.9572\tLR: 0.010000\n",
            "Training Epoch: 19 [32384/50000]\tLoss: 3.9714\tLR: 0.010000\n",
            "Training Epoch: 19 [32512/50000]\tLoss: 3.9320\tLR: 0.010000\n",
            "Training Epoch: 19 [32640/50000]\tLoss: 4.0496\tLR: 0.010000\n",
            "Training Epoch: 19 [32768/50000]\tLoss: 4.1548\tLR: 0.010000\n",
            "Training Epoch: 19 [32896/50000]\tLoss: 3.8522\tLR: 0.010000\n",
            "Training Epoch: 19 [33024/50000]\tLoss: 4.0487\tLR: 0.010000\n",
            "Training Epoch: 19 [33152/50000]\tLoss: 3.9064\tLR: 0.010000\n",
            "Training Epoch: 19 [33280/50000]\tLoss: 3.8055\tLR: 0.010000\n",
            "Training Epoch: 19 [33408/50000]\tLoss: 3.8795\tLR: 0.010000\n",
            "Training Epoch: 19 [33536/50000]\tLoss: 3.9786\tLR: 0.010000\n",
            "Training Epoch: 19 [33664/50000]\tLoss: 4.0269\tLR: 0.010000\n",
            "Training Epoch: 19 [33792/50000]\tLoss: 3.9122\tLR: 0.010000\n",
            "Training Epoch: 19 [33920/50000]\tLoss: 3.9679\tLR: 0.010000\n",
            "Training Epoch: 19 [34048/50000]\tLoss: 3.8516\tLR: 0.010000\n",
            "Training Epoch: 19 [34176/50000]\tLoss: 3.9895\tLR: 0.010000\n",
            "Training Epoch: 19 [34304/50000]\tLoss: 3.9725\tLR: 0.010000\n",
            "Training Epoch: 19 [34432/50000]\tLoss: 4.0121\tLR: 0.010000\n",
            "Training Epoch: 19 [34560/50000]\tLoss: 4.0252\tLR: 0.010000\n",
            "Training Epoch: 19 [34688/50000]\tLoss: 3.9292\tLR: 0.010000\n",
            "Training Epoch: 19 [34816/50000]\tLoss: 4.0177\tLR: 0.010000\n",
            "Training Epoch: 19 [34944/50000]\tLoss: 4.1552\tLR: 0.010000\n",
            "Training Epoch: 19 [35072/50000]\tLoss: 3.9994\tLR: 0.010000\n",
            "Training Epoch: 19 [35200/50000]\tLoss: 3.8868\tLR: 0.010000\n",
            "Training Epoch: 19 [35328/50000]\tLoss: 3.7962\tLR: 0.010000\n",
            "Training Epoch: 19 [35456/50000]\tLoss: 3.8345\tLR: 0.010000\n",
            "Training Epoch: 19 [35584/50000]\tLoss: 4.1915\tLR: 0.010000\n",
            "Training Epoch: 19 [35712/50000]\tLoss: 4.1030\tLR: 0.010000\n",
            "Training Epoch: 19 [35840/50000]\tLoss: 3.9213\tLR: 0.010000\n",
            "Training Epoch: 19 [35968/50000]\tLoss: 4.1077\tLR: 0.010000\n",
            "Training Epoch: 19 [36096/50000]\tLoss: 4.1668\tLR: 0.010000\n",
            "Training Epoch: 19 [36224/50000]\tLoss: 3.9379\tLR: 0.010000\n",
            "Training Epoch: 19 [36352/50000]\tLoss: 4.0336\tLR: 0.010000\n",
            "Training Epoch: 19 [36480/50000]\tLoss: 3.8179\tLR: 0.010000\n",
            "Training Epoch: 19 [36608/50000]\tLoss: 3.8884\tLR: 0.010000\n",
            "Training Epoch: 19 [36736/50000]\tLoss: 3.8890\tLR: 0.010000\n",
            "Training Epoch: 19 [36864/50000]\tLoss: 4.1283\tLR: 0.010000\n",
            "Training Epoch: 19 [36992/50000]\tLoss: 3.8363\tLR: 0.010000\n",
            "Training Epoch: 19 [37120/50000]\tLoss: 4.1498\tLR: 0.010000\n",
            "Training Epoch: 19 [37248/50000]\tLoss: 3.9324\tLR: 0.010000\n",
            "Training Epoch: 19 [37376/50000]\tLoss: 4.0595\tLR: 0.010000\n",
            "Training Epoch: 19 [37504/50000]\tLoss: 3.8542\tLR: 0.010000\n",
            "Training Epoch: 19 [37632/50000]\tLoss: 3.8327\tLR: 0.010000\n",
            "Training Epoch: 19 [37760/50000]\tLoss: 4.0236\tLR: 0.010000\n",
            "Training Epoch: 19 [37888/50000]\tLoss: 4.1832\tLR: 0.010000\n",
            "Training Epoch: 19 [38016/50000]\tLoss: 3.9934\tLR: 0.010000\n",
            "Training Epoch: 19 [38144/50000]\tLoss: 3.8866\tLR: 0.010000\n",
            "Training Epoch: 19 [38272/50000]\tLoss: 3.8781\tLR: 0.010000\n",
            "Training Epoch: 19 [38400/50000]\tLoss: 3.9210\tLR: 0.010000\n",
            "Training Epoch: 19 [38528/50000]\tLoss: 3.9264\tLR: 0.010000\n",
            "Training Epoch: 19 [38656/50000]\tLoss: 3.9344\tLR: 0.010000\n",
            "Training Epoch: 19 [38784/50000]\tLoss: 3.9824\tLR: 0.010000\n",
            "Training Epoch: 19 [38912/50000]\tLoss: 3.8438\tLR: 0.010000\n",
            "Training Epoch: 19 [39040/50000]\tLoss: 4.0099\tLR: 0.010000\n",
            "Training Epoch: 19 [39168/50000]\tLoss: 4.0162\tLR: 0.010000\n",
            "Training Epoch: 19 [39296/50000]\tLoss: 3.8428\tLR: 0.010000\n",
            "Training Epoch: 19 [39424/50000]\tLoss: 3.8456\tLR: 0.010000\n",
            "Training Epoch: 19 [39552/50000]\tLoss: 3.8556\tLR: 0.010000\n",
            "Training Epoch: 19 [39680/50000]\tLoss: 3.9023\tLR: 0.010000\n",
            "Training Epoch: 19 [39808/50000]\tLoss: 4.0955\tLR: 0.010000\n",
            "Training Epoch: 19 [39936/50000]\tLoss: 4.0116\tLR: 0.010000\n",
            "Training Epoch: 19 [40064/50000]\tLoss: 4.0932\tLR: 0.010000\n",
            "Training Epoch: 19 [40192/50000]\tLoss: 3.9359\tLR: 0.010000\n",
            "Training Epoch: 19 [40320/50000]\tLoss: 3.8087\tLR: 0.010000\n",
            "Training Epoch: 19 [40448/50000]\tLoss: 3.9682\tLR: 0.010000\n",
            "Training Epoch: 19 [40576/50000]\tLoss: 3.6408\tLR: 0.010000\n",
            "Training Epoch: 19 [40704/50000]\tLoss: 3.8664\tLR: 0.010000\n",
            "Training Epoch: 19 [40832/50000]\tLoss: 3.8795\tLR: 0.010000\n",
            "Training Epoch: 19 [40960/50000]\tLoss: 3.9270\tLR: 0.010000\n",
            "Training Epoch: 19 [41088/50000]\tLoss: 4.0499\tLR: 0.010000\n",
            "Training Epoch: 19 [41216/50000]\tLoss: 4.1029\tLR: 0.010000\n",
            "Training Epoch: 19 [41344/50000]\tLoss: 3.9495\tLR: 0.010000\n",
            "Training Epoch: 19 [41472/50000]\tLoss: 3.8390\tLR: 0.010000\n",
            "Training Epoch: 19 [41600/50000]\tLoss: 3.8980\tLR: 0.010000\n",
            "Training Epoch: 19 [41728/50000]\tLoss: 3.9282\tLR: 0.010000\n",
            "Training Epoch: 19 [41856/50000]\tLoss: 4.1178\tLR: 0.010000\n",
            "Training Epoch: 19 [41984/50000]\tLoss: 3.8748\tLR: 0.010000\n",
            "Training Epoch: 19 [42112/50000]\tLoss: 3.9906\tLR: 0.010000\n",
            "Training Epoch: 19 [42240/50000]\tLoss: 3.9199\tLR: 0.010000\n",
            "Training Epoch: 19 [42368/50000]\tLoss: 4.0863\tLR: 0.010000\n",
            "Training Epoch: 19 [42496/50000]\tLoss: 4.1182\tLR: 0.010000\n",
            "Training Epoch: 19 [42624/50000]\tLoss: 3.9556\tLR: 0.010000\n",
            "Training Epoch: 19 [42752/50000]\tLoss: 4.0166\tLR: 0.010000\n",
            "Training Epoch: 19 [42880/50000]\tLoss: 3.7950\tLR: 0.010000\n",
            "Training Epoch: 19 [43008/50000]\tLoss: 3.9362\tLR: 0.010000\n",
            "Training Epoch: 19 [43136/50000]\tLoss: 3.9273\tLR: 0.010000\n",
            "Training Epoch: 19 [43264/50000]\tLoss: 3.8500\tLR: 0.010000\n",
            "Training Epoch: 19 [43392/50000]\tLoss: 3.9575\tLR: 0.010000\n",
            "Training Epoch: 19 [43520/50000]\tLoss: 4.0527\tLR: 0.010000\n",
            "Training Epoch: 19 [43648/50000]\tLoss: 4.1007\tLR: 0.010000\n",
            "Training Epoch: 19 [43776/50000]\tLoss: 4.0260\tLR: 0.010000\n",
            "Training Epoch: 19 [43904/50000]\tLoss: 3.9962\tLR: 0.010000\n",
            "Training Epoch: 19 [44032/50000]\tLoss: 4.0630\tLR: 0.010000\n",
            "Training Epoch: 19 [44160/50000]\tLoss: 4.0099\tLR: 0.010000\n",
            "Training Epoch: 19 [44288/50000]\tLoss: 4.1021\tLR: 0.010000\n",
            "Training Epoch: 19 [44416/50000]\tLoss: 3.9742\tLR: 0.010000\n",
            "Training Epoch: 19 [44544/50000]\tLoss: 4.0130\tLR: 0.010000\n",
            "Training Epoch: 19 [44672/50000]\tLoss: 3.8721\tLR: 0.010000\n",
            "Training Epoch: 19 [44800/50000]\tLoss: 3.7558\tLR: 0.010000\n",
            "Training Epoch: 19 [44928/50000]\tLoss: 4.0429\tLR: 0.010000\n",
            "Training Epoch: 19 [45056/50000]\tLoss: 3.7959\tLR: 0.010000\n",
            "Training Epoch: 19 [45184/50000]\tLoss: 3.9011\tLR: 0.010000\n",
            "Training Epoch: 19 [45312/50000]\tLoss: 3.9508\tLR: 0.010000\n",
            "Training Epoch: 19 [45440/50000]\tLoss: 4.0011\tLR: 0.010000\n",
            "Training Epoch: 19 [45568/50000]\tLoss: 3.8550\tLR: 0.010000\n",
            "Training Epoch: 19 [45696/50000]\tLoss: 3.8927\tLR: 0.010000\n",
            "Training Epoch: 19 [45824/50000]\tLoss: 3.7854\tLR: 0.010000\n",
            "Training Epoch: 19 [45952/50000]\tLoss: 3.9081\tLR: 0.010000\n",
            "Training Epoch: 19 [46080/50000]\tLoss: 3.8832\tLR: 0.010000\n",
            "Training Epoch: 19 [46208/50000]\tLoss: 3.9436\tLR: 0.010000\n",
            "Training Epoch: 19 [46336/50000]\tLoss: 4.0278\tLR: 0.010000\n",
            "Training Epoch: 19 [46464/50000]\tLoss: 4.1161\tLR: 0.010000\n",
            "Training Epoch: 19 [46592/50000]\tLoss: 3.7405\tLR: 0.010000\n",
            "Training Epoch: 19 [46720/50000]\tLoss: 3.8764\tLR: 0.010000\n",
            "Training Epoch: 19 [46848/50000]\tLoss: 4.0642\tLR: 0.010000\n",
            "Training Epoch: 19 [46976/50000]\tLoss: 4.0868\tLR: 0.010000\n",
            "Training Epoch: 19 [47104/50000]\tLoss: 3.8859\tLR: 0.010000\n",
            "Training Epoch: 19 [47232/50000]\tLoss: 3.8180\tLR: 0.010000\n",
            "Training Epoch: 19 [47360/50000]\tLoss: 3.9292\tLR: 0.010000\n",
            "Training Epoch: 19 [47488/50000]\tLoss: 3.8208\tLR: 0.010000\n",
            "Training Epoch: 19 [47616/50000]\tLoss: 4.0460\tLR: 0.010000\n",
            "Training Epoch: 19 [47744/50000]\tLoss: 3.9232\tLR: 0.010000\n",
            "Training Epoch: 19 [47872/50000]\tLoss: 3.9703\tLR: 0.010000\n",
            "Training Epoch: 19 [48000/50000]\tLoss: 4.0255\tLR: 0.010000\n",
            "Training Epoch: 19 [48128/50000]\tLoss: 3.8702\tLR: 0.010000\n",
            "Training Epoch: 19 [48256/50000]\tLoss: 3.8869\tLR: 0.010000\n",
            "Training Epoch: 19 [48384/50000]\tLoss: 3.9389\tLR: 0.010000\n",
            "Training Epoch: 19 [48512/50000]\tLoss: 4.0097\tLR: 0.010000\n",
            "Training Epoch: 19 [48640/50000]\tLoss: 4.0703\tLR: 0.010000\n",
            "Training Epoch: 19 [48768/50000]\tLoss: 4.0688\tLR: 0.010000\n",
            "Training Epoch: 19 [48896/50000]\tLoss: 4.0691\tLR: 0.010000\n",
            "Training Epoch: 19 [49024/50000]\tLoss: 4.0646\tLR: 0.010000\n",
            "Training Epoch: 19 [49152/50000]\tLoss: 3.8533\tLR: 0.010000\n",
            "Training Epoch: 19 [49280/50000]\tLoss: 4.0027\tLR: 0.010000\n",
            "Training Epoch: 19 [49408/50000]\tLoss: 4.0369\tLR: 0.010000\n",
            "Training Epoch: 19 [49536/50000]\tLoss: 3.7946\tLR: 0.010000\n",
            "Training Epoch: 19 [49664/50000]\tLoss: 4.0430\tLR: 0.010000\n",
            "Training Epoch: 19 [49792/50000]\tLoss: 4.0116\tLR: 0.010000\n",
            "Training Epoch: 19 [49920/50000]\tLoss: 3.8698\tLR: 0.010000\n",
            "Training Epoch: 19 [50000/50000]\tLoss: 4.0167\tLR: 0.010000\n",
            "epoch 19 training time consumed: 146.14s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 201676 GiB | 201676 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 199132 GiB | 199132 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2544 GiB |   2544 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 201676 GiB | 201676 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 199132 GiB | 199132 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2544 GiB |   2544 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 201124 GiB | 201124 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 198580 GiB | 198579 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2544 GiB |   2544 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 168464 GiB | 168463 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 165589 GiB | 165589 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   2874 GiB |   2874 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |   12288 K  |   12288 K  |\n",
            "|       from large pool |       5    |     146    |    5956 K  |    5956 K  |\n",
            "|       from small pool |     516    |     682    |    6332 K  |    6331 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |   12288 K  |   12288 K  |\n",
            "|       from large pool |       5    |     146    |    5956 K  |    5956 K  |\n",
            "|       from small pool |     516    |     682    |    6332 K  |    6331 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |     117    |    4570 K  |    4570 K  |\n",
            "|       from large pool |       4    |      46    |    2800 K  |    2800 K  |\n",
            "|       from small pool |      74    |      89    |    1769 K  |    1769 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0313, Accuracy: 0.0931, Time consumed:9.39s\n",
            "\n",
            "Training Epoch: 20 [128/50000]\tLoss: 4.0882\tLR: 0.010000\n",
            "Training Epoch: 20 [256/50000]\tLoss: 3.9641\tLR: 0.010000\n",
            "Training Epoch: 20 [384/50000]\tLoss: 4.0061\tLR: 0.010000\n",
            "Training Epoch: 20 [512/50000]\tLoss: 3.8115\tLR: 0.010000\n",
            "Training Epoch: 20 [640/50000]\tLoss: 4.2111\tLR: 0.010000\n",
            "Training Epoch: 20 [768/50000]\tLoss: 4.0516\tLR: 0.010000\n",
            "Training Epoch: 20 [896/50000]\tLoss: 3.9889\tLR: 0.010000\n",
            "Training Epoch: 20 [1024/50000]\tLoss: 4.0563\tLR: 0.010000\n",
            "Training Epoch: 20 [1152/50000]\tLoss: 4.1262\tLR: 0.010000\n",
            "Training Epoch: 20 [1280/50000]\tLoss: 3.9496\tLR: 0.010000\n",
            "Training Epoch: 20 [1408/50000]\tLoss: 3.8215\tLR: 0.010000\n",
            "Training Epoch: 20 [1536/50000]\tLoss: 3.9421\tLR: 0.010000\n",
            "Training Epoch: 20 [1664/50000]\tLoss: 3.9108\tLR: 0.010000\n",
            "Training Epoch: 20 [1792/50000]\tLoss: 3.9018\tLR: 0.010000\n",
            "Training Epoch: 20 [1920/50000]\tLoss: 3.9342\tLR: 0.010000\n",
            "Training Epoch: 20 [2048/50000]\tLoss: 3.9570\tLR: 0.010000\n",
            "Training Epoch: 20 [2176/50000]\tLoss: 3.8840\tLR: 0.010000\n",
            "Training Epoch: 20 [2304/50000]\tLoss: 3.8841\tLR: 0.010000\n",
            "Training Epoch: 20 [2432/50000]\tLoss: 3.9370\tLR: 0.010000\n",
            "Training Epoch: 20 [2560/50000]\tLoss: 4.0547\tLR: 0.010000\n",
            "Training Epoch: 20 [2688/50000]\tLoss: 4.1199\tLR: 0.010000\n",
            "Training Epoch: 20 [2816/50000]\tLoss: 3.7977\tLR: 0.010000\n",
            "Training Epoch: 20 [2944/50000]\tLoss: 3.8630\tLR: 0.010000\n",
            "Training Epoch: 20 [3072/50000]\tLoss: 4.0602\tLR: 0.010000\n",
            "Training Epoch: 20 [3200/50000]\tLoss: 3.9171\tLR: 0.010000\n",
            "Training Epoch: 20 [3328/50000]\tLoss: 3.7995\tLR: 0.010000\n",
            "Training Epoch: 20 [3456/50000]\tLoss: 3.9224\tLR: 0.010000\n",
            "Training Epoch: 20 [3584/50000]\tLoss: 4.1461\tLR: 0.010000\n",
            "Training Epoch: 20 [3712/50000]\tLoss: 3.9446\tLR: 0.010000\n",
            "Training Epoch: 20 [3840/50000]\tLoss: 3.9016\tLR: 0.010000\n",
            "Training Epoch: 20 [3968/50000]\tLoss: 3.9828\tLR: 0.010000\n",
            "Training Epoch: 20 [4096/50000]\tLoss: 3.9411\tLR: 0.010000\n",
            "Training Epoch: 20 [4224/50000]\tLoss: 3.9981\tLR: 0.010000\n",
            "Training Epoch: 20 [4352/50000]\tLoss: 4.0015\tLR: 0.010000\n",
            "Training Epoch: 20 [4480/50000]\tLoss: 4.0006\tLR: 0.010000\n",
            "Training Epoch: 20 [4608/50000]\tLoss: 3.8154\tLR: 0.010000\n",
            "Training Epoch: 20 [4736/50000]\tLoss: 3.7163\tLR: 0.010000\n",
            "Training Epoch: 20 [4864/50000]\tLoss: 3.9154\tLR: 0.010000\n",
            "Training Epoch: 20 [4992/50000]\tLoss: 3.9154\tLR: 0.010000\n",
            "Training Epoch: 20 [5120/50000]\tLoss: 3.8696\tLR: 0.010000\n",
            "Training Epoch: 20 [5248/50000]\tLoss: 3.9730\tLR: 0.010000\n",
            "Training Epoch: 20 [5376/50000]\tLoss: 3.8006\tLR: 0.010000\n",
            "Training Epoch: 20 [5504/50000]\tLoss: 3.9984\tLR: 0.010000\n",
            "Training Epoch: 20 [5632/50000]\tLoss: 3.7230\tLR: 0.010000\n",
            "Training Epoch: 20 [5760/50000]\tLoss: 4.0355\tLR: 0.010000\n",
            "Training Epoch: 20 [5888/50000]\tLoss: 3.9055\tLR: 0.010000\n",
            "Training Epoch: 20 [6016/50000]\tLoss: 3.9254\tLR: 0.010000\n",
            "Training Epoch: 20 [6144/50000]\tLoss: 4.0443\tLR: 0.010000\n",
            "Training Epoch: 20 [6272/50000]\tLoss: 3.9461\tLR: 0.010000\n",
            "Training Epoch: 20 [6400/50000]\tLoss: 3.9429\tLR: 0.010000\n",
            "Training Epoch: 20 [6528/50000]\tLoss: 3.9788\tLR: 0.010000\n",
            "Training Epoch: 20 [6656/50000]\tLoss: 3.8790\tLR: 0.010000\n",
            "Training Epoch: 20 [6784/50000]\tLoss: 3.9084\tLR: 0.010000\n",
            "Training Epoch: 20 [6912/50000]\tLoss: 3.9587\tLR: 0.010000\n",
            "Training Epoch: 20 [7040/50000]\tLoss: 3.8734\tLR: 0.010000\n",
            "Training Epoch: 20 [7168/50000]\tLoss: 4.0133\tLR: 0.010000\n",
            "Training Epoch: 20 [7296/50000]\tLoss: 4.0771\tLR: 0.010000\n",
            "Training Epoch: 20 [7424/50000]\tLoss: 4.0068\tLR: 0.010000\n",
            "Training Epoch: 20 [7552/50000]\tLoss: 3.7197\tLR: 0.010000\n",
            "Training Epoch: 20 [7680/50000]\tLoss: 3.9161\tLR: 0.010000\n",
            "Training Epoch: 20 [7808/50000]\tLoss: 3.9500\tLR: 0.010000\n",
            "Training Epoch: 20 [7936/50000]\tLoss: 3.7959\tLR: 0.010000\n",
            "Training Epoch: 20 [8064/50000]\tLoss: 3.9839\tLR: 0.010000\n",
            "Training Epoch: 20 [8192/50000]\tLoss: 3.9320\tLR: 0.010000\n",
            "Training Epoch: 20 [8320/50000]\tLoss: 3.9448\tLR: 0.010000\n",
            "Training Epoch: 20 [8448/50000]\tLoss: 3.8042\tLR: 0.010000\n",
            "Training Epoch: 20 [8576/50000]\tLoss: 4.1250\tLR: 0.010000\n",
            "Training Epoch: 20 [8704/50000]\tLoss: 3.9326\tLR: 0.010000\n",
            "Training Epoch: 20 [8832/50000]\tLoss: 3.8572\tLR: 0.010000\n",
            "Training Epoch: 20 [8960/50000]\tLoss: 4.0236\tLR: 0.010000\n",
            "Training Epoch: 20 [9088/50000]\tLoss: 4.0364\tLR: 0.010000\n",
            "Training Epoch: 20 [9216/50000]\tLoss: 3.8051\tLR: 0.010000\n",
            "Training Epoch: 20 [9344/50000]\tLoss: 3.8731\tLR: 0.010000\n",
            "Training Epoch: 20 [9472/50000]\tLoss: 3.8721\tLR: 0.010000\n",
            "Training Epoch: 20 [9600/50000]\tLoss: 3.9191\tLR: 0.010000\n",
            "Training Epoch: 20 [9728/50000]\tLoss: 3.9843\tLR: 0.010000\n",
            "Training Epoch: 20 [9856/50000]\tLoss: 3.8649\tLR: 0.010000\n",
            "Training Epoch: 20 [9984/50000]\tLoss: 3.9783\tLR: 0.010000\n",
            "Training Epoch: 20 [10112/50000]\tLoss: 3.8124\tLR: 0.010000\n",
            "Training Epoch: 20 [10240/50000]\tLoss: 3.7359\tLR: 0.010000\n",
            "Training Epoch: 20 [10368/50000]\tLoss: 3.8584\tLR: 0.010000\n",
            "Training Epoch: 20 [10496/50000]\tLoss: 3.9169\tLR: 0.010000\n",
            "Training Epoch: 20 [10624/50000]\tLoss: 3.7801\tLR: 0.010000\n",
            "Training Epoch: 20 [10752/50000]\tLoss: 3.8302\tLR: 0.010000\n",
            "Training Epoch: 20 [10880/50000]\tLoss: 3.8847\tLR: 0.010000\n",
            "Training Epoch: 20 [11008/50000]\tLoss: 3.8947\tLR: 0.010000\n",
            "Training Epoch: 20 [11136/50000]\tLoss: 3.8803\tLR: 0.010000\n",
            "Training Epoch: 20 [11264/50000]\tLoss: 3.7070\tLR: 0.010000\n",
            "Training Epoch: 20 [11392/50000]\tLoss: 3.9093\tLR: 0.010000\n",
            "Training Epoch: 20 [11520/50000]\tLoss: 3.8808\tLR: 0.010000\n",
            "Training Epoch: 20 [11648/50000]\tLoss: 3.8705\tLR: 0.010000\n",
            "Training Epoch: 20 [11776/50000]\tLoss: 3.8588\tLR: 0.010000\n",
            "Training Epoch: 20 [11904/50000]\tLoss: 3.7253\tLR: 0.010000\n",
            "Training Epoch: 20 [12032/50000]\tLoss: 4.0639\tLR: 0.010000\n",
            "Training Epoch: 20 [12160/50000]\tLoss: 3.9923\tLR: 0.010000\n",
            "Training Epoch: 20 [12288/50000]\tLoss: 3.8894\tLR: 0.010000\n",
            "Training Epoch: 20 [12416/50000]\tLoss: 4.1698\tLR: 0.010000\n",
            "Training Epoch: 20 [12544/50000]\tLoss: 4.0603\tLR: 0.010000\n",
            "Training Epoch: 20 [12672/50000]\tLoss: 4.0811\tLR: 0.010000\n",
            "Training Epoch: 20 [12800/50000]\tLoss: 3.9048\tLR: 0.010000\n",
            "Training Epoch: 20 [12928/50000]\tLoss: 3.8826\tLR: 0.010000\n",
            "Training Epoch: 20 [13056/50000]\tLoss: 4.2079\tLR: 0.010000\n",
            "Training Epoch: 20 [13184/50000]\tLoss: 3.9183\tLR: 0.010000\n",
            "Training Epoch: 20 [13312/50000]\tLoss: 3.8992\tLR: 0.010000\n",
            "Training Epoch: 20 [13440/50000]\tLoss: 4.1064\tLR: 0.010000\n",
            "Training Epoch: 20 [13568/50000]\tLoss: 4.0673\tLR: 0.010000\n",
            "Training Epoch: 20 [13696/50000]\tLoss: 3.9724\tLR: 0.010000\n",
            "Training Epoch: 20 [13824/50000]\tLoss: 4.0193\tLR: 0.010000\n",
            "Training Epoch: 20 [13952/50000]\tLoss: 3.8664\tLR: 0.010000\n",
            "Training Epoch: 20 [14080/50000]\tLoss: 3.7879\tLR: 0.010000\n",
            "Training Epoch: 20 [14208/50000]\tLoss: 4.0143\tLR: 0.010000\n",
            "Training Epoch: 20 [14336/50000]\tLoss: 4.0705\tLR: 0.010000\n",
            "Training Epoch: 20 [14464/50000]\tLoss: 3.9515\tLR: 0.010000\n",
            "Training Epoch: 20 [14592/50000]\tLoss: 4.0075\tLR: 0.010000\n",
            "Training Epoch: 20 [14720/50000]\tLoss: 3.9035\tLR: 0.010000\n",
            "Training Epoch: 20 [14848/50000]\tLoss: 4.0056\tLR: 0.010000\n",
            "Training Epoch: 20 [14976/50000]\tLoss: 4.1150\tLR: 0.010000\n",
            "Training Epoch: 20 [15104/50000]\tLoss: 3.7591\tLR: 0.010000\n",
            "Training Epoch: 20 [15232/50000]\tLoss: 3.8290\tLR: 0.010000\n",
            "Training Epoch: 20 [15360/50000]\tLoss: 3.9136\tLR: 0.010000\n",
            "Training Epoch: 20 [15488/50000]\tLoss: 3.9523\tLR: 0.010000\n",
            "Training Epoch: 20 [15616/50000]\tLoss: 4.1807\tLR: 0.010000\n",
            "Training Epoch: 20 [15744/50000]\tLoss: 3.9292\tLR: 0.010000\n",
            "Training Epoch: 20 [15872/50000]\tLoss: 4.0118\tLR: 0.010000\n",
            "Training Epoch: 20 [16000/50000]\tLoss: 3.9811\tLR: 0.010000\n",
            "Training Epoch: 20 [16128/50000]\tLoss: 3.9861\tLR: 0.010000\n",
            "Training Epoch: 20 [16256/50000]\tLoss: 4.0388\tLR: 0.010000\n",
            "Training Epoch: 20 [16384/50000]\tLoss: 3.9340\tLR: 0.010000\n",
            "Training Epoch: 20 [16512/50000]\tLoss: 3.8856\tLR: 0.010000\n",
            "Training Epoch: 20 [16640/50000]\tLoss: 3.8551\tLR: 0.010000\n",
            "Training Epoch: 20 [16768/50000]\tLoss: 3.8868\tLR: 0.010000\n",
            "Training Epoch: 20 [16896/50000]\tLoss: 3.7422\tLR: 0.010000\n",
            "Training Epoch: 20 [17024/50000]\tLoss: 4.0001\tLR: 0.010000\n",
            "Training Epoch: 20 [17152/50000]\tLoss: 3.8630\tLR: 0.010000\n",
            "Training Epoch: 20 [17280/50000]\tLoss: 3.9693\tLR: 0.010000\n",
            "Training Epoch: 20 [17408/50000]\tLoss: 3.9137\tLR: 0.010000\n",
            "Training Epoch: 20 [17536/50000]\tLoss: 3.8401\tLR: 0.010000\n",
            "Training Epoch: 20 [17664/50000]\tLoss: 4.0308\tLR: 0.010000\n",
            "Training Epoch: 20 [17792/50000]\tLoss: 4.0894\tLR: 0.010000\n",
            "Training Epoch: 20 [17920/50000]\tLoss: 3.9802\tLR: 0.010000\n",
            "Training Epoch: 20 [18048/50000]\tLoss: 3.9820\tLR: 0.010000\n",
            "Training Epoch: 20 [18176/50000]\tLoss: 4.1867\tLR: 0.010000\n",
            "Training Epoch: 20 [18304/50000]\tLoss: 3.8677\tLR: 0.010000\n",
            "Training Epoch: 20 [18432/50000]\tLoss: 3.9089\tLR: 0.010000\n",
            "Training Epoch: 20 [18560/50000]\tLoss: 3.9434\tLR: 0.010000\n",
            "Training Epoch: 20 [18688/50000]\tLoss: 3.9669\tLR: 0.010000\n",
            "Training Epoch: 20 [18816/50000]\tLoss: 3.7737\tLR: 0.010000\n",
            "Training Epoch: 20 [18944/50000]\tLoss: 3.7976\tLR: 0.010000\n",
            "Training Epoch: 20 [19072/50000]\tLoss: 3.8522\tLR: 0.010000\n",
            "Training Epoch: 20 [19200/50000]\tLoss: 4.0740\tLR: 0.010000\n",
            "Training Epoch: 20 [19328/50000]\tLoss: 3.8578\tLR: 0.010000\n",
            "Training Epoch: 20 [19456/50000]\tLoss: 4.2319\tLR: 0.010000\n",
            "Training Epoch: 20 [19584/50000]\tLoss: 4.0450\tLR: 0.010000\n",
            "Training Epoch: 20 [19712/50000]\tLoss: 3.8579\tLR: 0.010000\n",
            "Training Epoch: 20 [19840/50000]\tLoss: 3.8542\tLR: 0.010000\n",
            "Training Epoch: 20 [19968/50000]\tLoss: 3.9640\tLR: 0.010000\n",
            "Training Epoch: 20 [20096/50000]\tLoss: 3.9478\tLR: 0.010000\n",
            "Training Epoch: 20 [20224/50000]\tLoss: 3.8560\tLR: 0.010000\n",
            "Training Epoch: 20 [20352/50000]\tLoss: 3.9587\tLR: 0.010000\n",
            "Training Epoch: 20 [20480/50000]\tLoss: 3.9005\tLR: 0.010000\n",
            "Training Epoch: 20 [20608/50000]\tLoss: 3.8644\tLR: 0.010000\n",
            "Training Epoch: 20 [20736/50000]\tLoss: 3.8897\tLR: 0.010000\n",
            "Training Epoch: 20 [20864/50000]\tLoss: 4.0095\tLR: 0.010000\n",
            "Training Epoch: 20 [20992/50000]\tLoss: 3.8111\tLR: 0.010000\n",
            "Training Epoch: 20 [21120/50000]\tLoss: 3.8268\tLR: 0.010000\n",
            "Training Epoch: 20 [21248/50000]\tLoss: 3.7784\tLR: 0.010000\n",
            "Training Epoch: 20 [21376/50000]\tLoss: 3.6780\tLR: 0.010000\n",
            "Training Epoch: 20 [21504/50000]\tLoss: 3.9028\tLR: 0.010000\n",
            "Training Epoch: 20 [21632/50000]\tLoss: 4.0219\tLR: 0.010000\n",
            "Training Epoch: 20 [21760/50000]\tLoss: 3.8410\tLR: 0.010000\n",
            "Training Epoch: 20 [21888/50000]\tLoss: 3.8476\tLR: 0.010000\n",
            "Training Epoch: 20 [22016/50000]\tLoss: 3.7847\tLR: 0.010000\n",
            "Training Epoch: 20 [22144/50000]\tLoss: 3.9376\tLR: 0.010000\n",
            "Training Epoch: 20 [22272/50000]\tLoss: 3.9043\tLR: 0.010000\n",
            "Training Epoch: 20 [22400/50000]\tLoss: 3.8512\tLR: 0.010000\n",
            "Training Epoch: 20 [22528/50000]\tLoss: 4.0538\tLR: 0.010000\n",
            "Training Epoch: 20 [22656/50000]\tLoss: 3.9380\tLR: 0.010000\n",
            "Training Epoch: 20 [22784/50000]\tLoss: 3.7100\tLR: 0.010000\n",
            "Training Epoch: 20 [22912/50000]\tLoss: 3.8972\tLR: 0.010000\n",
            "Training Epoch: 20 [23040/50000]\tLoss: 3.8264\tLR: 0.010000\n",
            "Training Epoch: 20 [23168/50000]\tLoss: 3.9351\tLR: 0.010000\n",
            "Training Epoch: 20 [23296/50000]\tLoss: 3.9876\tLR: 0.010000\n",
            "Training Epoch: 20 [23424/50000]\tLoss: 4.1313\tLR: 0.010000\n",
            "Training Epoch: 20 [23552/50000]\tLoss: 3.9281\tLR: 0.010000\n",
            "Training Epoch: 20 [23680/50000]\tLoss: 3.9016\tLR: 0.010000\n",
            "Training Epoch: 20 [23808/50000]\tLoss: 3.9994\tLR: 0.010000\n",
            "Training Epoch: 20 [23936/50000]\tLoss: 4.0066\tLR: 0.010000\n",
            "Training Epoch: 20 [24064/50000]\tLoss: 3.9775\tLR: 0.010000\n",
            "Training Epoch: 20 [24192/50000]\tLoss: 3.9144\tLR: 0.010000\n",
            "Training Epoch: 20 [24320/50000]\tLoss: 3.8744\tLR: 0.010000\n",
            "Training Epoch: 20 [24448/50000]\tLoss: 3.7203\tLR: 0.010000\n",
            "Training Epoch: 20 [24576/50000]\tLoss: 3.9232\tLR: 0.010000\n",
            "Training Epoch: 20 [24704/50000]\tLoss: 3.9910\tLR: 0.010000\n",
            "Training Epoch: 20 [24832/50000]\tLoss: 4.0383\tLR: 0.010000\n",
            "Training Epoch: 20 [24960/50000]\tLoss: 3.9461\tLR: 0.010000\n",
            "Training Epoch: 20 [25088/50000]\tLoss: 3.7340\tLR: 0.010000\n",
            "Training Epoch: 20 [25216/50000]\tLoss: 3.8723\tLR: 0.010000\n",
            "Training Epoch: 20 [25344/50000]\tLoss: 3.9550\tLR: 0.010000\n",
            "Training Epoch: 20 [25472/50000]\tLoss: 4.0248\tLR: 0.010000\n",
            "Training Epoch: 20 [25600/50000]\tLoss: 3.8340\tLR: 0.010000\n",
            "Training Epoch: 20 [25728/50000]\tLoss: 3.9002\tLR: 0.010000\n",
            "Training Epoch: 20 [25856/50000]\tLoss: 3.8552\tLR: 0.010000\n",
            "Training Epoch: 20 [25984/50000]\tLoss: 4.0607\tLR: 0.010000\n",
            "Training Epoch: 20 [26112/50000]\tLoss: 4.0208\tLR: 0.010000\n",
            "Training Epoch: 20 [26240/50000]\tLoss: 3.8941\tLR: 0.010000\n",
            "Training Epoch: 20 [26368/50000]\tLoss: 3.9316\tLR: 0.010000\n",
            "Training Epoch: 20 [26496/50000]\tLoss: 3.9884\tLR: 0.010000\n",
            "Training Epoch: 20 [26624/50000]\tLoss: 3.9532\tLR: 0.010000\n",
            "Training Epoch: 20 [26752/50000]\tLoss: 3.9844\tLR: 0.010000\n",
            "Training Epoch: 20 [26880/50000]\tLoss: 4.1408\tLR: 0.010000\n",
            "Training Epoch: 20 [27008/50000]\tLoss: 3.8960\tLR: 0.010000\n",
            "Training Epoch: 20 [27136/50000]\tLoss: 3.8314\tLR: 0.010000\n",
            "Training Epoch: 20 [27264/50000]\tLoss: 3.9616\tLR: 0.010000\n",
            "Training Epoch: 20 [27392/50000]\tLoss: 4.0663\tLR: 0.010000\n",
            "Training Epoch: 20 [27520/50000]\tLoss: 3.7363\tLR: 0.010000\n",
            "Training Epoch: 20 [27648/50000]\tLoss: 3.9831\tLR: 0.010000\n",
            "Training Epoch: 20 [27776/50000]\tLoss: 4.0096\tLR: 0.010000\n",
            "Training Epoch: 20 [27904/50000]\tLoss: 3.7690\tLR: 0.010000\n",
            "Training Epoch: 20 [28032/50000]\tLoss: 3.8420\tLR: 0.010000\n",
            "Training Epoch: 20 [28160/50000]\tLoss: 3.8536\tLR: 0.010000\n",
            "Training Epoch: 20 [28288/50000]\tLoss: 4.0084\tLR: 0.010000\n",
            "Training Epoch: 20 [28416/50000]\tLoss: 3.9247\tLR: 0.010000\n",
            "Training Epoch: 20 [28544/50000]\tLoss: 4.0485\tLR: 0.010000\n",
            "Training Epoch: 20 [28672/50000]\tLoss: 4.0698\tLR: 0.010000\n",
            "Training Epoch: 20 [28800/50000]\tLoss: 3.8143\tLR: 0.010000\n",
            "Training Epoch: 20 [28928/50000]\tLoss: 3.9585\tLR: 0.010000\n",
            "Training Epoch: 20 [29056/50000]\tLoss: 3.9772\tLR: 0.010000\n",
            "Training Epoch: 20 [29184/50000]\tLoss: 3.7708\tLR: 0.010000\n",
            "Training Epoch: 20 [29312/50000]\tLoss: 4.1063\tLR: 0.010000\n",
            "Training Epoch: 20 [29440/50000]\tLoss: 3.9980\tLR: 0.010000\n",
            "Training Epoch: 20 [29568/50000]\tLoss: 3.8887\tLR: 0.010000\n",
            "Training Epoch: 20 [29696/50000]\tLoss: 4.0623\tLR: 0.010000\n",
            "Training Epoch: 20 [29824/50000]\tLoss: 3.8453\tLR: 0.010000\n",
            "Training Epoch: 20 [29952/50000]\tLoss: 3.8157\tLR: 0.010000\n",
            "Training Epoch: 20 [30080/50000]\tLoss: 3.9725\tLR: 0.010000\n",
            "Training Epoch: 20 [30208/50000]\tLoss: 3.6872\tLR: 0.010000\n",
            "Training Epoch: 20 [30336/50000]\tLoss: 3.8536\tLR: 0.010000\n",
            "Training Epoch: 20 [30464/50000]\tLoss: 3.7903\tLR: 0.010000\n",
            "Training Epoch: 20 [30592/50000]\tLoss: 3.9840\tLR: 0.010000\n",
            "Training Epoch: 20 [30720/50000]\tLoss: 3.9668\tLR: 0.010000\n",
            "Training Epoch: 20 [30848/50000]\tLoss: 3.9385\tLR: 0.010000\n",
            "Training Epoch: 20 [30976/50000]\tLoss: 3.9597\tLR: 0.010000\n",
            "Training Epoch: 20 [31104/50000]\tLoss: 3.8322\tLR: 0.010000\n",
            "Training Epoch: 20 [31232/50000]\tLoss: 4.0159\tLR: 0.010000\n",
            "Training Epoch: 20 [31360/50000]\tLoss: 3.9574\tLR: 0.010000\n",
            "Training Epoch: 20 [31488/50000]\tLoss: 3.9754\tLR: 0.010000\n",
            "Training Epoch: 20 [31616/50000]\tLoss: 3.9431\tLR: 0.010000\n",
            "Training Epoch: 20 [31744/50000]\tLoss: 3.8607\tLR: 0.010000\n",
            "Training Epoch: 20 [31872/50000]\tLoss: 3.6102\tLR: 0.010000\n",
            "Training Epoch: 20 [32000/50000]\tLoss: 3.9034\tLR: 0.010000\n",
            "Training Epoch: 20 [32128/50000]\tLoss: 3.9855\tLR: 0.010000\n",
            "Training Epoch: 20 [32256/50000]\tLoss: 4.1457\tLR: 0.010000\n",
            "Training Epoch: 20 [32384/50000]\tLoss: 3.8592\tLR: 0.010000\n",
            "Training Epoch: 20 [32512/50000]\tLoss: 3.8304\tLR: 0.010000\n",
            "Training Epoch: 20 [32640/50000]\tLoss: 3.7138\tLR: 0.010000\n",
            "Training Epoch: 20 [32768/50000]\tLoss: 3.8951\tLR: 0.010000\n",
            "Training Epoch: 20 [32896/50000]\tLoss: 4.0125\tLR: 0.010000\n",
            "Training Epoch: 20 [33024/50000]\tLoss: 3.8392\tLR: 0.010000\n",
            "Training Epoch: 20 [33152/50000]\tLoss: 3.8457\tLR: 0.010000\n",
            "Training Epoch: 20 [33280/50000]\tLoss: 4.0033\tLR: 0.010000\n",
            "Training Epoch: 20 [33408/50000]\tLoss: 3.8966\tLR: 0.010000\n",
            "Training Epoch: 20 [33536/50000]\tLoss: 4.0169\tLR: 0.010000\n",
            "Training Epoch: 20 [33664/50000]\tLoss: 3.9142\tLR: 0.010000\n",
            "Training Epoch: 20 [33792/50000]\tLoss: 3.7993\tLR: 0.010000\n",
            "Training Epoch: 20 [33920/50000]\tLoss: 3.8728\tLR: 0.010000\n",
            "Training Epoch: 20 [34048/50000]\tLoss: 3.8073\tLR: 0.010000\n",
            "Training Epoch: 20 [34176/50000]\tLoss: 3.8942\tLR: 0.010000\n",
            "Training Epoch: 20 [34304/50000]\tLoss: 3.9015\tLR: 0.010000\n",
            "Training Epoch: 20 [34432/50000]\tLoss: 4.0569\tLR: 0.010000\n",
            "Training Epoch: 20 [34560/50000]\tLoss: 3.9452\tLR: 0.010000\n",
            "Training Epoch: 20 [34688/50000]\tLoss: 4.0000\tLR: 0.010000\n",
            "Training Epoch: 20 [34816/50000]\tLoss: 4.0030\tLR: 0.010000\n",
            "Training Epoch: 20 [34944/50000]\tLoss: 3.7098\tLR: 0.010000\n",
            "Training Epoch: 20 [35072/50000]\tLoss: 3.9509\tLR: 0.010000\n",
            "Training Epoch: 20 [35200/50000]\tLoss: 3.8785\tLR: 0.010000\n",
            "Training Epoch: 20 [35328/50000]\tLoss: 3.9115\tLR: 0.010000\n",
            "Training Epoch: 20 [35456/50000]\tLoss: 4.0513\tLR: 0.010000\n",
            "Training Epoch: 20 [35584/50000]\tLoss: 4.0088\tLR: 0.010000\n",
            "Training Epoch: 20 [35712/50000]\tLoss: 3.8663\tLR: 0.010000\n",
            "Training Epoch: 20 [35840/50000]\tLoss: 3.9108\tLR: 0.010000\n",
            "Training Epoch: 20 [35968/50000]\tLoss: 3.6555\tLR: 0.010000\n",
            "Training Epoch: 20 [36096/50000]\tLoss: 3.9610\tLR: 0.010000\n",
            "Training Epoch: 20 [36224/50000]\tLoss: 4.0731\tLR: 0.010000\n",
            "Training Epoch: 20 [36352/50000]\tLoss: 3.7983\tLR: 0.010000\n",
            "Training Epoch: 20 [36480/50000]\tLoss: 3.9253\tLR: 0.010000\n",
            "Training Epoch: 20 [36608/50000]\tLoss: 4.0882\tLR: 0.010000\n",
            "Training Epoch: 20 [36736/50000]\tLoss: 4.1448\tLR: 0.010000\n",
            "Training Epoch: 20 [36864/50000]\tLoss: 3.8850\tLR: 0.010000\n",
            "Training Epoch: 20 [36992/50000]\tLoss: 3.9641\tLR: 0.010000\n",
            "Training Epoch: 20 [37120/50000]\tLoss: 3.8101\tLR: 0.010000\n",
            "Training Epoch: 20 [37248/50000]\tLoss: 3.7797\tLR: 0.010000\n",
            "Training Epoch: 20 [37376/50000]\tLoss: 3.9798\tLR: 0.010000\n",
            "Training Epoch: 20 [37504/50000]\tLoss: 3.9367\tLR: 0.010000\n",
            "Training Epoch: 20 [37632/50000]\tLoss: 3.9971\tLR: 0.010000\n",
            "Training Epoch: 20 [37760/50000]\tLoss: 3.8847\tLR: 0.010000\n",
            "Training Epoch: 20 [37888/50000]\tLoss: 3.8901\tLR: 0.010000\n",
            "Training Epoch: 20 [38016/50000]\tLoss: 3.9803\tLR: 0.010000\n",
            "Training Epoch: 20 [38144/50000]\tLoss: 4.0464\tLR: 0.010000\n",
            "Training Epoch: 20 [38272/50000]\tLoss: 3.9182\tLR: 0.010000\n",
            "Training Epoch: 20 [38400/50000]\tLoss: 4.0185\tLR: 0.010000\n",
            "Training Epoch: 20 [38528/50000]\tLoss: 3.8530\tLR: 0.010000\n",
            "Training Epoch: 20 [38656/50000]\tLoss: 3.7780\tLR: 0.010000\n",
            "Training Epoch: 20 [38784/50000]\tLoss: 4.0864\tLR: 0.010000\n",
            "Training Epoch: 20 [38912/50000]\tLoss: 3.8635\tLR: 0.010000\n",
            "Training Epoch: 20 [39040/50000]\tLoss: 3.7398\tLR: 0.010000\n",
            "Training Epoch: 20 [39168/50000]\tLoss: 3.9089\tLR: 0.010000\n",
            "Training Epoch: 20 [39296/50000]\tLoss: 3.9184\tLR: 0.010000\n",
            "Training Epoch: 20 [39424/50000]\tLoss: 4.0173\tLR: 0.010000\n",
            "Training Epoch: 20 [39552/50000]\tLoss: 3.9344\tLR: 0.010000\n",
            "Training Epoch: 20 [39680/50000]\tLoss: 3.8582\tLR: 0.010000\n",
            "Training Epoch: 20 [39808/50000]\tLoss: 4.0156\tLR: 0.010000\n",
            "Training Epoch: 20 [39936/50000]\tLoss: 3.8531\tLR: 0.010000\n",
            "Training Epoch: 20 [40064/50000]\tLoss: 3.9335\tLR: 0.010000\n",
            "Training Epoch: 20 [40192/50000]\tLoss: 3.8313\tLR: 0.010000\n",
            "Training Epoch: 20 [40320/50000]\tLoss: 3.8294\tLR: 0.010000\n",
            "Training Epoch: 20 [40448/50000]\tLoss: 4.0689\tLR: 0.010000\n",
            "Training Epoch: 20 [40576/50000]\tLoss: 3.8533\tLR: 0.010000\n",
            "Training Epoch: 20 [40704/50000]\tLoss: 3.9642\tLR: 0.010000\n",
            "Training Epoch: 20 [40832/50000]\tLoss: 3.9967\tLR: 0.010000\n",
            "Training Epoch: 20 [40960/50000]\tLoss: 4.1146\tLR: 0.010000\n",
            "Training Epoch: 20 [41088/50000]\tLoss: 3.8911\tLR: 0.010000\n",
            "Training Epoch: 20 [41216/50000]\tLoss: 3.8726\tLR: 0.010000\n",
            "Training Epoch: 20 [41344/50000]\tLoss: 3.8810\tLR: 0.010000\n",
            "Training Epoch: 20 [41472/50000]\tLoss: 4.0489\tLR: 0.010000\n",
            "Training Epoch: 20 [41600/50000]\tLoss: 3.8716\tLR: 0.010000\n",
            "Training Epoch: 20 [41728/50000]\tLoss: 3.9389\tLR: 0.010000\n",
            "Training Epoch: 20 [41856/50000]\tLoss: 3.9382\tLR: 0.010000\n",
            "Training Epoch: 20 [41984/50000]\tLoss: 3.9552\tLR: 0.010000\n",
            "Training Epoch: 20 [42112/50000]\tLoss: 3.9865\tLR: 0.010000\n",
            "Training Epoch: 20 [42240/50000]\tLoss: 3.7905\tLR: 0.010000\n",
            "Training Epoch: 20 [42368/50000]\tLoss: 3.9457\tLR: 0.010000\n",
            "Training Epoch: 20 [42496/50000]\tLoss: 4.0905\tLR: 0.010000\n",
            "Training Epoch: 20 [42624/50000]\tLoss: 3.8763\tLR: 0.010000\n",
            "Training Epoch: 20 [42752/50000]\tLoss: 3.9199\tLR: 0.010000\n",
            "Training Epoch: 20 [42880/50000]\tLoss: 3.9092\tLR: 0.010000\n",
            "Training Epoch: 20 [43008/50000]\tLoss: 3.7461\tLR: 0.010000\n",
            "Training Epoch: 20 [43136/50000]\tLoss: 3.9476\tLR: 0.010000\n",
            "Training Epoch: 20 [43264/50000]\tLoss: 3.8170\tLR: 0.010000\n",
            "Training Epoch: 20 [43392/50000]\tLoss: 3.8666\tLR: 0.010000\n",
            "Training Epoch: 20 [43520/50000]\tLoss: 3.8172\tLR: 0.010000\n",
            "Training Epoch: 20 [43648/50000]\tLoss: 3.7950\tLR: 0.010000\n",
            "Training Epoch: 20 [43776/50000]\tLoss: 3.8318\tLR: 0.010000\n",
            "Training Epoch: 20 [43904/50000]\tLoss: 3.7868\tLR: 0.010000\n",
            "Training Epoch: 20 [44032/50000]\tLoss: 3.8978\tLR: 0.010000\n",
            "Training Epoch: 20 [44160/50000]\tLoss: 3.9708\tLR: 0.010000\n",
            "Training Epoch: 20 [44288/50000]\tLoss: 3.9855\tLR: 0.010000\n",
            "Training Epoch: 20 [44416/50000]\tLoss: 3.8837\tLR: 0.010000\n",
            "Training Epoch: 20 [44544/50000]\tLoss: 3.8995\tLR: 0.010000\n",
            "Training Epoch: 20 [44672/50000]\tLoss: 4.0342\tLR: 0.010000\n",
            "Training Epoch: 20 [44800/50000]\tLoss: 3.8688\tLR: 0.010000\n",
            "Training Epoch: 20 [44928/50000]\tLoss: 3.9822\tLR: 0.010000\n",
            "Training Epoch: 20 [45056/50000]\tLoss: 3.8017\tLR: 0.010000\n",
            "Training Epoch: 20 [45184/50000]\tLoss: 4.0978\tLR: 0.010000\n",
            "Training Epoch: 20 [45312/50000]\tLoss: 4.0051\tLR: 0.010000\n",
            "Training Epoch: 20 [45440/50000]\tLoss: 3.9318\tLR: 0.010000\n",
            "Training Epoch: 20 [45568/50000]\tLoss: 3.9092\tLR: 0.010000\n",
            "Training Epoch: 20 [45696/50000]\tLoss: 3.9450\tLR: 0.010000\n",
            "Training Epoch: 20 [45824/50000]\tLoss: 3.8011\tLR: 0.010000\n",
            "Training Epoch: 20 [45952/50000]\tLoss: 3.8142\tLR: 0.010000\n",
            "Training Epoch: 20 [46080/50000]\tLoss: 3.9939\tLR: 0.010000\n",
            "Training Epoch: 20 [46208/50000]\tLoss: 3.9302\tLR: 0.010000\n",
            "Training Epoch: 20 [46336/50000]\tLoss: 3.9277\tLR: 0.010000\n",
            "Training Epoch: 20 [46464/50000]\tLoss: 3.9555\tLR: 0.010000\n",
            "Training Epoch: 20 [46592/50000]\tLoss: 3.9546\tLR: 0.010000\n",
            "Training Epoch: 20 [46720/50000]\tLoss: 3.9089\tLR: 0.010000\n",
            "Training Epoch: 20 [46848/50000]\tLoss: 3.8290\tLR: 0.010000\n",
            "Training Epoch: 20 [46976/50000]\tLoss: 3.7989\tLR: 0.010000\n",
            "Training Epoch: 20 [47104/50000]\tLoss: 3.8974\tLR: 0.010000\n",
            "Training Epoch: 20 [47232/50000]\tLoss: 4.0106\tLR: 0.010000\n",
            "Training Epoch: 20 [47360/50000]\tLoss: 3.8965\tLR: 0.010000\n",
            "Training Epoch: 20 [47488/50000]\tLoss: 3.9330\tLR: 0.010000\n",
            "Training Epoch: 20 [47616/50000]\tLoss: 3.8765\tLR: 0.010000\n",
            "Training Epoch: 20 [47744/50000]\tLoss: 3.7632\tLR: 0.010000\n",
            "Training Epoch: 20 [47872/50000]\tLoss: 3.9347\tLR: 0.010000\n",
            "Training Epoch: 20 [48000/50000]\tLoss: 3.8834\tLR: 0.010000\n",
            "Training Epoch: 20 [48128/50000]\tLoss: 3.9586\tLR: 0.010000\n",
            "Training Epoch: 20 [48256/50000]\tLoss: 3.8314\tLR: 0.010000\n",
            "Training Epoch: 20 [48384/50000]\tLoss: 3.9555\tLR: 0.010000\n",
            "Training Epoch: 20 [48512/50000]\tLoss: 3.9548\tLR: 0.010000\n",
            "Training Epoch: 20 [48640/50000]\tLoss: 3.9506\tLR: 0.010000\n",
            "Training Epoch: 20 [48768/50000]\tLoss: 3.7683\tLR: 0.010000\n",
            "Training Epoch: 20 [48896/50000]\tLoss: 3.9508\tLR: 0.010000\n",
            "Training Epoch: 20 [49024/50000]\tLoss: 3.6551\tLR: 0.010000\n",
            "Training Epoch: 20 [49152/50000]\tLoss: 3.7432\tLR: 0.010000\n",
            "Training Epoch: 20 [49280/50000]\tLoss: 3.7990\tLR: 0.010000\n",
            "Training Epoch: 20 [49408/50000]\tLoss: 4.1341\tLR: 0.010000\n",
            "Training Epoch: 20 [49536/50000]\tLoss: 4.0395\tLR: 0.010000\n",
            "Training Epoch: 20 [49664/50000]\tLoss: 3.8342\tLR: 0.010000\n",
            "Training Epoch: 20 [49792/50000]\tLoss: 3.8736\tLR: 0.010000\n",
            "Training Epoch: 20 [49920/50000]\tLoss: 3.8551\tLR: 0.010000\n",
            "Training Epoch: 20 [50000/50000]\tLoss: 3.8615\tLR: 0.010000\n",
            "epoch 20 training time consumed: 144.92s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 163162 KiB |   3406 MiB | 212289 GiB | 212289 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 209611 GiB | 209611 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2678 GiB |   2678 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 163162 KiB |   3406 MiB | 212289 GiB | 212289 GiB |\n",
            "|       from large pool |  26240 KiB |   3299 MiB | 209611 GiB | 209611 GiB |\n",
            "|       from small pool | 136922 KiB |    178 MiB |   2678 GiB |   2678 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 163157 KiB |   3399 MiB | 211707 GiB | 211707 GiB |\n",
            "|       from large pool |  26240 KiB |   3287 MiB | 209029 GiB | 209029 GiB |\n",
            "|       from small pool | 136917 KiB |    178 MiB |   2678 GiB |   2677 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   3926 MiB |   3926 MiB |   3926 MiB |      0 B   |\n",
            "|       from large pool |   3742 MiB |   3742 MiB |   3742 MiB |      0 B   |\n",
            "|       from small pool |    184 MiB |    184 MiB |    184 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  53926 KiB |    933 MiB | 177419 GiB | 177419 GiB |\n",
            "|       from large pool |  47488 KiB |    925 MiB | 174393 GiB | 174393 GiB |\n",
            "|       from small pool |   6438 KiB |     32 MiB |   3026 GiB |   3026 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     521    |     688    |   12935 K  |   12934 K  |\n",
            "|       from large pool |       5    |     146    |    6270 K  |    6270 K  |\n",
            "|       from small pool |     516    |     682    |    6665 K  |    6664 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     521    |     688    |   12935 K  |   12934 K  |\n",
            "|       from large pool |       5    |     146    |    6270 K  |    6270 K  |\n",
            "|       from small pool |     516    |     682    |    6665 K  |    6664 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     173    |     173    |     173    |       0    |\n",
            "|       from large pool |      81    |      81    |      81    |       0    |\n",
            "|       from small pool |      92    |      92    |      92    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |     117    |    4810 K  |    4810 K  |\n",
            "|       from large pool |       4    |      46    |    2947 K  |    2947 K  |\n",
            "|       from small pool |      76    |      89    |    1863 K  |    1863 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0307, Accuracy: 0.1091, Time consumed:9.46s\n",
            "\n",
            "saving weights file to checkpoint/densenet169/Tuesday_25_July_2023_08h_03m_33s/densenet169-20-regular.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py -net densenet169 -weights checkpoint/densenet169/Tuesday_25_July_2023_08h_03m_33s/densenet169-20-regular.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dh-kmvoADMrD",
        "outputId": "db3247bb-9691-4e09-b17a-8e38b99d20c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "DenseNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (features): Sequential(\n",
            "    (dense_block_layer_0): Sequential(\n",
            "      (bottle_neck_layer_0): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_1): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_2): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_3): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_4): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_5): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transition_layer_0): Transition(\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      )\n",
            "    )\n",
            "    (dense_block_layer_1): Sequential(\n",
            "      (bottle_neck_layer_0): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_1): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_2): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_3): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_4): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_5): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_6): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_7): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_8): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_9): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_10): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_11): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transition_layer_1): Transition(\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      )\n",
            "    )\n",
            "    (dense_block_layer_2): Sequential(\n",
            "      (bottle_neck_layer_0): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_1): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_2): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_3): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_4): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_5): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_6): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_7): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_8): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_9): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_10): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_11): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_12): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_13): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_14): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_15): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_16): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_17): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_18): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_19): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_20): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_21): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_22): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_23): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_24): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_25): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_26): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_27): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_28): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_29): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_30): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_31): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transition_layer_2): Transition(\n",
            "      (down_sample): Sequential(\n",
            "        (0): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      )\n",
            "    )\n",
            "    (dense_block3): Sequential(\n",
            "      (bottle_neck_layer_0): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_1): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_2): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_3): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_4): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_5): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_6): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_7): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_8): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_9): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_10): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_11): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_12): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_13): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_14): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1088, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_15): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1120, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_16): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1152, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_17): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1184, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_18): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1216, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_19): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1248, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_20): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1280, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_21): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1312, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_22): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1344, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_23): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1376, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_24): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1408, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_25): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1440, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_26): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1472, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_27): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1504, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_28): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1536, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_29): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1568, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_30): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1600, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "      (bottle_neck_layer_31): Bottleneck(\n",
            "        (bottle_neck): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): Conv2d(1632, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (2): ReLU(inplace=True)\n",
            "          (3): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (linear): Linear(in_features=1664, out_features=100, bias=True)\n",
            ")\n",
            "iteration: 1\ttotal 625 iterations\n",
            "iteration: 2\ttotal 625 iterations\n",
            "iteration: 3\ttotal 625 iterations\n",
            "iteration: 4\ttotal 625 iterations\n",
            "iteration: 5\ttotal 625 iterations\n",
            "iteration: 6\ttotal 625 iterations\n",
            "iteration: 7\ttotal 625 iterations\n",
            "iteration: 8\ttotal 625 iterations\n",
            "iteration: 9\ttotal 625 iterations\n",
            "iteration: 10\ttotal 625 iterations\n",
            "iteration: 11\ttotal 625 iterations\n",
            "iteration: 12\ttotal 625 iterations\n",
            "iteration: 13\ttotal 625 iterations\n",
            "iteration: 14\ttotal 625 iterations\n",
            "iteration: 15\ttotal 625 iterations\n",
            "iteration: 16\ttotal 625 iterations\n",
            "iteration: 17\ttotal 625 iterations\n",
            "iteration: 18\ttotal 625 iterations\n",
            "iteration: 19\ttotal 625 iterations\n",
            "iteration: 20\ttotal 625 iterations\n",
            "iteration: 21\ttotal 625 iterations\n",
            "iteration: 22\ttotal 625 iterations\n",
            "iteration: 23\ttotal 625 iterations\n",
            "iteration: 24\ttotal 625 iterations\n",
            "iteration: 25\ttotal 625 iterations\n",
            "iteration: 26\ttotal 625 iterations\n",
            "iteration: 27\ttotal 625 iterations\n",
            "iteration: 28\ttotal 625 iterations\n",
            "iteration: 29\ttotal 625 iterations\n",
            "iteration: 30\ttotal 625 iterations\n",
            "iteration: 31\ttotal 625 iterations\n",
            "iteration: 32\ttotal 625 iterations\n",
            "iteration: 33\ttotal 625 iterations\n",
            "iteration: 34\ttotal 625 iterations\n",
            "iteration: 35\ttotal 625 iterations\n",
            "iteration: 36\ttotal 625 iterations\n",
            "iteration: 37\ttotal 625 iterations\n",
            "iteration: 38\ttotal 625 iterations\n",
            "iteration: 39\ttotal 625 iterations\n",
            "iteration: 40\ttotal 625 iterations\n",
            "iteration: 41\ttotal 625 iterations\n",
            "iteration: 42\ttotal 625 iterations\n",
            "iteration: 43\ttotal 625 iterations\n",
            "iteration: 44\ttotal 625 iterations\n",
            "iteration: 45\ttotal 625 iterations\n",
            "iteration: 46\ttotal 625 iterations\n",
            "iteration: 47\ttotal 625 iterations\n",
            "iteration: 48\ttotal 625 iterations\n",
            "iteration: 49\ttotal 625 iterations\n",
            "iteration: 50\ttotal 625 iterations\n",
            "iteration: 51\ttotal 625 iterations\n",
            "iteration: 52\ttotal 625 iterations\n",
            "iteration: 53\ttotal 625 iterations\n",
            "iteration: 54\ttotal 625 iterations\n",
            "iteration: 55\ttotal 625 iterations\n",
            "iteration: 56\ttotal 625 iterations\n",
            "iteration: 57\ttotal 625 iterations\n",
            "iteration: 58\ttotal 625 iterations\n",
            "iteration: 59\ttotal 625 iterations\n",
            "iteration: 60\ttotal 625 iterations\n",
            "iteration: 61\ttotal 625 iterations\n",
            "iteration: 62\ttotal 625 iterations\n",
            "iteration: 63\ttotal 625 iterations\n",
            "iteration: 64\ttotal 625 iterations\n",
            "iteration: 65\ttotal 625 iterations\n",
            "iteration: 66\ttotal 625 iterations\n",
            "iteration: 67\ttotal 625 iterations\n",
            "iteration: 68\ttotal 625 iterations\n",
            "iteration: 69\ttotal 625 iterations\n",
            "iteration: 70\ttotal 625 iterations\n",
            "iteration: 71\ttotal 625 iterations\n",
            "iteration: 72\ttotal 625 iterations\n",
            "iteration: 73\ttotal 625 iterations\n",
            "iteration: 74\ttotal 625 iterations\n",
            "iteration: 75\ttotal 625 iterations\n",
            "iteration: 76\ttotal 625 iterations\n",
            "iteration: 77\ttotal 625 iterations\n",
            "iteration: 78\ttotal 625 iterations\n",
            "iteration: 79\ttotal 625 iterations\n",
            "iteration: 80\ttotal 625 iterations\n",
            "iteration: 81\ttotal 625 iterations\n",
            "iteration: 82\ttotal 625 iterations\n",
            "iteration: 83\ttotal 625 iterations\n",
            "iteration: 84\ttotal 625 iterations\n",
            "iteration: 85\ttotal 625 iterations\n",
            "iteration: 86\ttotal 625 iterations\n",
            "iteration: 87\ttotal 625 iterations\n",
            "iteration: 88\ttotal 625 iterations\n",
            "iteration: 89\ttotal 625 iterations\n",
            "iteration: 90\ttotal 625 iterations\n",
            "iteration: 91\ttotal 625 iterations\n",
            "iteration: 92\ttotal 625 iterations\n",
            "iteration: 93\ttotal 625 iterations\n",
            "iteration: 94\ttotal 625 iterations\n",
            "iteration: 95\ttotal 625 iterations\n",
            "iteration: 96\ttotal 625 iterations\n",
            "iteration: 97\ttotal 625 iterations\n",
            "iteration: 98\ttotal 625 iterations\n",
            "iteration: 99\ttotal 625 iterations\n",
            "iteration: 100\ttotal 625 iterations\n",
            "iteration: 101\ttotal 625 iterations\n",
            "iteration: 102\ttotal 625 iterations\n",
            "iteration: 103\ttotal 625 iterations\n",
            "iteration: 104\ttotal 625 iterations\n",
            "iteration: 105\ttotal 625 iterations\n",
            "iteration: 106\ttotal 625 iterations\n",
            "iteration: 107\ttotal 625 iterations\n",
            "iteration: 108\ttotal 625 iterations\n",
            "iteration: 109\ttotal 625 iterations\n",
            "iteration: 110\ttotal 625 iterations\n",
            "iteration: 111\ttotal 625 iterations\n",
            "iteration: 112\ttotal 625 iterations\n",
            "iteration: 113\ttotal 625 iterations\n",
            "iteration: 114\ttotal 625 iterations\n",
            "iteration: 115\ttotal 625 iterations\n",
            "iteration: 116\ttotal 625 iterations\n",
            "iteration: 117\ttotal 625 iterations\n",
            "iteration: 118\ttotal 625 iterations\n",
            "iteration: 119\ttotal 625 iterations\n",
            "iteration: 120\ttotal 625 iterations\n",
            "iteration: 121\ttotal 625 iterations\n",
            "iteration: 122\ttotal 625 iterations\n",
            "iteration: 123\ttotal 625 iterations\n",
            "iteration: 124\ttotal 625 iterations\n",
            "iteration: 125\ttotal 625 iterations\n",
            "iteration: 126\ttotal 625 iterations\n",
            "iteration: 127\ttotal 625 iterations\n",
            "iteration: 128\ttotal 625 iterations\n",
            "iteration: 129\ttotal 625 iterations\n",
            "iteration: 130\ttotal 625 iterations\n",
            "iteration: 131\ttotal 625 iterations\n",
            "iteration: 132\ttotal 625 iterations\n",
            "iteration: 133\ttotal 625 iterations\n",
            "iteration: 134\ttotal 625 iterations\n",
            "iteration: 135\ttotal 625 iterations\n",
            "iteration: 136\ttotal 625 iterations\n",
            "iteration: 137\ttotal 625 iterations\n",
            "iteration: 138\ttotal 625 iterations\n",
            "iteration: 139\ttotal 625 iterations\n",
            "iteration: 140\ttotal 625 iterations\n",
            "iteration: 141\ttotal 625 iterations\n",
            "iteration: 142\ttotal 625 iterations\n",
            "iteration: 143\ttotal 625 iterations\n",
            "iteration: 144\ttotal 625 iterations\n",
            "iteration: 145\ttotal 625 iterations\n",
            "iteration: 146\ttotal 625 iterations\n",
            "iteration: 147\ttotal 625 iterations\n",
            "iteration: 148\ttotal 625 iterations\n",
            "iteration: 149\ttotal 625 iterations\n",
            "iteration: 150\ttotal 625 iterations\n",
            "iteration: 151\ttotal 625 iterations\n",
            "iteration: 152\ttotal 625 iterations\n",
            "iteration: 153\ttotal 625 iterations\n",
            "iteration: 154\ttotal 625 iterations\n",
            "iteration: 155\ttotal 625 iterations\n",
            "iteration: 156\ttotal 625 iterations\n",
            "iteration: 157\ttotal 625 iterations\n",
            "iteration: 158\ttotal 625 iterations\n",
            "iteration: 159\ttotal 625 iterations\n",
            "iteration: 160\ttotal 625 iterations\n",
            "iteration: 161\ttotal 625 iterations\n",
            "iteration: 162\ttotal 625 iterations\n",
            "iteration: 163\ttotal 625 iterations\n",
            "iteration: 164\ttotal 625 iterations\n",
            "iteration: 165\ttotal 625 iterations\n",
            "iteration: 166\ttotal 625 iterations\n",
            "iteration: 167\ttotal 625 iterations\n",
            "iteration: 168\ttotal 625 iterations\n",
            "iteration: 169\ttotal 625 iterations\n",
            "iteration: 170\ttotal 625 iterations\n",
            "iteration: 171\ttotal 625 iterations\n",
            "iteration: 172\ttotal 625 iterations\n",
            "iteration: 173\ttotal 625 iterations\n",
            "iteration: 174\ttotal 625 iterations\n",
            "iteration: 175\ttotal 625 iterations\n",
            "iteration: 176\ttotal 625 iterations\n",
            "iteration: 177\ttotal 625 iterations\n",
            "iteration: 178\ttotal 625 iterations\n",
            "iteration: 179\ttotal 625 iterations\n",
            "iteration: 180\ttotal 625 iterations\n",
            "iteration: 181\ttotal 625 iterations\n",
            "iteration: 182\ttotal 625 iterations\n",
            "iteration: 183\ttotal 625 iterations\n",
            "iteration: 184\ttotal 625 iterations\n",
            "iteration: 185\ttotal 625 iterations\n",
            "iteration: 186\ttotal 625 iterations\n",
            "iteration: 187\ttotal 625 iterations\n",
            "iteration: 188\ttotal 625 iterations\n",
            "iteration: 189\ttotal 625 iterations\n",
            "iteration: 190\ttotal 625 iterations\n",
            "iteration: 191\ttotal 625 iterations\n",
            "iteration: 192\ttotal 625 iterations\n",
            "iteration: 193\ttotal 625 iterations\n",
            "iteration: 194\ttotal 625 iterations\n",
            "iteration: 195\ttotal 625 iterations\n",
            "iteration: 196\ttotal 625 iterations\n",
            "iteration: 197\ttotal 625 iterations\n",
            "iteration: 198\ttotal 625 iterations\n",
            "iteration: 199\ttotal 625 iterations\n",
            "iteration: 200\ttotal 625 iterations\n",
            "iteration: 201\ttotal 625 iterations\n",
            "iteration: 202\ttotal 625 iterations\n",
            "iteration: 203\ttotal 625 iterations\n",
            "iteration: 204\ttotal 625 iterations\n",
            "iteration: 205\ttotal 625 iterations\n",
            "iteration: 206\ttotal 625 iterations\n",
            "iteration: 207\ttotal 625 iterations\n",
            "iteration: 208\ttotal 625 iterations\n",
            "iteration: 209\ttotal 625 iterations\n",
            "iteration: 210\ttotal 625 iterations\n",
            "iteration: 211\ttotal 625 iterations\n",
            "iteration: 212\ttotal 625 iterations\n",
            "iteration: 213\ttotal 625 iterations\n",
            "iteration: 214\ttotal 625 iterations\n",
            "iteration: 215\ttotal 625 iterations\n",
            "iteration: 216\ttotal 625 iterations\n",
            "iteration: 217\ttotal 625 iterations\n",
            "iteration: 218\ttotal 625 iterations\n",
            "iteration: 219\ttotal 625 iterations\n",
            "iteration: 220\ttotal 625 iterations\n",
            "iteration: 221\ttotal 625 iterations\n",
            "iteration: 222\ttotal 625 iterations\n",
            "iteration: 223\ttotal 625 iterations\n",
            "iteration: 224\ttotal 625 iterations\n",
            "iteration: 225\ttotal 625 iterations\n",
            "iteration: 226\ttotal 625 iterations\n",
            "iteration: 227\ttotal 625 iterations\n",
            "iteration: 228\ttotal 625 iterations\n",
            "iteration: 229\ttotal 625 iterations\n",
            "iteration: 230\ttotal 625 iterations\n",
            "iteration: 231\ttotal 625 iterations\n",
            "iteration: 232\ttotal 625 iterations\n",
            "iteration: 233\ttotal 625 iterations\n",
            "iteration: 234\ttotal 625 iterations\n",
            "iteration: 235\ttotal 625 iterations\n",
            "iteration: 236\ttotal 625 iterations\n",
            "iteration: 237\ttotal 625 iterations\n",
            "iteration: 238\ttotal 625 iterations\n",
            "iteration: 239\ttotal 625 iterations\n",
            "iteration: 240\ttotal 625 iterations\n",
            "iteration: 241\ttotal 625 iterations\n",
            "iteration: 242\ttotal 625 iterations\n",
            "iteration: 243\ttotal 625 iterations\n",
            "iteration: 244\ttotal 625 iterations\n",
            "iteration: 245\ttotal 625 iterations\n",
            "iteration: 246\ttotal 625 iterations\n",
            "iteration: 247\ttotal 625 iterations\n",
            "iteration: 248\ttotal 625 iterations\n",
            "iteration: 249\ttotal 625 iterations\n",
            "iteration: 250\ttotal 625 iterations\n",
            "iteration: 251\ttotal 625 iterations\n",
            "iteration: 252\ttotal 625 iterations\n",
            "iteration: 253\ttotal 625 iterations\n",
            "iteration: 254\ttotal 625 iterations\n",
            "iteration: 255\ttotal 625 iterations\n",
            "iteration: 256\ttotal 625 iterations\n",
            "iteration: 257\ttotal 625 iterations\n",
            "iteration: 258\ttotal 625 iterations\n",
            "iteration: 259\ttotal 625 iterations\n",
            "iteration: 260\ttotal 625 iterations\n",
            "iteration: 261\ttotal 625 iterations\n",
            "iteration: 262\ttotal 625 iterations\n",
            "iteration: 263\ttotal 625 iterations\n",
            "iteration: 264\ttotal 625 iterations\n",
            "iteration: 265\ttotal 625 iterations\n",
            "iteration: 266\ttotal 625 iterations\n",
            "iteration: 267\ttotal 625 iterations\n",
            "iteration: 268\ttotal 625 iterations\n",
            "iteration: 269\ttotal 625 iterations\n",
            "iteration: 270\ttotal 625 iterations\n",
            "iteration: 271\ttotal 625 iterations\n",
            "iteration: 272\ttotal 625 iterations\n",
            "iteration: 273\ttotal 625 iterations\n",
            "iteration: 274\ttotal 625 iterations\n",
            "iteration: 275\ttotal 625 iterations\n",
            "iteration: 276\ttotal 625 iterations\n",
            "iteration: 277\ttotal 625 iterations\n",
            "iteration: 278\ttotal 625 iterations\n",
            "iteration: 279\ttotal 625 iterations\n",
            "iteration: 280\ttotal 625 iterations\n",
            "iteration: 281\ttotal 625 iterations\n",
            "iteration: 282\ttotal 625 iterations\n",
            "iteration: 283\ttotal 625 iterations\n",
            "iteration: 284\ttotal 625 iterations\n",
            "iteration: 285\ttotal 625 iterations\n",
            "iteration: 286\ttotal 625 iterations\n",
            "iteration: 287\ttotal 625 iterations\n",
            "iteration: 288\ttotal 625 iterations\n",
            "iteration: 289\ttotal 625 iterations\n",
            "iteration: 290\ttotal 625 iterations\n",
            "iteration: 291\ttotal 625 iterations\n",
            "iteration: 292\ttotal 625 iterations\n",
            "iteration: 293\ttotal 625 iterations\n",
            "iteration: 294\ttotal 625 iterations\n",
            "iteration: 295\ttotal 625 iterations\n",
            "iteration: 296\ttotal 625 iterations\n",
            "iteration: 297\ttotal 625 iterations\n",
            "iteration: 298\ttotal 625 iterations\n",
            "iteration: 299\ttotal 625 iterations\n",
            "iteration: 300\ttotal 625 iterations\n",
            "iteration: 301\ttotal 625 iterations\n",
            "iteration: 302\ttotal 625 iterations\n",
            "iteration: 303\ttotal 625 iterations\n",
            "iteration: 304\ttotal 625 iterations\n",
            "iteration: 305\ttotal 625 iterations\n",
            "iteration: 306\ttotal 625 iterations\n",
            "iteration: 307\ttotal 625 iterations\n",
            "iteration: 308\ttotal 625 iterations\n",
            "iteration: 309\ttotal 625 iterations\n",
            "iteration: 310\ttotal 625 iterations\n",
            "iteration: 311\ttotal 625 iterations\n",
            "iteration: 312\ttotal 625 iterations\n",
            "iteration: 313\ttotal 625 iterations\n",
            "iteration: 314\ttotal 625 iterations\n",
            "iteration: 315\ttotal 625 iterations\n",
            "iteration: 316\ttotal 625 iterations\n",
            "iteration: 317\ttotal 625 iterations\n",
            "iteration: 318\ttotal 625 iterations\n",
            "iteration: 319\ttotal 625 iterations\n",
            "iteration: 320\ttotal 625 iterations\n",
            "iteration: 321\ttotal 625 iterations\n",
            "iteration: 322\ttotal 625 iterations\n",
            "iteration: 323\ttotal 625 iterations\n",
            "iteration: 324\ttotal 625 iterations\n",
            "iteration: 325\ttotal 625 iterations\n",
            "iteration: 326\ttotal 625 iterations\n",
            "iteration: 327\ttotal 625 iterations\n",
            "iteration: 328\ttotal 625 iterations\n",
            "iteration: 329\ttotal 625 iterations\n",
            "iteration: 330\ttotal 625 iterations\n",
            "iteration: 331\ttotal 625 iterations\n",
            "iteration: 332\ttotal 625 iterations\n",
            "iteration: 333\ttotal 625 iterations\n",
            "iteration: 334\ttotal 625 iterations\n",
            "iteration: 335\ttotal 625 iterations\n",
            "iteration: 336\ttotal 625 iterations\n",
            "iteration: 337\ttotal 625 iterations\n",
            "iteration: 338\ttotal 625 iterations\n",
            "iteration: 339\ttotal 625 iterations\n",
            "iteration: 340\ttotal 625 iterations\n",
            "iteration: 341\ttotal 625 iterations\n",
            "iteration: 342\ttotal 625 iterations\n",
            "iteration: 343\ttotal 625 iterations\n",
            "iteration: 344\ttotal 625 iterations\n",
            "iteration: 345\ttotal 625 iterations\n",
            "iteration: 346\ttotal 625 iterations\n",
            "iteration: 347\ttotal 625 iterations\n",
            "iteration: 348\ttotal 625 iterations\n",
            "iteration: 349\ttotal 625 iterations\n",
            "iteration: 350\ttotal 625 iterations\n",
            "iteration: 351\ttotal 625 iterations\n",
            "iteration: 352\ttotal 625 iterations\n",
            "iteration: 353\ttotal 625 iterations\n",
            "iteration: 354\ttotal 625 iterations\n",
            "iteration: 355\ttotal 625 iterations\n",
            "iteration: 356\ttotal 625 iterations\n",
            "iteration: 357\ttotal 625 iterations\n",
            "iteration: 358\ttotal 625 iterations\n",
            "iteration: 359\ttotal 625 iterations\n",
            "iteration: 360\ttotal 625 iterations\n",
            "iteration: 361\ttotal 625 iterations\n",
            "iteration: 362\ttotal 625 iterations\n",
            "iteration: 363\ttotal 625 iterations\n",
            "iteration: 364\ttotal 625 iterations\n",
            "iteration: 365\ttotal 625 iterations\n",
            "iteration: 366\ttotal 625 iterations\n",
            "iteration: 367\ttotal 625 iterations\n",
            "iteration: 368\ttotal 625 iterations\n",
            "iteration: 369\ttotal 625 iterations\n",
            "iteration: 370\ttotal 625 iterations\n",
            "iteration: 371\ttotal 625 iterations\n",
            "iteration: 372\ttotal 625 iterations\n",
            "iteration: 373\ttotal 625 iterations\n",
            "iteration: 374\ttotal 625 iterations\n",
            "iteration: 375\ttotal 625 iterations\n",
            "iteration: 376\ttotal 625 iterations\n",
            "iteration: 377\ttotal 625 iterations\n",
            "iteration: 378\ttotal 625 iterations\n",
            "iteration: 379\ttotal 625 iterations\n",
            "iteration: 380\ttotal 625 iterations\n",
            "iteration: 381\ttotal 625 iterations\n",
            "iteration: 382\ttotal 625 iterations\n",
            "iteration: 383\ttotal 625 iterations\n",
            "iteration: 384\ttotal 625 iterations\n",
            "iteration: 385\ttotal 625 iterations\n",
            "iteration: 386\ttotal 625 iterations\n",
            "iteration: 387\ttotal 625 iterations\n",
            "iteration: 388\ttotal 625 iterations\n",
            "iteration: 389\ttotal 625 iterations\n",
            "iteration: 390\ttotal 625 iterations\n",
            "iteration: 391\ttotal 625 iterations\n",
            "iteration: 392\ttotal 625 iterations\n",
            "iteration: 393\ttotal 625 iterations\n",
            "iteration: 394\ttotal 625 iterations\n",
            "iteration: 395\ttotal 625 iterations\n",
            "iteration: 396\ttotal 625 iterations\n",
            "iteration: 397\ttotal 625 iterations\n",
            "iteration: 398\ttotal 625 iterations\n",
            "iteration: 399\ttotal 625 iterations\n",
            "iteration: 400\ttotal 625 iterations\n",
            "iteration: 401\ttotal 625 iterations\n",
            "iteration: 402\ttotal 625 iterations\n",
            "iteration: 403\ttotal 625 iterations\n",
            "iteration: 404\ttotal 625 iterations\n",
            "iteration: 405\ttotal 625 iterations\n",
            "iteration: 406\ttotal 625 iterations\n",
            "iteration: 407\ttotal 625 iterations\n",
            "iteration: 408\ttotal 625 iterations\n",
            "iteration: 409\ttotal 625 iterations\n",
            "iteration: 410\ttotal 625 iterations\n",
            "iteration: 411\ttotal 625 iterations\n",
            "iteration: 412\ttotal 625 iterations\n",
            "iteration: 413\ttotal 625 iterations\n",
            "iteration: 414\ttotal 625 iterations\n",
            "iteration: 415\ttotal 625 iterations\n",
            "iteration: 416\ttotal 625 iterations\n",
            "iteration: 417\ttotal 625 iterations\n",
            "iteration: 418\ttotal 625 iterations\n",
            "iteration: 419\ttotal 625 iterations\n",
            "iteration: 420\ttotal 625 iterations\n",
            "iteration: 421\ttotal 625 iterations\n",
            "iteration: 422\ttotal 625 iterations\n",
            "iteration: 423\ttotal 625 iterations\n",
            "iteration: 424\ttotal 625 iterations\n",
            "iteration: 425\ttotal 625 iterations\n",
            "iteration: 426\ttotal 625 iterations\n",
            "iteration: 427\ttotal 625 iterations\n",
            "iteration: 428\ttotal 625 iterations\n",
            "iteration: 429\ttotal 625 iterations\n",
            "iteration: 430\ttotal 625 iterations\n",
            "iteration: 431\ttotal 625 iterations\n",
            "iteration: 432\ttotal 625 iterations\n",
            "iteration: 433\ttotal 625 iterations\n",
            "iteration: 434\ttotal 625 iterations\n",
            "iteration: 435\ttotal 625 iterations\n",
            "iteration: 436\ttotal 625 iterations\n",
            "iteration: 437\ttotal 625 iterations\n",
            "iteration: 438\ttotal 625 iterations\n",
            "iteration: 439\ttotal 625 iterations\n",
            "iteration: 440\ttotal 625 iterations\n",
            "iteration: 441\ttotal 625 iterations\n",
            "iteration: 442\ttotal 625 iterations\n",
            "iteration: 443\ttotal 625 iterations\n",
            "iteration: 444\ttotal 625 iterations\n",
            "iteration: 445\ttotal 625 iterations\n",
            "iteration: 446\ttotal 625 iterations\n",
            "iteration: 447\ttotal 625 iterations\n",
            "iteration: 448\ttotal 625 iterations\n",
            "iteration: 449\ttotal 625 iterations\n",
            "iteration: 450\ttotal 625 iterations\n",
            "iteration: 451\ttotal 625 iterations\n",
            "iteration: 452\ttotal 625 iterations\n",
            "iteration: 453\ttotal 625 iterations\n",
            "iteration: 454\ttotal 625 iterations\n",
            "iteration: 455\ttotal 625 iterations\n",
            "iteration: 456\ttotal 625 iterations\n",
            "iteration: 457\ttotal 625 iterations\n",
            "iteration: 458\ttotal 625 iterations\n",
            "iteration: 459\ttotal 625 iterations\n",
            "iteration: 460\ttotal 625 iterations\n",
            "iteration: 461\ttotal 625 iterations\n",
            "iteration: 462\ttotal 625 iterations\n",
            "iteration: 463\ttotal 625 iterations\n",
            "iteration: 464\ttotal 625 iterations\n",
            "iteration: 465\ttotal 625 iterations\n",
            "iteration: 466\ttotal 625 iterations\n",
            "iteration: 467\ttotal 625 iterations\n",
            "iteration: 468\ttotal 625 iterations\n",
            "iteration: 469\ttotal 625 iterations\n",
            "iteration: 470\ttotal 625 iterations\n",
            "iteration: 471\ttotal 625 iterations\n",
            "iteration: 472\ttotal 625 iterations\n",
            "iteration: 473\ttotal 625 iterations\n",
            "iteration: 474\ttotal 625 iterations\n",
            "iteration: 475\ttotal 625 iterations\n",
            "iteration: 476\ttotal 625 iterations\n",
            "iteration: 477\ttotal 625 iterations\n",
            "iteration: 478\ttotal 625 iterations\n",
            "iteration: 479\ttotal 625 iterations\n",
            "iteration: 480\ttotal 625 iterations\n",
            "iteration: 481\ttotal 625 iterations\n",
            "iteration: 482\ttotal 625 iterations\n",
            "iteration: 483\ttotal 625 iterations\n",
            "iteration: 484\ttotal 625 iterations\n",
            "iteration: 485\ttotal 625 iterations\n",
            "iteration: 486\ttotal 625 iterations\n",
            "iteration: 487\ttotal 625 iterations\n",
            "iteration: 488\ttotal 625 iterations\n",
            "iteration: 489\ttotal 625 iterations\n",
            "iteration: 490\ttotal 625 iterations\n",
            "iteration: 491\ttotal 625 iterations\n",
            "iteration: 492\ttotal 625 iterations\n",
            "iteration: 493\ttotal 625 iterations\n",
            "iteration: 494\ttotal 625 iterations\n",
            "iteration: 495\ttotal 625 iterations\n",
            "iteration: 496\ttotal 625 iterations\n",
            "iteration: 497\ttotal 625 iterations\n",
            "iteration: 498\ttotal 625 iterations\n",
            "iteration: 499\ttotal 625 iterations\n",
            "iteration: 500\ttotal 625 iterations\n",
            "iteration: 501\ttotal 625 iterations\n",
            "iteration: 502\ttotal 625 iterations\n",
            "iteration: 503\ttotal 625 iterations\n",
            "iteration: 504\ttotal 625 iterations\n",
            "iteration: 505\ttotal 625 iterations\n",
            "iteration: 506\ttotal 625 iterations\n",
            "iteration: 507\ttotal 625 iterations\n",
            "iteration: 508\ttotal 625 iterations\n",
            "iteration: 509\ttotal 625 iterations\n",
            "iteration: 510\ttotal 625 iterations\n",
            "iteration: 511\ttotal 625 iterations\n",
            "iteration: 512\ttotal 625 iterations\n",
            "iteration: 513\ttotal 625 iterations\n",
            "iteration: 514\ttotal 625 iterations\n",
            "iteration: 515\ttotal 625 iterations\n",
            "iteration: 516\ttotal 625 iterations\n",
            "iteration: 517\ttotal 625 iterations\n",
            "iteration: 518\ttotal 625 iterations\n",
            "iteration: 519\ttotal 625 iterations\n",
            "iteration: 520\ttotal 625 iterations\n",
            "iteration: 521\ttotal 625 iterations\n",
            "iteration: 522\ttotal 625 iterations\n",
            "iteration: 523\ttotal 625 iterations\n",
            "iteration: 524\ttotal 625 iterations\n",
            "iteration: 525\ttotal 625 iterations\n",
            "iteration: 526\ttotal 625 iterations\n",
            "iteration: 527\ttotal 625 iterations\n",
            "iteration: 528\ttotal 625 iterations\n",
            "iteration: 529\ttotal 625 iterations\n",
            "iteration: 530\ttotal 625 iterations\n",
            "iteration: 531\ttotal 625 iterations\n",
            "iteration: 532\ttotal 625 iterations\n",
            "iteration: 533\ttotal 625 iterations\n",
            "iteration: 534\ttotal 625 iterations\n",
            "iteration: 535\ttotal 625 iterations\n",
            "iteration: 536\ttotal 625 iterations\n",
            "iteration: 537\ttotal 625 iterations\n",
            "iteration: 538\ttotal 625 iterations\n",
            "iteration: 539\ttotal 625 iterations\n",
            "iteration: 540\ttotal 625 iterations\n",
            "iteration: 541\ttotal 625 iterations\n",
            "iteration: 542\ttotal 625 iterations\n",
            "iteration: 543\ttotal 625 iterations\n",
            "iteration: 544\ttotal 625 iterations\n",
            "iteration: 545\ttotal 625 iterations\n",
            "iteration: 546\ttotal 625 iterations\n",
            "iteration: 547\ttotal 625 iterations\n",
            "iteration: 548\ttotal 625 iterations\n",
            "iteration: 549\ttotal 625 iterations\n",
            "iteration: 550\ttotal 625 iterations\n",
            "iteration: 551\ttotal 625 iterations\n",
            "iteration: 552\ttotal 625 iterations\n",
            "iteration: 553\ttotal 625 iterations\n",
            "iteration: 554\ttotal 625 iterations\n",
            "iteration: 555\ttotal 625 iterations\n",
            "iteration: 556\ttotal 625 iterations\n",
            "iteration: 557\ttotal 625 iterations\n",
            "iteration: 558\ttotal 625 iterations\n",
            "iteration: 559\ttotal 625 iterations\n",
            "iteration: 560\ttotal 625 iterations\n",
            "iteration: 561\ttotal 625 iterations\n",
            "iteration: 562\ttotal 625 iterations\n",
            "iteration: 563\ttotal 625 iterations\n",
            "iteration: 564\ttotal 625 iterations\n",
            "iteration: 565\ttotal 625 iterations\n",
            "iteration: 566\ttotal 625 iterations\n",
            "iteration: 567\ttotal 625 iterations\n",
            "iteration: 568\ttotal 625 iterations\n",
            "iteration: 569\ttotal 625 iterations\n",
            "iteration: 570\ttotal 625 iterations\n",
            "iteration: 571\ttotal 625 iterations\n",
            "iteration: 572\ttotal 625 iterations\n",
            "iteration: 573\ttotal 625 iterations\n",
            "iteration: 574\ttotal 625 iterations\n",
            "iteration: 575\ttotal 625 iterations\n",
            "iteration: 576\ttotal 625 iterations\n",
            "iteration: 577\ttotal 625 iterations\n",
            "iteration: 578\ttotal 625 iterations\n",
            "iteration: 579\ttotal 625 iterations\n",
            "iteration: 580\ttotal 625 iterations\n",
            "iteration: 581\ttotal 625 iterations\n",
            "iteration: 582\ttotal 625 iterations\n",
            "iteration: 583\ttotal 625 iterations\n",
            "iteration: 584\ttotal 625 iterations\n",
            "iteration: 585\ttotal 625 iterations\n",
            "iteration: 586\ttotal 625 iterations\n",
            "iteration: 587\ttotal 625 iterations\n",
            "iteration: 588\ttotal 625 iterations\n",
            "iteration: 589\ttotal 625 iterations\n",
            "iteration: 590\ttotal 625 iterations\n",
            "iteration: 591\ttotal 625 iterations\n",
            "iteration: 592\ttotal 625 iterations\n",
            "iteration: 593\ttotal 625 iterations\n",
            "iteration: 594\ttotal 625 iterations\n",
            "iteration: 595\ttotal 625 iterations\n",
            "iteration: 596\ttotal 625 iterations\n",
            "iteration: 597\ttotal 625 iterations\n",
            "iteration: 598\ttotal 625 iterations\n",
            "iteration: 599\ttotal 625 iterations\n",
            "iteration: 600\ttotal 625 iterations\n",
            "iteration: 601\ttotal 625 iterations\n",
            "iteration: 602\ttotal 625 iterations\n",
            "iteration: 603\ttotal 625 iterations\n",
            "iteration: 604\ttotal 625 iterations\n",
            "iteration: 605\ttotal 625 iterations\n",
            "iteration: 606\ttotal 625 iterations\n",
            "iteration: 607\ttotal 625 iterations\n",
            "iteration: 608\ttotal 625 iterations\n",
            "iteration: 609\ttotal 625 iterations\n",
            "iteration: 610\ttotal 625 iterations\n",
            "iteration: 611\ttotal 625 iterations\n",
            "iteration: 612\ttotal 625 iterations\n",
            "iteration: 613\ttotal 625 iterations\n",
            "iteration: 614\ttotal 625 iterations\n",
            "iteration: 615\ttotal 625 iterations\n",
            "iteration: 616\ttotal 625 iterations\n",
            "iteration: 617\ttotal 625 iterations\n",
            "iteration: 618\ttotal 625 iterations\n",
            "iteration: 619\ttotal 625 iterations\n",
            "iteration: 620\ttotal 625 iterations\n",
            "iteration: 621\ttotal 625 iterations\n",
            "iteration: 622\ttotal 625 iterations\n",
            "iteration: 623\ttotal 625 iterations\n",
            "iteration: 624\ttotal 625 iterations\n",
            "iteration: 625\ttotal 625 iterations\n",
            "\n",
            "Top 1 err:  tensor(0.8909)\n",
            "Top 5 err:  tensor(0.6823)\n",
            "Parameter numbers: 12484900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without batch norm\n",
        "!python train.py -net googlenet -gpu"
      ],
      "metadata": {
        "id": "z3F4DyjeDMnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bafc76-7baa-4333-d89a-ce9cb5823e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 9 [40832/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [40960/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [41088/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [41216/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 9 [41344/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 9 [41472/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [41600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 9 [41728/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [41856/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [41984/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [42112/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [42240/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 9 [42368/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [42496/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [42624/50000]\tLoss: 4.6078\tLR: 0.010000\n",
            "Training Epoch: 9 [42752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [42880/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 9 [43008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [43136/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 9 [43264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [43392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [43520/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [43648/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 9 [43776/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [43904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 9 [44032/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 9 [44160/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 9 [44288/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [44416/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [44544/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 9 [44672/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [44800/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 9 [44928/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45056/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [45184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [45312/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [45440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45568/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 9 [45696/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 9 [45824/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [45952/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [46080/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 9 [46208/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [46336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [46464/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 9 [46592/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 9 [46720/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [46848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 9 [46976/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 9 [47104/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [47232/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [47360/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [47488/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [47616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [47744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [47872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [48000/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [48128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 9 [48256/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [48384/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [48512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [48640/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [48768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [48896/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 9 [49024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [49152/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 9 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [49408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [49536/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [49792/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 9 [49920/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [50000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "epoch 9 training time consumed: 54.98s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  40943 GiB |  40943 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  40673 GiB |  40672 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    270 GiB |    270 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  40943 GiB |  40943 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  40673 GiB |  40672 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    270 GiB |    270 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  40877 GiB |  40877 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  40607 GiB |  40607 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    270 GiB |    270 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  32601 GiB |  32601 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  32308 GiB |  32307 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    293 GiB |    293 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    2608 K  |    2608 K  |\n",
            "|       from large pool |      20    |     103    |    1238 K  |    1238 K  |\n",
            "|       from small pool |     384    |     506    |    1369 K  |    1369 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    2608 K  |    2608 K  |\n",
            "|       from large pool |      20    |     103    |    1238 K  |    1238 K  |\n",
            "|       from small pool |     384    |     506    |    1369 K  |    1369 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |  737294    |  737268    |\n",
            "|       from large pool |       8    |      17    |  491711    |  491703    |\n",
            "|       from small pool |      18    |      33    |  245583    |  245565    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:4.30s\n",
            "\n",
            "Training Epoch: 10 [128/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [256/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [384/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [512/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [640/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [896/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [1024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [1152/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [1280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [1408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [1536/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [1664/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [1920/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [2048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [2176/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [2304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [2432/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [2560/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [2688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [2816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [2944/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [3072/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [3200/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [3328/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [3456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [3584/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [3712/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [3840/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [3968/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [4096/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [4224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [4352/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [4480/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [4608/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [4736/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [4864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [4992/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [5120/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [5248/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [5376/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [5504/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [5632/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [5760/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [5888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [6016/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 10 [6144/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [6272/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [6400/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [6528/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [6656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [6784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [6912/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [7040/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [7168/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [7296/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [7424/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [7552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [7680/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [7808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [7936/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [8064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [8192/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [8320/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [8448/50000]\tLoss: 4.6033\tLR: 0.010000\n",
            "Training Epoch: 10 [8576/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [8704/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [8832/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [8960/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [9088/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [9216/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [9344/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [9472/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [9600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [9728/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [9856/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [9984/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [10112/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [10240/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [10368/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 10 [10496/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [10624/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [10752/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [10880/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [11008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [11264/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [11392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [11520/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [11648/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [11776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [11904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [12032/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [12160/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [12288/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [12416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [12544/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 10 [12672/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [12800/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [12928/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [13056/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [13184/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [13312/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [13440/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [13568/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [13696/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [13824/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [13952/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [14080/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [14208/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [14336/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [14464/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [14592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [14720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [14848/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [14976/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [15104/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [15232/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [15360/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [15488/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 10 [15616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [15744/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [15872/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [16000/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 10 [16128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [16256/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [16384/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [16512/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [16640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [16768/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [16896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [17024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [17152/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [17280/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [17408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [17536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [17664/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [17792/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 10 [17920/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [18048/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 10 [18176/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [18304/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [18432/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [18560/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [18688/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [18816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [18944/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [19072/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [19200/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [19328/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [19456/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [19584/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [19712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [19840/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [19968/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [20096/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [20224/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [20352/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [20480/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 10 [20608/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [20736/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [20864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [20992/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [21120/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [21248/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [21376/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [21504/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [21632/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [21760/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [21888/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [22016/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [22144/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [22272/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [22400/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [22528/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [22656/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [22784/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 10 [22912/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [23040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [23168/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [23296/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [23424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [23552/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [23680/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [23808/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [23936/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [24064/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [24192/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [24320/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [24448/50000]\tLoss: 4.6081\tLR: 0.010000\n",
            "Training Epoch: 10 [24576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [24704/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [24832/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [24960/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [25088/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 10 [25216/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [25344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [25472/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [25600/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 10 [25728/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 10 [25856/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [25984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [26112/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [26240/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 10 [26368/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [26496/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [26624/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [26752/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [26880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [27008/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [27136/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [27264/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [27392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [27520/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [27648/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [27776/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [27904/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [28032/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [28160/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [28288/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [28416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [28544/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [28672/50000]\tLoss: 4.6034\tLR: 0.010000\n",
            "Training Epoch: 10 [28800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [28928/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [29056/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [29184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [29312/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [29440/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [29568/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [29696/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [29824/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [29952/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [30080/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [30208/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [30336/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [30464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [30592/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [30720/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 10 [30848/50000]\tLoss: 4.6078\tLR: 0.010000\n",
            "Training Epoch: 10 [30976/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 10 [31104/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [31232/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [31360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [31488/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [31616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [31744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [31872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [32000/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [32128/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [32256/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [32384/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [32512/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [32640/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [32768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [32896/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [33024/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [33152/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [33280/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [33408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [33536/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [33664/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [33792/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [33920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [34048/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [34176/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [34304/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [34432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [34560/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [34688/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [34816/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [34944/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [35072/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 10 [35200/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [35328/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [35456/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [35584/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [35712/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [35840/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [35968/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [36096/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [36224/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [36352/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [36480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [36608/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [36736/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [36864/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 10 [36992/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [37120/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [37248/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 10 [37376/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [37504/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [37632/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 10 [37760/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [37888/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [38016/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [38144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [38272/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [38400/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [38528/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [38656/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [38784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [38912/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [39040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [39168/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [39296/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 10 [39424/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [39552/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [39680/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [39808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [39936/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [40064/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [40192/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [40320/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [40448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [40576/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [40704/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [40832/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [40960/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [41088/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [41216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [41344/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [41472/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [41600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [41728/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [41856/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [41984/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [42112/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [42240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [42368/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [42496/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [42624/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [42752/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [42880/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [43008/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [43136/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [43264/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [43392/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [43520/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [43648/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [43776/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [43904/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [44032/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [44160/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [44288/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [44416/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [44544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [44672/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [44800/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [44928/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [45056/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [45184/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [45312/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [45440/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [45568/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [45696/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [45824/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [45952/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [46080/50000]\tLoss: 4.6077\tLR: 0.010000\n",
            "Training Epoch: 10 [46208/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [46336/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [46464/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 10 [46592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [46720/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [46848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [46976/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [47104/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [47232/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [47360/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [47488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [47616/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [47744/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [47872/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [48000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [48128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [48256/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [48384/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [48512/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [48640/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [48768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [48896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [49024/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [49152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [49280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [49408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [49536/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [49792/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [49920/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [50000/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "epoch 10 training time consumed: 54.77s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  45490 GiB |  45490 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  45190 GiB |  45189 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    300 GiB |    300 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  45490 GiB |  45490 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  45190 GiB |  45189 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    300 GiB |    300 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  45417 GiB |  45417 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  45116 GiB |  45116 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    300 GiB |    300 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  36224 GiB |  36223 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  35897 GiB |  35897 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    326 GiB |    326 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    2898 K  |    2897 K  |\n",
            "|       from large pool |      20    |     103    |    1376 K  |    1376 K  |\n",
            "|       from small pool |     384    |     506    |    1522 K  |    1521 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    2898 K  |    2897 K  |\n",
            "|       from large pool |      20    |     103    |    1376 K  |    1376 K  |\n",
            "|       from small pool |     384    |     506    |    1522 K  |    1521 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      22    |      41    |     818 K  |     818 K  |\n",
            "|       from large pool |       8    |      17    |     546 K  |     546 K  |\n",
            "|       from small pool |      14    |      33    |     272 K  |     272 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.71s\n",
            "\n",
            "saving weights file to checkpoint/googlenet/Tuesday_25_July_2023_11h_07m_22s/googlenet-10-regular.pth\n",
            "Training Epoch: 11 [128/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [384/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [512/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [896/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [1024/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [1152/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [1280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [1408/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [1536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [1664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [1792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [1920/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [2048/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 11 [2176/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [2304/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [2432/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [2560/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [2688/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [2816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [2944/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [3072/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [3200/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 11 [3328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [3456/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [3584/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [3712/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [3840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [3968/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [4096/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [4224/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [4352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [4480/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [4608/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [4736/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [4864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [4992/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [5120/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [5248/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [5376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [5504/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [5632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [5760/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [5888/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [6016/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [6144/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [6272/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [6400/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [6528/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [6656/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [6784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [6912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [7168/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [7296/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [7424/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [7552/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [7680/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [7808/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 11 [7936/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [8064/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [8192/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [8320/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 11 [8448/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [8576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [8704/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [8832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [8960/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [9088/50000]\tLoss: 4.6033\tLR: 0.010000\n",
            "Training Epoch: 11 [9216/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 11 [9344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [9472/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 11 [9600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [9728/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [9856/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [9984/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [10112/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [10240/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [10496/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [10624/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [10752/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [10880/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [11008/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [11136/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [11264/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [11392/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [11520/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 11 [11648/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [11776/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 11 [11904/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [12032/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [12160/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [12288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [12416/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [12544/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 11 [12672/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [12800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [12928/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [13056/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [13184/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [13312/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [13440/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [13568/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [13696/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [13824/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [13952/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [14080/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [14208/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [14336/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [14464/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [14592/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [14720/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [14848/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [14976/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [15104/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 11 [15232/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [15360/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [15488/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [15616/50000]\tLoss: 4.6086\tLR: 0.010000\n",
            "Training Epoch: 11 [15744/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [15872/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [16000/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [16128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [16256/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [16384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [16512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [16640/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [16768/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [16896/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [17024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [17152/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [17280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [17408/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [17536/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 11 [17664/50000]\tLoss: 4.6028\tLR: 0.010000\n",
            "Training Epoch: 11 [17792/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [17920/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [18048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [18176/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [18304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [18432/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [18560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [18688/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 11 [18816/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [18944/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [19072/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [19200/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [19328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [19456/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [19584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [19712/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [19840/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [19968/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [20096/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [20224/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [20352/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [20480/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [20608/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [20736/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 11 [20864/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [20992/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [21120/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [21248/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [21376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [21504/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 11 [21632/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [21760/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [21888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [22016/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 11 [22144/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [22272/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [22400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [22528/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [22656/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 11 [22784/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [22912/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [23040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [23168/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [23296/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [23424/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [23552/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [23680/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 11 [23808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [23936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [24064/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [24192/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [24320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [24448/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [24576/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [24704/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [24832/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [24960/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [25088/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [25216/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [25344/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [25472/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [25600/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [25728/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [25856/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [25984/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [26112/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [26240/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [26368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [26496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [26624/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [26752/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [26880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [27008/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [27136/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [27264/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [27392/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [27520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [27648/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [27776/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [27904/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [28032/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [28160/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [28288/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 11 [28416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [28544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [28672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [28800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [28928/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [29056/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 11 [29184/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [29312/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [29440/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [29568/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 11 [29696/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [29824/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [29952/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [30080/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 11 [30208/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [30336/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [30464/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [30592/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 11 [30720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [30848/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [30976/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [31104/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [31232/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [31360/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [31488/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [31616/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [31744/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [31872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [32000/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [32128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [32256/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [32384/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [32512/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [32640/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [32768/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [32896/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [33024/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [33152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [33280/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 11 [33408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [33536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [33664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [33792/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [33920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [34048/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [34176/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [34304/50000]\tLoss: 4.6078\tLR: 0.010000\n",
            "Training Epoch: 11 [34432/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [34560/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [34688/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [34816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [34944/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [35072/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [35200/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [35328/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [35456/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [35584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [35712/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [35840/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [35968/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [36096/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [36224/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [36352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [36480/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [36608/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [36736/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [36864/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [36992/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [37120/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [37248/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [37376/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [37504/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [37632/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [37760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [37888/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [38016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [38144/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [38272/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [38400/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [38528/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 11 [38656/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [38784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [38912/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [39040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [39168/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [39296/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [39424/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [39552/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [39680/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [39808/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [39936/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [40064/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [40192/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [40320/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [40448/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [40576/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [40704/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [40832/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [40960/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [41088/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [41216/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [41344/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [41472/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [41600/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [41728/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [41856/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [41984/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [42112/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [42240/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [42368/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [42496/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [42624/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [42752/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [42880/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [43008/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [43136/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [43264/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 11 [43392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [43520/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [43648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [43776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [43904/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [44032/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [44160/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [44288/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [44416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [44544/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [44672/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [44800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [44928/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [45056/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [45184/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [45312/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [45440/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [45568/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [45696/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [45824/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [45952/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [46080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [46208/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [46336/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [46464/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [46592/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [46720/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [46848/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [46976/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [47104/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [47232/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [47360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [47488/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [47616/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [47744/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [47872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [48000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [48128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [48256/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [48384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [48512/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [48640/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [48768/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [48896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [49024/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [49152/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [49408/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [49536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [49664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [49792/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [49920/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [50000/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "epoch 11 training time consumed: 54.96s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  50037 GiB |  50037 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  49706 GiB |  49706 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    330 GiB |    330 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  50037 GiB |  50037 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  49706 GiB |  49706 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    330 GiB |    330 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  49956 GiB |  49956 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  49626 GiB |  49626 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    330 GiB |    330 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  39846 GiB |  39846 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  39487 GiB |  39487 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    358 GiB |    358 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    3187 K  |    3187 K  |\n",
            "|       from large pool |      20    |     103    |    1513 K  |    1513 K  |\n",
            "|       from small pool |     384    |     506    |    1674 K  |    1673 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    3187 K  |    3187 K  |\n",
            "|       from large pool |      20    |     103    |    1513 K  |    1513 K  |\n",
            "|       from small pool |     384    |     506    |    1674 K  |    1673 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |     900 K  |     900 K  |\n",
            "|       from large pool |       8    |      17    |     600 K  |     600 K  |\n",
            "|       from small pool |      18    |      33    |     299 K  |     299 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.76s\n",
            "\n",
            "Training Epoch: 12 [128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [640/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [768/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [896/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [1024/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [1152/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [1280/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [1408/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [1536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [1664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [1920/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [2048/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [2176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [2304/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [2432/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [2560/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [2688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [2816/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [2944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [3072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [3200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [3328/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [3456/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [3584/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [3712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [3840/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [3968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [4096/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [4224/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [4352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [4480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [4608/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [4736/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [4864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [4992/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [5120/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [5248/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [5376/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [5504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [5632/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [5760/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [5888/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [6016/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [6144/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [6272/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [6400/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [6528/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [6656/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [6784/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [6912/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [7040/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [7168/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [7296/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [7424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [7552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [7680/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [7808/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [7936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [8064/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [8192/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [8320/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [8448/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [8576/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [8704/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [8832/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [8960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [9088/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [9216/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [9344/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [9472/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [9600/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [9728/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [9856/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [9984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [10112/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [10240/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [10496/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [10624/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [10752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [10880/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 12 [11008/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [11136/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [11264/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [11392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [11520/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [11648/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [11776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [11904/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [12032/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [12160/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [12288/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [12416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [12544/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [12672/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [12800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [12928/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [13056/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [13184/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [13312/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [13440/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [13568/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [13696/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [13824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [13952/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [14080/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [14208/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [14336/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [14464/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [14592/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [14720/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [14848/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [14976/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [15104/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [15232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [15360/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [15488/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [15616/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [15744/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [15872/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [16000/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [16128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [16256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [16384/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [16512/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [16640/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [16768/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 12 [16896/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [17024/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [17152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [17280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [17408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [17536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [17664/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [17792/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [17920/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [18048/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [18176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [18304/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [18432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [18560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [18688/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [18816/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [18944/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [19072/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [19200/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [19328/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [19456/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [19584/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [19712/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [19840/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [19968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [20096/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [20224/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [20352/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [20480/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [20608/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [20736/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [20864/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [20992/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [21120/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [21248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [21376/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [21504/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [21632/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [21760/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 12 [21888/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [22016/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [22144/50000]\tLoss: 4.6034\tLR: 0.010000\n",
            "Training Epoch: 12 [22272/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [22400/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [22528/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [22656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [22784/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [22912/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [23040/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [23168/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 12 [23296/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [23424/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 12 [23552/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [23680/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [23808/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [23936/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [24064/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [24192/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [24320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [24448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [24576/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [24704/50000]\tLoss: 4.6081\tLR: 0.010000\n",
            "Training Epoch: 12 [24832/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [24960/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [25088/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 12 [25216/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [25344/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [25472/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [25600/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [25728/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [25856/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [25984/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [26112/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [26240/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 12 [26368/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [26496/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [26624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [26752/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [26880/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [27008/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [27136/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [27264/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [27392/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [27520/50000]\tLoss: 4.6080\tLR: 0.010000\n",
            "Training Epoch: 12 [27648/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [27776/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [27904/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [28032/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [28160/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 12 [28288/50000]\tLoss: 4.6033\tLR: 0.010000\n",
            "Training Epoch: 12 [28416/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 12 [28544/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [28672/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [28800/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [28928/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [29056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [29184/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [29312/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [29440/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [29568/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [29696/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [29824/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [29952/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [30080/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [30208/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [30336/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 12 [30464/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [30592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [30720/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [30848/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [30976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [31104/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [31232/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 12 [31360/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [31488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [31616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [31744/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [31872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [32000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [32128/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [32256/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 12 [32384/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [32512/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [32640/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [32768/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [32896/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [33024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [33152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [33280/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [33408/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [33536/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [33664/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [33792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [33920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [34048/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [34176/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [34304/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [34432/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [34560/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [34688/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [34816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [34944/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [35072/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [35200/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [35328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [35456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [35584/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [35712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [35840/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [35968/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [36096/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [36224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [36352/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [36480/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [36608/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [36736/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [36864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [36992/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [37120/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [37248/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [37376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [37504/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [37632/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [37760/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [37888/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [38016/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [38144/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [38272/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [38400/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [38528/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [38656/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [38784/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [38912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [39040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [39168/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [39296/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [39424/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [39552/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [39680/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 12 [39808/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [39936/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [40064/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [40192/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 12 [40320/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [40448/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [40576/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [40704/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [40832/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [40960/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [41088/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [41216/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [41472/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [41600/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [41728/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [41856/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [41984/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [42112/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [42240/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [42368/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [42496/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [42624/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [42752/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [42880/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [43008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [43136/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [43264/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [43392/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [43520/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [43648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [43776/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [43904/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [44032/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [44160/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [44288/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [44416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [44544/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [44672/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 12 [44800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [44928/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [45056/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [45184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [45312/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [45440/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [45568/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [45696/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [45824/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [45952/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [46080/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [46208/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [46336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [46464/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [46592/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [46720/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 12 [46848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [46976/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [47104/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [47232/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [47360/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [47488/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [47616/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [47744/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [47872/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [48000/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [48128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [48256/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [48384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [48512/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [48640/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [48768/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [48896/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [49024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [49152/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [49280/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [49408/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [49536/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [49664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [49792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [49920/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [50000/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "epoch 12 training time consumed: 55.03s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  54584 GiB |  54584 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  54223 GiB |  54223 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    360 GiB |    360 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  54584 GiB |  54584 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  54223 GiB |  54223 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    360 GiB |    360 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  54496 GiB |  54496 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  54136 GiB |  54136 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    360 GiB |    360 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  43469 GiB |  43468 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  43077 GiB |  43077 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    391 GiB |    391 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    3477 K  |    3477 K  |\n",
            "|       from large pool |      20    |     103    |    1651 K  |    1651 K  |\n",
            "|       from small pool |     384    |     506    |    1826 K  |    1825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    3477 K  |    3477 K  |\n",
            "|       from large pool |      20    |     103    |    1651 K  |    1651 K  |\n",
            "|       from small pool |     384    |     506    |    1826 K  |    1825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      24    |      41    |     984 K  |     984 K  |\n",
            "|       from large pool |       8    |      17    |     655 K  |     655 K  |\n",
            "|       from small pool |      16    |      33    |     328 K  |     328 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.85s\n",
            "\n",
            "Training Epoch: 13 [128/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [384/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [512/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [768/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [1024/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [1152/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [1280/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [1408/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [1536/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [1664/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [1792/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [1920/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [2048/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [2176/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [2304/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [2432/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [2560/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [2688/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [2816/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [2944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [3072/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [3200/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [3328/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [3456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [3584/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [3712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [3840/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [3968/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [4096/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [4224/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [4352/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [4480/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [4608/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [4736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [4864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [4992/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [5120/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [5248/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [5376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [5504/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [5632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [5760/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [5888/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [6016/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 13 [6144/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [6272/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [6400/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [6528/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [6656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [6784/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [6912/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [7040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [7168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [7296/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [7424/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [7552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [7680/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [7808/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [7936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [8064/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [8192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [8320/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [8448/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [8576/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [8704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [8832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [8960/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [9088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [9216/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [9344/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [9472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [9600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [9728/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [9856/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [9984/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [10112/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [10240/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [10368/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [10496/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [10624/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [10752/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [10880/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [11008/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [11136/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [11264/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [11392/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [11520/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [11648/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [11776/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [11904/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [12032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [12160/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [12288/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [12416/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 13 [12544/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [12672/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [12800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [12928/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [13056/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [13184/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [13312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [13440/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [13568/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [13696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [13824/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [13952/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [14080/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [14208/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [14336/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 13 [14464/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [14592/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [14720/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 13 [14848/50000]\tLoss: 4.6033\tLR: 0.010000\n",
            "Training Epoch: 13 [14976/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [15104/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [15232/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [15360/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [15488/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [15616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [15744/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [15872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [16000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [16128/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [16256/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [16384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [16512/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [16640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [16768/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [16896/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [17024/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [17152/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [17280/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [17408/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [17536/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [17664/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [17792/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [17920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [18048/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [18176/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [18304/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [18432/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [18560/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [18688/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [18816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [18944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [19072/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [19200/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [19328/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [19456/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [19584/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [19712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [19840/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [19968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [20096/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [20224/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [20352/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [20480/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [20608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [20736/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [20864/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [20992/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [21120/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [21248/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [21376/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [21504/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [21632/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [21760/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [21888/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [22016/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [22144/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [22272/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [22400/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [22528/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [22656/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [22784/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [22912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [23040/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [23168/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 13 [23296/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [23424/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [23552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [23680/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [23808/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [23936/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [24064/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [24192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [24320/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [24448/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [24576/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [24704/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [24832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [24960/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [25088/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 13 [25216/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [25344/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 13 [25472/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [25600/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [25728/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [25856/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [25984/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [26112/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [26240/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [26368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [26496/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [26624/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [26752/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [26880/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [27008/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [27136/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [27264/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [27392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [27520/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [27648/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [27776/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [27904/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [28032/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [28160/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 13 [28288/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [28416/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [28544/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [28672/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [28800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [28928/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [29056/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [29184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [29312/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [29440/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [29568/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [29696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [29824/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [29952/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [30080/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [30208/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [30336/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [30464/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [30592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [30720/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [30848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [30976/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [31104/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [31232/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [31360/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [31488/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [31616/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [31744/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [31872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [32000/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [32128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [32256/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [32384/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [32512/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [32640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [32768/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [32896/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [33024/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [33152/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 13 [33280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [33408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [33536/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 13 [33664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [33792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [33920/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 13 [34048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [34176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [34304/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [34432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [34560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [34688/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [34816/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [34944/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [35072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [35200/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 13 [35328/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [35456/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [35584/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [35712/50000]\tLoss: 4.6030\tLR: 0.010000\n",
            "Training Epoch: 13 [35840/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [35968/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [36096/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [36224/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [36352/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [36480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [36608/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [36736/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [36864/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [36992/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [37120/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [37248/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [37376/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [37504/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [37632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [37760/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [37888/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [38016/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 13 [38144/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [38272/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [38400/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [38528/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [38656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [38784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [38912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [39040/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [39168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [39296/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [39424/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [39552/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [39680/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [39808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [39936/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [40064/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [40192/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [40320/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [40448/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 13 [40576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [40704/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [40832/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [40960/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [41088/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [41216/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [41472/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [41600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [41728/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [41856/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [41984/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [42112/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [42240/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [42368/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [42496/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [42624/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [42752/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [42880/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [43008/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [43136/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [43264/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [43392/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [43520/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [43648/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [43776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [43904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [44032/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [44160/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [44288/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [44416/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [44544/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [44672/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [44800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [44928/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [45056/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [45184/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [45312/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 13 [45440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [45568/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [45696/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [45824/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [45952/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [46080/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [46208/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [46336/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [46464/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [46592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [46720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [46848/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [46976/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [47104/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [47232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [47360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [47488/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [47616/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [47744/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [47872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [48000/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [48128/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [48256/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [48384/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [48512/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [48640/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [48768/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [48896/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [49024/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [49152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [49280/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [49408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [49536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [49664/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [49792/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [49920/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [50000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "epoch 13 training time consumed: 54.65s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  59131 GiB |  59131 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  58740 GiB |  58740 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    390 GiB |    390 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  59131 GiB |  59131 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  58740 GiB |  58740 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    390 GiB |    390 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  59036 GiB |  59036 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  58645 GiB |  58645 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    390 GiB |    390 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  47091 GiB |  47091 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  46667 GiB |  46667 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    423 GiB |    423 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    3767 K  |    3766 K  |\n",
            "|       from large pool |      20    |     103    |    1788 K  |    1788 K  |\n",
            "|       from small pool |     384    |     506    |    1978 K  |    1977 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    3767 K  |    3766 K  |\n",
            "|       from large pool |      20    |     103    |    1788 K  |    1788 K  |\n",
            "|       from small pool |     384    |     506    |    1978 K  |    1977 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |    1066 K  |    1066 K  |\n",
            "|       from large pool |       8    |      17    |     710 K  |     710 K  |\n",
            "|       from small pool |      18    |      33    |     356 K  |     356 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:4.29s\n",
            "\n",
            "Training Epoch: 14 [128/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [256/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [384/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [1024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [1152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [1280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [1408/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [1536/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [1664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [1792/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [1920/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [2048/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [2176/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [2304/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [2432/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [2560/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [2688/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [2816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [2944/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [3072/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [3200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [3328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [3456/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [3584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [3712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [3840/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [3968/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [4096/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [4224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [4352/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [4480/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [4608/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [4736/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [4864/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [4992/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [5120/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [5248/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [5376/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [5504/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [5632/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [5760/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [5888/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [6016/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [6144/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [6272/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [6400/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [6528/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [6656/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [6784/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [6912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [7040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [7168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [7296/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [7424/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [7552/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [7680/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [7808/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [7936/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [8064/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [8192/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [8320/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [8448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [8576/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [8704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [8832/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [8960/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [9088/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [9216/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [9344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [9472/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [9600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [9728/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [9856/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [9984/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [10112/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [10240/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [10368/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [10496/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [10624/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [10752/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [10880/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [11008/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [11136/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [11264/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [11392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [11520/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [11648/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [11776/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [11904/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [12032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [12160/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [12288/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [12416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [12544/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [12672/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [12800/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [12928/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [13056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [13184/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 14 [13312/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [13440/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [13568/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [13696/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [13824/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [13952/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 14 [14080/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [14208/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [14336/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [14464/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [14592/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [14720/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [14848/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [14976/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [15104/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [15232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [15360/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [15488/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [15616/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [15744/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [15872/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [16000/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [16128/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [16256/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [16384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [16512/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [16640/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [16768/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [16896/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [17024/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [17152/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [17280/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 14 [17408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [17536/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 14 [17664/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [17792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [17920/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [18048/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [18176/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [18304/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [18432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [18560/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [18688/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 14 [18816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [18944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [19072/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [19200/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [19328/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [19456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [19584/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [19712/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [19840/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [19968/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [20096/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [20224/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [20352/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [20480/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [20608/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [20736/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [20864/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [20992/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [21120/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [21248/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [21376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [21504/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [21632/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [21760/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [21888/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [22016/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [22144/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [22272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [22400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [22528/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [22656/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [22784/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [22912/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [23040/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [23168/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [23296/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 14 [23424/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [23552/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [23680/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [23808/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [23936/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [24064/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 14 [24192/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [24320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [24448/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [24576/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 14 [24704/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [24832/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [24960/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [25088/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [25216/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 14 [25344/50000]\tLoss: 4.6092\tLR: 0.010000\n",
            "Training Epoch: 14 [25472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [25600/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [25728/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [25856/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [25984/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 14 [26112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [26240/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [26368/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [26496/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [26624/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [26752/50000]\tLoss: 4.6080\tLR: 0.010000\n",
            "Training Epoch: 14 [26880/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [27008/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [27136/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [27264/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [27392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [27520/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [27648/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [27776/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [27904/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [28032/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [28160/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [28288/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [28416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [28544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [28672/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [28800/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [28928/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [29056/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [29184/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [29312/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [29440/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 14 [29568/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [29696/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [29824/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 14 [29952/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [30080/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [30208/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [30336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [30464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [30592/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [30720/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [30848/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [30976/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [31104/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [31232/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [31360/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [31488/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 14 [31616/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [31744/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [31872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [32000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [32128/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [32256/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [32384/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [32512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [32640/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [32768/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [32896/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [33024/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [33152/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 14 [33280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [33408/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [33536/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 14 [33664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [33792/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [33920/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [34048/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [34176/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [34304/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [34432/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [34560/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [34688/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [34816/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [34944/50000]\tLoss: 4.6077\tLR: 0.010000\n",
            "Training Epoch: 14 [35072/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [35200/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [35328/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [35456/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [35584/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 14 [35712/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [35840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [35968/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [36096/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [36224/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [36352/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [36480/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [36608/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [36736/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [36864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [36992/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [37120/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [37248/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [37376/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [37504/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [37632/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [37760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [37888/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [38016/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [38144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [38272/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [38400/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 14 [38528/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [38656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [38784/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [38912/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 14 [39040/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [39168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [39296/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [39424/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [39552/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 14 [39680/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [39808/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [39936/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [40064/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [40192/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [40320/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [40448/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [40576/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [40704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [40832/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [40960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [41088/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [41216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [41344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [41472/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [41600/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [41728/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [41856/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [41984/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [42112/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [42240/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [42368/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [42496/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 14 [42624/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [42752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [42880/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [43008/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [43136/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [43264/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [43392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [43520/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [43648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [43776/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 14 [43904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [44032/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [44160/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [44288/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [44416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [44544/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [44672/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [44800/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [44928/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [45056/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [45184/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [45312/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [45440/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [45568/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [45696/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [45824/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [45952/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [46080/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [46208/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [46336/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [46464/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [46592/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [46720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [46848/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [46976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [47104/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [47232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [47360/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [47488/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [47616/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [47744/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [47872/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [48000/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [48128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [48256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [48384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [48512/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [48640/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [48768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [48896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [49024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [49152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [49280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [49408/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [49536/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [49664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [49792/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [49920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [50000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "epoch 14 training time consumed: 54.59s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  63678 GiB |  63678 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  63257 GiB |  63257 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    420 GiB |    420 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  63678 GiB |  63678 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  63257 GiB |  63257 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    420 GiB |    420 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  63576 GiB |  63575 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  63155 GiB |  63155 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    420 GiB |    420 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  50714 GiB |  50713 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  50257 GiB |  50257 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    456 GiB |    456 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    4056 K  |    4056 K  |\n",
            "|       from large pool |      20    |     103    |    1926 K  |    1926 K  |\n",
            "|       from small pool |     384    |     506    |    2130 K  |    2130 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    4056 K  |    4056 K  |\n",
            "|       from large pool |      20    |     103    |    1926 K  |    1926 K  |\n",
            "|       from small pool |     384    |     506    |    2130 K  |    2130 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      23    |      41    |    1147 K  |    1147 K  |\n",
            "|       from large pool |       8    |      17    |     764 K  |     764 K  |\n",
            "|       from small pool |      15    |      33    |     383 K  |     383 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.98s\n",
            "\n",
            "Training Epoch: 15 [128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [256/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [512/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [640/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [768/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [896/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [1024/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [1152/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [1280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [1408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [1536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [1664/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [1792/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [1920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [2048/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [2176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [2304/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [2432/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [2560/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [2688/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [2816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [2944/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [3072/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [3200/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [3328/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [3456/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [3584/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [3712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [3840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [3968/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [4096/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [4224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [4352/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [4480/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [4608/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [4736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [4864/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [4992/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [5120/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [5248/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [5376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [5504/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [5632/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [5760/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [5888/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [6016/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [6144/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [6272/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [6400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [6528/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [6656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [6784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [6912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [7040/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [7168/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [7296/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [7424/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [7552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [7680/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [7808/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [7936/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [8064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [8192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [8320/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [8448/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [8576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [8704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [8832/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [8960/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [9088/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [9216/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [9344/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [9472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [9600/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [9728/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [9856/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [9984/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [10112/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [10240/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [10368/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [10496/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [10624/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [10752/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [10880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [11008/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [11136/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [11264/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [11392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [11520/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [11648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [11776/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [11904/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [12032/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [12160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [12288/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [12416/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [12544/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [12672/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [12800/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [12928/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [13056/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [13184/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [13312/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [13440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [13568/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [13696/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 15 [13824/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [13952/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [14080/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [14208/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [14336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [14464/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [14592/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [14720/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [14848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [14976/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [15104/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [15232/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [15360/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [15488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [15616/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [15744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [15872/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 15 [16000/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [16128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [16256/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [16384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [16512/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 15 [16640/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [16768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [16896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [17024/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [17152/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [17280/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [17408/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [17536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [17664/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 15 [17792/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [17920/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [18048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [18176/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [18304/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [18432/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [18560/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [18688/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [18816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [18944/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [19072/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 15 [19200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [19328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [19456/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [19584/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [19712/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [19840/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [19968/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [20096/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [20224/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [20352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [20480/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [20608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [20736/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [20864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [20992/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [21120/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [21248/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [21376/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [21504/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [21632/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [21760/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [21888/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 15 [22016/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [22144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [22272/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [22400/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [22528/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [22656/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [22784/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [22912/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [23040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [23168/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [23296/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 15 [23424/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [23552/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [23680/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [23808/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [23936/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [24064/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [24192/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [24320/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [24448/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [24576/50000]\tLoss: 4.6078\tLR: 0.010000\n",
            "Training Epoch: 15 [24704/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [24832/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 15 [24960/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [25088/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [25216/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [25344/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [25472/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 15 [25600/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [25728/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [25856/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [25984/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [26112/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [26240/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [26368/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [26496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [26624/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [26752/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [26880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [27008/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [27136/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [27264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [27392/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [27520/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [27648/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [27776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [27904/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [28032/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [28160/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [28288/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [28416/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [28544/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [28672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [28800/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 15 [28928/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [29056/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [29184/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [29312/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 15 [29440/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [29568/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [29696/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [29824/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [29952/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [30080/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [30208/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [30336/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [30464/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [30592/50000]\tLoss: 4.6080\tLR: 0.010000\n",
            "Training Epoch: 15 [30720/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [30848/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [30976/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [31104/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [31232/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [31360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [31488/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [31616/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 15 [31744/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [31872/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [32000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [32128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [32256/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [32384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [32512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [32640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [32768/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [32896/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [33024/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 15 [33152/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [33280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [33408/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [33536/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [33664/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [33792/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [33920/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [34048/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [34176/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [34304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [34432/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 15 [34560/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 15 [34688/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [34816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [34944/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [35072/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [35200/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [35328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [35456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [35584/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [35712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [35840/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [35968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [36096/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [36224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [36352/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 15 [36480/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [36608/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [36736/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [36864/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [36992/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [37120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [37248/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [37376/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [37504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [37632/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [37760/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [37888/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [38016/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [38144/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 15 [38272/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [38400/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [38528/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [38656/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [38784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [38912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [39040/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [39168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [39296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [39424/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [39552/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [39680/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [39808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [39936/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [40064/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [40192/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [40320/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [40448/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [40576/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [40704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [40832/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [40960/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [41088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [41216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [41344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [41472/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [41600/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [41728/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [41856/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [41984/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [42112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [42240/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [42368/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [42496/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [42624/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [42752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [42880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [43008/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [43136/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [43264/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [43392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [43520/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [43648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [43776/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [43904/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [44032/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [44160/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [44288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [44416/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [44544/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [44672/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [44800/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [44928/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [45056/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 15 [45184/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [45312/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [45440/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [45568/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [45696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [45824/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [45952/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [46080/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [46208/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [46336/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [46464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [46592/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [46720/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [46848/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [46976/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [47104/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [47232/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [47360/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [47488/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [47616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [47744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [47872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [48000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [48128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [48256/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [48384/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [48512/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [48640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [48768/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [48896/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49024/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [49152/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [49280/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49536/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49664/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [49792/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [50000/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "epoch 15 training time consumed: 54.91s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  68225 GiB |  68225 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  67774 GiB |  67774 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    450 GiB |    450 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  68225 GiB |  68225 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  67774 GiB |  67774 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    450 GiB |    450 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  68115 GiB |  68115 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  67665 GiB |  67664 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    450 GiB |    450 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  54336 GiB |  54336 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  53847 GiB |  53846 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    489 GiB |    489 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    4346 K  |    4346 K  |\n",
            "|       from large pool |      20    |     103    |    2064 K  |    2064 K  |\n",
            "|       from small pool |     384    |     506    |    2282 K  |    2282 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    4346 K  |    4346 K  |\n",
            "|       from large pool |      20    |     103    |    2064 K  |    2064 K  |\n",
            "|       from small pool |     384    |     506    |    2282 K  |    2282 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |    1228 K  |    1228 K  |\n",
            "|       from large pool |       8    |      17    |     819 K  |     819 K  |\n",
            "|       from small pool |      18    |      33    |     409 K  |     409 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.73s\n",
            "\n",
            "Training Epoch: 16 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [256/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [384/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [512/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [640/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [1024/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [1152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [1280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [1408/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [1536/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [1664/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [1792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [1920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [2048/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [2176/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [2304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [2432/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [2560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [2688/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [2816/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [2944/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [3072/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [3200/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [3328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [3456/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [3584/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [3712/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [3840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [3968/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [4096/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [4224/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [4352/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [4480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [4608/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [4736/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [4864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [4992/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [5120/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [5248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [5376/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [5504/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 16 [5632/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [5760/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [5888/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [6016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [6144/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [6272/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [6400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [6528/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [6656/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 16 [6784/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [6912/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [7168/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [7296/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 16 [7424/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [7552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [7680/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [7808/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [7936/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [8064/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [8192/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [8320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [8448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [8576/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [8704/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [8832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [8960/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [9088/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [9216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [9344/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [9472/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [9600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [9728/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [9856/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [9984/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [10112/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [10240/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [10368/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [10496/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [10624/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [10752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [10880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [11008/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [11136/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [11264/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 16 [11392/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [11520/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [11648/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [11776/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 16 [11904/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [12032/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [12160/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [12288/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [12416/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [12544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [12672/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [12800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [12928/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [13056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [13184/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [13312/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [13440/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [13568/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [13696/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [13824/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [13952/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [14080/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [14208/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [14336/50000]\tLoss: 4.6079\tLR: 0.010000\n",
            "Training Epoch: 16 [14464/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [14592/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [14720/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [14848/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [14976/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [15104/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [15232/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [15360/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [15488/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [15616/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [15744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [15872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [16000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [16128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [16256/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [16384/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [16512/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [16640/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 16 [16768/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [16896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [17024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [17152/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [17280/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [17408/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [17536/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [17664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [17792/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [17920/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [18048/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [18176/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [18304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [18432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [18560/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [18688/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [18816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [18944/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [19072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [19200/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [19328/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [19456/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [19584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [19712/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [19840/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [19968/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [20096/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [20224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [20352/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [20480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [20608/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [20736/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [20864/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [20992/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [21120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [21248/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [21376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [21504/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [21632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [21760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [21888/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [22016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [22144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [22272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [22400/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [22528/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [22656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [22784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [22912/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 16 [23040/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [23168/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 16 [23296/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [23424/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [23552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [23680/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [23808/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [23936/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [24064/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [24192/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 16 [24320/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [24448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [24576/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [24704/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [24832/50000]\tLoss: 4.6086\tLR: 0.010000\n",
            "Training Epoch: 16 [24960/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [25088/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [25216/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [25344/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [25472/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [25600/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [25728/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 16 [25856/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [25984/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [26112/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [26240/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [26368/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [26496/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [26624/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [26752/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [26880/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [27008/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [27136/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 16 [27264/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [27392/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [27520/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [27648/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [27776/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [27904/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [28032/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [28160/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [28288/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [28416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [28544/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [28672/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [28800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [28928/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [29056/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [29184/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [29312/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [29440/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [29568/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [29696/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [29824/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [29952/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [30080/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 16 [30208/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [30336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [30464/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [30592/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [30720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [30848/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [30976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [31104/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [31232/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [31360/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [31488/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [31616/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [31744/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [31872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [32000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [32128/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [32256/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [32384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [32512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [32640/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 16 [32768/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [32896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [33024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [33152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [33280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [33408/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [33536/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [33664/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [33792/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [33920/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [34048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [34176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [34304/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [34432/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [34560/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [34688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [34816/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [34944/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [35072/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [35200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [35328/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 16 [35456/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [35584/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [35712/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [35840/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [35968/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [36096/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [36224/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [36352/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [36480/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [36608/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [36736/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [36864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [36992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [37120/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [37248/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [37376/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [37504/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [37632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [37760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [37888/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [38016/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [38144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [38272/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [38400/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 16 [38528/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [38656/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [38784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [38912/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [39040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [39168/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [39296/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [39424/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 16 [39552/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [39680/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [39808/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [39936/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [40064/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [40192/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [40320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [40448/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [40576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [40704/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [40832/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [40960/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [41088/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [41216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [41344/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [41472/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [41600/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [41728/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [41856/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [41984/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [42112/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [42240/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [42368/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [42496/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [42624/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [42752/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [42880/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [43008/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [43136/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [43264/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [43392/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [43520/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [43648/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [43776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [43904/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [44032/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [44160/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [44288/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [44416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [44544/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [44672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [44800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [44928/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [45056/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [45184/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [45312/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [45440/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [45568/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [45696/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [45824/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [45952/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [46080/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [46208/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [46336/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [46464/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [46592/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [46720/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [46848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [46976/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [47104/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [47232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [47360/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [47488/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [47616/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [47744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [47872/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [48000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [48128/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [48256/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [48384/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [48512/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [48640/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [48768/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [48896/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [49024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [49152/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [49280/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [49408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [49536/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [49664/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [49792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [49920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [50000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "epoch 16 training time consumed: 54.53s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  72772 GiB |  72772 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  72291 GiB |  72291 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    481 GiB |    481 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  72772 GiB |  72772 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  72291 GiB |  72291 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    481 GiB |    481 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  72655 GiB |  72655 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  72174 GiB |  72174 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    480 GiB |    480 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  57958 GiB |  57958 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  57436 GiB |  57436 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    521 GiB |    521 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    4636 K  |    4635 K  |\n",
            "|       from large pool |      20    |     103    |    2201 K  |    2201 K  |\n",
            "|       from small pool |     384    |     506    |    2434 K  |    2434 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    4636 K  |    4635 K  |\n",
            "|       from large pool |      20    |     103    |    2201 K  |    2201 K  |\n",
            "|       from small pool |     384    |     506    |    2434 K  |    2434 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      22    |      41    |    1310 K  |    1310 K  |\n",
            "|       from large pool |       8    |      17    |     874 K  |     874 K  |\n",
            "|       from small pool |      14    |      33    |     436 K  |     435 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.71s\n",
            "\n",
            "Training Epoch: 17 [128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [512/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [768/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [1024/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [1152/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [1280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [1408/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [1536/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [1664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [1792/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [1920/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [2176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [2432/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [2560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2688/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [2816/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2944/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [3072/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [3200/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [3328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [3456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [3584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [3712/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [3840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [3968/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [4096/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [4224/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [4352/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [4480/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [4608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [4736/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [4864/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [4992/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [5120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [5248/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 17 [5376/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [5504/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [5632/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [5760/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [5888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [6016/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [6144/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [6272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [6400/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [6528/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [6656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [6784/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [6912/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [7040/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [7168/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [7296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [7424/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [7552/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [7680/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [7808/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [7936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [8064/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [8192/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [8320/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 17 [8448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [8576/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [8704/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 17 [8832/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [8960/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [9088/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [9216/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [9344/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [9472/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [9600/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [9728/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [9856/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [9984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [10112/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [10240/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [10368/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [10496/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [10624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [10752/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [10880/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [11008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [11136/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [11264/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [11392/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [11520/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 17 [11648/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [11776/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [11904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [12032/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [12160/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [12288/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [12416/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [12544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [12672/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [12800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [12928/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [13056/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [13184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [13312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [13440/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 17 [13568/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [13696/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [13824/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [13952/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [14080/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [14208/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [14336/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [14464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [14592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [14720/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [14848/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [14976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [15104/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [15232/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [15360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [15488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [15616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [15744/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [15872/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 17 [16000/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [16128/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [16256/50000]\tLoss: 4.6084\tLR: 0.010000\n",
            "Training Epoch: 17 [16384/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [16512/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [16640/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [16768/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [16896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [17024/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [17152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [17280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [17408/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [17536/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [17664/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [17792/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [17920/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [18048/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [18176/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [18304/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [18432/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [18560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [18688/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [18816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [18944/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [19072/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [19200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [19328/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [19456/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [19584/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [19712/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [19840/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [19968/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [20096/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [20224/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [20352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [20480/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [20608/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [20736/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [20864/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [20992/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [21120/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [21248/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [21376/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [21504/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 17 [21632/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [21760/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [21888/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [22016/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [22144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [22272/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [22400/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [22528/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [22656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [22784/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [22912/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [23040/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [23168/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [23296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [23424/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [23552/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [23680/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [23808/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [23936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [24064/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [24192/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [24320/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [24448/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [24576/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [24704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [24832/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [24960/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [25088/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [25216/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [25344/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [25472/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [25600/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [25728/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [25856/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [25984/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [26112/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [26240/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [26368/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [26496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [26624/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [26752/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 17 [26880/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [27008/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [27136/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [27264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [27392/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [27520/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [27648/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [27776/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [27904/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [28032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [28160/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [28288/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 17 [28416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [28544/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [28672/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [28800/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [28928/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [29056/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [29184/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 17 [29312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [29440/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [29568/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [29696/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [29824/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [29952/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [30080/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [30208/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [30336/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [30464/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [30592/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [30720/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [30848/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [30976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [31104/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [31232/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [31360/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [31488/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [31616/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [31744/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [31872/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [32000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [32128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [32256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [32384/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [32512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [32640/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [32768/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [32896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [33024/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [33152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [33280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [33408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [33536/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [33664/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [33792/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [33920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [34048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [34176/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [34304/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [34432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [34560/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 17 [34688/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [34816/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 17 [34944/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [35072/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [35200/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [35328/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [35456/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [35584/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [35712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [35840/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [35968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [36096/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 17 [36224/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [36352/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [36480/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [36608/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [36736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [36864/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [36992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [37120/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [37248/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [37376/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [37504/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [37632/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [37760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [37888/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [38016/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [38144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [38272/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [38400/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [38528/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [38656/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [38784/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [38912/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [39040/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 17 [39168/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [39296/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [39424/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [39552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [39680/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [39808/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [39936/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [40064/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [40192/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [40320/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [40448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [40576/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [40704/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [40832/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [40960/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [41088/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [41216/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [41344/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [41472/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [41600/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [41728/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [41856/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [41984/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [42112/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 17 [42240/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [42368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [42496/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [42624/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [42752/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [42880/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [43008/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [43136/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [43264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [43392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [43520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [43648/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [43776/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [43904/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [44032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [44160/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [44288/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [44416/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [44544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [44672/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [44800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [44928/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [45056/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [45184/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [45312/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [45440/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [45568/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [45696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [45824/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [45952/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [46080/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [46208/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [46336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [46464/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [46592/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [46720/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [46848/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [46976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [47104/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [47232/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [47360/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [47488/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [47616/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [47744/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [47872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [48000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [48128/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [48256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [48384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [48512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [48640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [48768/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [48896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [49024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [49152/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [49280/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [49408/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [49536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [49792/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [49920/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [50000/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "epoch 17 training time consumed: 55.29s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  77319 GiB |  77319 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  76808 GiB |  76808 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    511 GiB |    511 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  77319 GiB |  77319 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  76808 GiB |  76808 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    511 GiB |    511 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  77195 GiB |  77195 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  76684 GiB |  76684 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    510 GiB |    510 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  61581 GiB |  61581 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  61026 GiB |  61026 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    554 GiB |    554 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    4926 K  |    4925 K  |\n",
            "|       from large pool |      20    |     103    |    2339 K  |    2339 K  |\n",
            "|       from small pool |     384    |     506    |    2586 K  |    2586 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    4926 K  |    4925 K  |\n",
            "|       from large pool |      20    |     103    |    2339 K  |    2339 K  |\n",
            "|       from small pool |     384    |     506    |    2586 K  |    2586 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |    1391 K  |    1391 K  |\n",
            "|       from large pool |       8    |      17    |     928 K  |     928 K  |\n",
            "|       from small pool |      18    |      33    |     463 K  |     463 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.93s\n",
            "\n",
            "Training Epoch: 18 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [512/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [640/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [768/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [1024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [1152/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [1280/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [1408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [1536/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [1664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [1792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [1920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [2048/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [2176/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [2304/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [2432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [2560/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [2688/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [2816/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [2944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [3072/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [3200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [3328/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [3456/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [3584/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [3712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [3840/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [3968/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [4096/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [4224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [4352/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [4480/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [4608/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [4736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [4864/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [4992/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [5120/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [5248/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [5376/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [5504/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [5632/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [5760/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [5888/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [6016/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [6144/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [6272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [6400/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [6528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [6656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [6784/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [6912/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [7040/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [7168/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [7296/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 18 [7424/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [7552/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [7680/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [7808/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 18 [7936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [8064/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [8192/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [8320/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [8448/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [8576/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [8704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [8832/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [8960/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [9088/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [9216/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [9344/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [9472/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [9600/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [9728/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [9856/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [9984/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [10112/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [10240/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [10368/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [10496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [10624/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [10752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [10880/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [11008/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [11264/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [11392/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [11520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [11648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [11776/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [11904/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [12032/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [12160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [12288/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [12416/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [12544/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [12672/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [12800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [12928/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [13056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [13184/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [13312/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [13440/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 18 [13568/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 18 [13696/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [13824/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [13952/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [14080/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 18 [14208/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 18 [14336/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [14464/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [14592/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [14720/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [14848/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [14976/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [15104/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [15232/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [15360/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [15488/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [15616/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [15744/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [15872/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [16000/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [16128/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [16256/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 18 [16384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [16512/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 18 [16640/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [16768/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [16896/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [17024/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [17152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [17280/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [17408/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 18 [17536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [17664/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [17792/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 18 [17920/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [18048/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [18176/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [18304/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [18432/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 18 [18560/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [18688/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [18816/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [18944/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [19072/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [19200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [19328/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 18 [19456/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [19584/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [19712/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [19840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [19968/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [20096/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 18 [20224/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 18 [20352/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [20480/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [20608/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [20736/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [20864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [20992/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [21120/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [21248/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 18 [21376/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [21504/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [21632/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [21760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [21888/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [22016/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [22144/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [22272/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [22400/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [22528/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [22656/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [22784/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [22912/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [23040/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [23168/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [23296/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [23424/50000]\tLoss: 4.6080\tLR: 0.010000\n",
            "Training Epoch: 18 [23552/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 18 [23680/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [23808/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [23936/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [24064/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [24192/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [24320/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [24448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [24576/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [24704/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [24832/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [24960/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [25088/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [25216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [25344/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [25472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [25600/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [25728/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [25856/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [25984/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [26112/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [26240/50000]\tLoss: 4.6088\tLR: 0.010000\n",
            "Training Epoch: 18 [26368/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [26496/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [26624/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [26752/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [26880/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [27008/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [27136/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [27264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [27392/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [27520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [27648/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [27776/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [27904/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [28032/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 18 [28160/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 18 [28288/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [28416/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [28544/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [28672/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [28800/50000]\tLoss: 4.6077\tLR: 0.010000\n",
            "Training Epoch: 18 [28928/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [29056/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [29184/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [29312/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [29440/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [29568/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [29696/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [29824/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [29952/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [30080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [30208/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [30336/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [30464/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [30592/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [30720/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [30848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [30976/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [31104/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [31232/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [31360/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [31488/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [31616/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [31744/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [31872/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [32000/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [32128/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [32256/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [32384/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [32512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [32640/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [32768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [32896/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [33024/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [33152/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [33280/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [33408/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [33536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [33664/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [33792/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [33920/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 18 [34048/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [34176/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [34304/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 18 [34432/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [34560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [34688/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [34816/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [34944/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 18 [35072/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [35200/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [35328/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [35456/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [35584/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [35712/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [35840/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [35968/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [36096/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [36224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [36352/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [36480/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 18 [36608/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [36736/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [36864/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [36992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [37120/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [37248/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [37376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [37504/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [37632/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [37760/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 18 [37888/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [38016/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [38144/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [38272/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [38400/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 18 [38528/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [38656/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [38784/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [38912/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [39040/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [39168/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [39296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [39424/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [39552/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [39680/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [39808/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [39936/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [40064/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [40192/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [40320/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [40448/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [40576/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [40704/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [40832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [40960/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [41088/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [41216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [41344/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [41472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [41600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [41728/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [41856/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [41984/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [42112/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [42240/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [42368/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [42496/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [42624/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [42752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [42880/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [43008/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [43136/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [43264/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [43392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [43520/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [43648/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [43776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [43904/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [44032/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [44160/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [44288/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [44416/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [44544/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [44672/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [44800/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [44928/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [45056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [45184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [45312/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [45440/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [45568/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [45696/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [45824/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [45952/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [46080/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [46208/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [46336/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [46464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [46592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [46720/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 18 [46848/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [46976/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [47104/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [47232/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [47360/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [47488/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [47616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [47744/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [47872/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [48000/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [48128/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [48256/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [48384/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [48512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [48640/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [48768/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [48896/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [49024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [49152/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [49280/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [49408/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [49536/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [49664/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [49792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [49920/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [50000/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "epoch 18 training time consumed: 54.97s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  81866 GiB |  81866 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  81325 GiB |  81325 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    541 GiB |    541 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  81866 GiB |  81866 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  81325 GiB |  81325 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    541 GiB |    541 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  81734 GiB |  81734 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  81194 GiB |  81193 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    540 GiB |    540 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  65203 GiB |  65203 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  64616 GiB |  64616 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    587 GiB |    587 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    5215 K  |    5215 K  |\n",
            "|       from large pool |      20    |     103    |    2476 K  |    2476 K  |\n",
            "|       from small pool |     384    |     506    |    2738 K  |    2738 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    5215 K  |    5215 K  |\n",
            "|       from large pool |      20    |     103    |    2476 K  |    2476 K  |\n",
            "|       from small pool |     384    |     506    |    2738 K  |    2738 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      24    |      41    |    1475 K  |    1475 K  |\n",
            "|       from large pool |       8    |      17    |     983 K  |     983 K  |\n",
            "|       from small pool |      16    |      33    |     492 K  |     492 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:4.24s\n",
            "\n",
            "Training Epoch: 19 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [512/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [640/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [768/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1024/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [1280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [1408/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [1536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [1664/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [1792/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [1920/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [2048/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [2176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [2304/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [2432/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [2560/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [2688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [2816/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [2944/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [3072/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [3200/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [3328/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [3456/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [3584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [3712/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [3840/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [3968/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [4096/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [4224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [4352/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [4480/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [4608/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [4736/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 19 [4864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [4992/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [5120/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [5248/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [5376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [5504/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [5632/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [5760/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [5888/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [6016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [6144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [6272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [6400/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [6528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [6656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [6784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [6912/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [7040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [7168/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [7296/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [7424/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [7552/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [7680/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [7808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [7936/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [8064/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [8192/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [8320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [8448/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [8576/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [8704/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [8832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [8960/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [9088/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [9216/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 19 [9344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [9472/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [9600/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [9728/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [9856/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 19 [9984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [10112/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [10240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [10496/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [10624/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [10752/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [10880/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [11008/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [11136/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [11264/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [11392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [11520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [11648/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [11776/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [11904/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 19 [12032/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [12160/50000]\tLoss: 4.6030\tLR: 0.010000\n",
            "Training Epoch: 19 [12288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [12416/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [12544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [12672/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [12800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [12928/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [13056/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [13184/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [13312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [13440/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [13568/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [13696/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [13824/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [13952/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [14080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [14208/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [14336/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [14464/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [14592/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 19 [14720/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 19 [14848/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [14976/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 19 [15104/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [15232/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [15360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [15488/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 19 [15616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [15744/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [15872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [16000/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 19 [16128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [16256/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [16384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [16512/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [16640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [16768/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [16896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [17024/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [17152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [17280/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [17408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [17536/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [17664/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 19 [17792/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [17920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [18048/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 19 [18176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [18304/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [18432/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [18560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [18688/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 19 [18816/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [18944/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [19072/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [19200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [19328/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [19456/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [19584/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [19712/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [19840/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [19968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [20096/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [20224/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [20352/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [20480/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [20608/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [20736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [20864/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [20992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [21120/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [21248/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 19 [21376/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [21504/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [21632/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [21760/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [21888/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [22016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [22144/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [22272/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [22400/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [22528/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [22656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [22784/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [22912/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [23040/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [23168/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [23296/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [23424/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [23552/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [23680/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 19 [23808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [23936/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [24064/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [24192/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [24320/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [24448/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [24576/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 19 [24704/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [24832/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [24960/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [25088/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [25216/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [25344/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [25472/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [25600/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [25728/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [25856/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [25984/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [26112/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [26240/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [26368/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [26496/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [26624/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [26752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [26880/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [27008/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [27136/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [27264/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [27392/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [27520/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [27648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [27776/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [27904/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [28032/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [28160/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [28288/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [28416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [28544/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [28672/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [28800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [28928/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [29056/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [29184/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [29312/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [29440/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [29568/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [29696/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [29824/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [29952/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [30080/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [30208/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [30336/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [30464/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [30592/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [30720/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [30848/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [30976/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [31104/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [31232/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [31360/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [31488/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [31616/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [31744/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [31872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [32000/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [32128/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [32256/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [32384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [32512/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [32640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [32768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [32896/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [33024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [33152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [33280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [33408/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 19 [33536/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [33664/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 19 [33792/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [33920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [34048/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 19 [34176/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [34304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [34432/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [34560/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [34688/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [34816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [34944/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [35072/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [35200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [35328/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [35456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [35584/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [35712/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [35840/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [35968/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 19 [36096/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [36224/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [36352/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [36480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [36608/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [36736/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [36864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [36992/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [37120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [37248/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [37376/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [37504/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [37632/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 19 [37760/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [37888/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [38016/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [38144/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [38272/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [38400/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [38528/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [38656/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [38784/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [38912/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [39040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [39168/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [39296/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [39424/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [39552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [39680/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [39808/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [39936/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [40064/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [40192/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 19 [40320/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [40448/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [40576/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [40704/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [40832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [40960/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [41088/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [41216/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [41472/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [41600/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [41728/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [41856/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [41984/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [42112/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [42240/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [42368/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [42496/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [42624/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [42752/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [42880/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [43008/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [43136/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [43264/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 19 [43392/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [43520/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [43648/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [43776/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [43904/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [44032/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [44160/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [44288/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [44416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [44544/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [44672/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [44800/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [44928/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [45056/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [45184/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [45312/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [45440/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [45568/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 19 [45696/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [45824/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [45952/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [46080/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [46208/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [46336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [46464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [46592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [46720/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [46848/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [46976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [47104/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [47232/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [47360/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [47488/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [47616/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [47744/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [47872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [48000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [48128/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [48256/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [48384/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [48512/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [48640/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [48768/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [48896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [49024/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [49152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [49280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [49408/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [49536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [49664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [49792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [49920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [50000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "epoch 19 training time consumed: 54.84s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  86414 GiB |  86413 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  85842 GiB |  85842 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    571 GiB |    571 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  86414 GiB |  86413 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  85842 GiB |  85842 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    571 GiB |    571 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  86274 GiB |  86274 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  85703 GiB |  85703 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    570 GiB |    570 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  68825 GiB |  68825 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  68206 GiB |  68206 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    619 GiB |    619 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    5505 K  |    5505 K  |\n",
            "|       from large pool |      20    |     103    |    2614 K  |    2614 K  |\n",
            "|       from small pool |     384    |     506    |    2891 K  |    2890 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    5505 K  |    5505 K  |\n",
            "|       from large pool |      20    |     103    |    2614 K  |    2614 K  |\n",
            "|       from small pool |     384    |     506    |    2891 K  |    2890 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      26    |      41    |    1558 K  |    1558 K  |\n",
            "|       from large pool |       8    |      17    |    1037 K  |    1037 K  |\n",
            "|       from small pool |      18    |      33    |     520 K  |     520 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.77s\n",
            "\n",
            "Training Epoch: 20 [128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [256/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [512/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [640/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [896/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [1024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [1152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [1280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [1408/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [1536/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [1664/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [1792/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [1920/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [2048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [2176/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [2304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [2432/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [2560/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [2688/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [2816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [2944/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [3072/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [3200/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [3328/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [3456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [3584/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [3712/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [3840/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [3968/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [4096/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [4224/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [4352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [4480/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [4608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [4736/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [4864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [4992/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [5120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [5248/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [5376/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [5504/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [5632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [5760/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [5888/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [6016/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [6144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [6272/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [6400/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [6528/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [6656/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [6784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [6912/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [7040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [7168/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [7296/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [7424/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [7552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [7680/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [7808/50000]\tLoss: 4.6034\tLR: 0.010000\n",
            "Training Epoch: 20 [7936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [8064/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [8192/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [8320/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [8448/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [8576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [8704/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [8832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [8960/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [9088/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [9216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [9344/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [9472/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [9600/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [9728/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [9856/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [9984/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [10112/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [10240/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [10368/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [10496/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [10624/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [10752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [10880/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [11008/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [11136/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [11264/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [11392/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [11520/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [11648/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [11776/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [11904/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [12032/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [12160/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [12288/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 20 [12416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [12544/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [12672/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [12800/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [12928/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [13056/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [13184/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [13312/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [13440/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [13568/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [13696/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [13824/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [13952/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [14080/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [14208/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [14336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [14464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [14592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [14720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [14848/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [14976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [15104/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [15232/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [15360/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [15488/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [15616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [15744/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 20 [15872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [16000/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [16128/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [16256/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [16384/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [16512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [16640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [16768/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [16896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [17024/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [17152/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [17280/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [17408/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [17536/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [17664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [17792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [17920/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [18048/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [18176/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 20 [18304/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 20 [18432/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [18560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [18688/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [18816/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [18944/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [19072/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [19200/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [19328/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [19456/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [19584/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [19712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [19840/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [19968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [20096/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [20224/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [20352/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [20480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [20608/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [20736/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [20864/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [20992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [21120/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [21248/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [21376/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [21504/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [21632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [21760/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [21888/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [22016/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 20 [22144/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [22272/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [22400/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [22528/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [22656/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [22784/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [22912/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [23040/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [23168/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [23296/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [23424/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [23552/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 20 [23680/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [23808/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [23936/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [24064/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [24192/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [24320/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [24448/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [24576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [24704/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [24832/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [24960/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [25088/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [25216/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [25344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [25472/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [25600/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [25728/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [25856/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [25984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [26112/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [26240/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [26368/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 20 [26496/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [26624/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [26752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [26880/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [27008/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [27136/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [27264/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [27392/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [27520/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [27648/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [27776/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [27904/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [28032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [28160/50000]\tLoss: 4.6082\tLR: 0.010000\n",
            "Training Epoch: 20 [28288/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [28416/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [28544/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [28672/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [28800/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [28928/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [29056/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [29184/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [29312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [29440/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [29568/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [29696/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [29824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [29952/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [30080/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [30208/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [30336/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [30464/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 20 [30592/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [30720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [30848/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [30976/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [31104/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [31232/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [31360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [31488/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [31616/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [31744/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [31872/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [32000/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [32128/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [32256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [32384/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [32512/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [32640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [32768/50000]\tLoss: 4.6032\tLR: 0.010000\n",
            "Training Epoch: 20 [32896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [33024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [33152/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [33280/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [33408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [33536/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [33664/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [33792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [33920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [34048/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [34176/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 20 [34304/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [34432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [34560/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [34688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [34816/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 20 [34944/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [35072/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [35200/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [35328/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [35456/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [35584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [35712/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [35840/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [35968/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [36096/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [36224/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [36352/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [36480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [36608/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [36736/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [36864/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 20 [36992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [37120/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [37248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [37376/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [37504/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [37632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [37760/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [37888/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [38016/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [38144/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [38272/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [38400/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [38528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [38656/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [38784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [38912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [39040/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [39168/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [39296/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [39424/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 20 [39552/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [39680/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [39808/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [39936/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [40064/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [40192/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [40320/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 20 [40448/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [40576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [40704/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [40832/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [40960/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [41088/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [41216/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [41472/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [41600/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [41728/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [41856/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [41984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [42112/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [42240/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [42368/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [42496/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [42624/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [42752/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [42880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [43008/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [43136/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [43264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [43392/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [43520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [43648/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [43776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [43904/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [44032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [44160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [44288/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [44416/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [44544/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [44672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [44800/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [44928/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [45056/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [45184/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [45312/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [45440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [45568/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [45696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [45824/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [45952/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [46080/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [46208/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [46336/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [46464/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [46592/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [46720/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [46848/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [46976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [47104/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [47232/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [47360/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [47488/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [47616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [47744/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [47872/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [48000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [48128/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [48256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [48384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [48512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [48640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [48768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [48896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [49024/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [49152/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [49280/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [49408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [49536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [49664/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [49792/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [49920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [50000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "epoch 20 training time consumed: 55.15s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |  94255 KiB |   1567 MiB |  90961 GiB |  90960 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  90359 GiB |  90359 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    601 GiB |    601 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |  94255 KiB |   1567 MiB |  90961 GiB |  90960 GiB |\n",
            "|       from large pool |  49312 KiB |   1529 MiB |  90359 GiB |  90359 GiB |\n",
            "|       from small pool |  44943 KiB |     59 MiB |    601 GiB |    601 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      |  91692 KiB |   1561 MiB |  90814 GiB |  90814 GiB |\n",
            "|       from large pool |  46790 KiB |   1522 MiB |  90213 GiB |  90213 GiB |\n",
            "|       from small pool |  44902 KiB |     59 MiB |    600 GiB |    600 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   1776 MiB |   1776 MiB |   1776 MiB |      0 B   |\n",
            "|       from large pool |   1716 MiB |   1716 MiB |   1716 MiB |      0 B   |\n",
            "|       from small pool |     60 MiB |     60 MiB |     60 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 194512 KiB | 582598 KiB |  72448 GiB |  72448 GiB |\n",
            "|       from large pool | 192352 KiB | 575934 KiB |  71796 GiB |  71796 GiB |\n",
            "|       from small pool |   2160 KiB |  22025 KiB |    652 GiB |    652 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     404    |     532    |    5795 K  |    5794 K  |\n",
            "|       from large pool |      20    |     103    |    2752 K  |    2751 K  |\n",
            "|       from small pool |     384    |     506    |    3043 K  |    3042 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     404    |     532    |    5795 K  |    5794 K  |\n",
            "|       from large pool |      20    |     103    |    2752 K  |    2751 K  |\n",
            "|       from small pool |     384    |     506    |    3043 K  |    3042 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      59    |      59    |      59    |       0    |\n",
            "|       from large pool |      29    |      29    |      29    |       0    |\n",
            "|       from small pool |      30    |      30    |      30    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      23    |      41    |    1639 K  |    1639 K  |\n",
            "|       from large pool |       8    |      17    |    1092 K  |    1092 K  |\n",
            "|       from small pool |      15    |      33    |     546 K  |     546 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:3.71s\n",
            "\n",
            "saving weights file to checkpoint/googlenet/Tuesday_25_July_2023_11h_07m_22s/googlenet-20-regular.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#without batch norm\n",
        "!python test.py -net googlenet -weights checkpoint/googlenet/Tuesday_25_July_2023_11h_07m_22s/googlenet-20-regular.pth"
      ],
      "metadata": {
        "id": "l5CwkFTtDMkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "183c7142-5287-49ff-a5e0-cb30b007a451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "GoogleNet(\n",
            "  (prelayer): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "  )\n",
            "  (a3): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (b3): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(32, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (a4): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(16, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (b4): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (c4): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(24, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (d4): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (e4): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (a5): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (b5): Inception(\n",
            "    (b1): Sequential(\n",
            "      (0): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "    )\n",
            "    (b2): Sequential(\n",
            "      (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "    )\n",
            "    (b3): Sequential(\n",
            "      (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): ReLU(inplace=True)\n",
            "      (2): Conv2d(48, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (b4): Sequential(\n",
            "      (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "      (1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (dropout): Dropout2d(p=0.4, inplace=False)\n",
            "  (linear): Linear(in_features=1024, out_features=100, bias=True)\n",
            ")\n",
            "iteration: 1\ttotal 625 iterations\n",
            "iteration: 2\ttotal 625 iterations\n",
            "iteration: 3\ttotal 625 iterations\n",
            "iteration: 4\ttotal 625 iterations\n",
            "iteration: 5\ttotal 625 iterations\n",
            "iteration: 6\ttotal 625 iterations\n",
            "iteration: 7\ttotal 625 iterations\n",
            "iteration: 8\ttotal 625 iterations\n",
            "iteration: 9\ttotal 625 iterations\n",
            "iteration: 10\ttotal 625 iterations\n",
            "iteration: 11\ttotal 625 iterations\n",
            "iteration: 12\ttotal 625 iterations\n",
            "iteration: 13\ttotal 625 iterations\n",
            "iteration: 14\ttotal 625 iterations\n",
            "iteration: 15\ttotal 625 iterations\n",
            "iteration: 16\ttotal 625 iterations\n",
            "iteration: 17\ttotal 625 iterations\n",
            "iteration: 18\ttotal 625 iterations\n",
            "iteration: 19\ttotal 625 iterations\n",
            "iteration: 20\ttotal 625 iterations\n",
            "iteration: 21\ttotal 625 iterations\n",
            "iteration: 22\ttotal 625 iterations\n",
            "iteration: 23\ttotal 625 iterations\n",
            "iteration: 24\ttotal 625 iterations\n",
            "iteration: 25\ttotal 625 iterations\n",
            "iteration: 26\ttotal 625 iterations\n",
            "iteration: 27\ttotal 625 iterations\n",
            "iteration: 28\ttotal 625 iterations\n",
            "iteration: 29\ttotal 625 iterations\n",
            "iteration: 30\ttotal 625 iterations\n",
            "iteration: 31\ttotal 625 iterations\n",
            "iteration: 32\ttotal 625 iterations\n",
            "iteration: 33\ttotal 625 iterations\n",
            "iteration: 34\ttotal 625 iterations\n",
            "iteration: 35\ttotal 625 iterations\n",
            "iteration: 36\ttotal 625 iterations\n",
            "iteration: 37\ttotal 625 iterations\n",
            "iteration: 38\ttotal 625 iterations\n",
            "iteration: 39\ttotal 625 iterations\n",
            "iteration: 40\ttotal 625 iterations\n",
            "iteration: 41\ttotal 625 iterations\n",
            "iteration: 42\ttotal 625 iterations\n",
            "iteration: 43\ttotal 625 iterations\n",
            "iteration: 44\ttotal 625 iterations\n",
            "iteration: 45\ttotal 625 iterations\n",
            "iteration: 46\ttotal 625 iterations\n",
            "iteration: 47\ttotal 625 iterations\n",
            "iteration: 48\ttotal 625 iterations\n",
            "iteration: 49\ttotal 625 iterations\n",
            "iteration: 50\ttotal 625 iterations\n",
            "iteration: 51\ttotal 625 iterations\n",
            "iteration: 52\ttotal 625 iterations\n",
            "iteration: 53\ttotal 625 iterations\n",
            "iteration: 54\ttotal 625 iterations\n",
            "iteration: 55\ttotal 625 iterations\n",
            "iteration: 56\ttotal 625 iterations\n",
            "iteration: 57\ttotal 625 iterations\n",
            "iteration: 58\ttotal 625 iterations\n",
            "iteration: 59\ttotal 625 iterations\n",
            "iteration: 60\ttotal 625 iterations\n",
            "iteration: 61\ttotal 625 iterations\n",
            "iteration: 62\ttotal 625 iterations\n",
            "iteration: 63\ttotal 625 iterations\n",
            "iteration: 64\ttotal 625 iterations\n",
            "iteration: 65\ttotal 625 iterations\n",
            "iteration: 66\ttotal 625 iterations\n",
            "iteration: 67\ttotal 625 iterations\n",
            "iteration: 68\ttotal 625 iterations\n",
            "iteration: 69\ttotal 625 iterations\n",
            "iteration: 70\ttotal 625 iterations\n",
            "iteration: 71\ttotal 625 iterations\n",
            "iteration: 72\ttotal 625 iterations\n",
            "iteration: 73\ttotal 625 iterations\n",
            "iteration: 74\ttotal 625 iterations\n",
            "iteration: 75\ttotal 625 iterations\n",
            "iteration: 76\ttotal 625 iterations\n",
            "iteration: 77\ttotal 625 iterations\n",
            "iteration: 78\ttotal 625 iterations\n",
            "iteration: 79\ttotal 625 iterations\n",
            "iteration: 80\ttotal 625 iterations\n",
            "iteration: 81\ttotal 625 iterations\n",
            "iteration: 82\ttotal 625 iterations\n",
            "iteration: 83\ttotal 625 iterations\n",
            "iteration: 84\ttotal 625 iterations\n",
            "iteration: 85\ttotal 625 iterations\n",
            "iteration: 86\ttotal 625 iterations\n",
            "iteration: 87\ttotal 625 iterations\n",
            "iteration: 88\ttotal 625 iterations\n",
            "iteration: 89\ttotal 625 iterations\n",
            "iteration: 90\ttotal 625 iterations\n",
            "iteration: 91\ttotal 625 iterations\n",
            "iteration: 92\ttotal 625 iterations\n",
            "iteration: 93\ttotal 625 iterations\n",
            "iteration: 94\ttotal 625 iterations\n",
            "iteration: 95\ttotal 625 iterations\n",
            "iteration: 96\ttotal 625 iterations\n",
            "iteration: 97\ttotal 625 iterations\n",
            "iteration: 98\ttotal 625 iterations\n",
            "iteration: 99\ttotal 625 iterations\n",
            "iteration: 100\ttotal 625 iterations\n",
            "iteration: 101\ttotal 625 iterations\n",
            "iteration: 102\ttotal 625 iterations\n",
            "iteration: 103\ttotal 625 iterations\n",
            "iteration: 104\ttotal 625 iterations\n",
            "iteration: 105\ttotal 625 iterations\n",
            "iteration: 106\ttotal 625 iterations\n",
            "iteration: 107\ttotal 625 iterations\n",
            "iteration: 108\ttotal 625 iterations\n",
            "iteration: 109\ttotal 625 iterations\n",
            "iteration: 110\ttotal 625 iterations\n",
            "iteration: 111\ttotal 625 iterations\n",
            "iteration: 112\ttotal 625 iterations\n",
            "iteration: 113\ttotal 625 iterations\n",
            "iteration: 114\ttotal 625 iterations\n",
            "iteration: 115\ttotal 625 iterations\n",
            "iteration: 116\ttotal 625 iterations\n",
            "iteration: 117\ttotal 625 iterations\n",
            "iteration: 118\ttotal 625 iterations\n",
            "iteration: 119\ttotal 625 iterations\n",
            "iteration: 120\ttotal 625 iterations\n",
            "iteration: 121\ttotal 625 iterations\n",
            "iteration: 122\ttotal 625 iterations\n",
            "iteration: 123\ttotal 625 iterations\n",
            "iteration: 124\ttotal 625 iterations\n",
            "iteration: 125\ttotal 625 iterations\n",
            "iteration: 126\ttotal 625 iterations\n",
            "iteration: 127\ttotal 625 iterations\n",
            "iteration: 128\ttotal 625 iterations\n",
            "iteration: 129\ttotal 625 iterations\n",
            "iteration: 130\ttotal 625 iterations\n",
            "iteration: 131\ttotal 625 iterations\n",
            "iteration: 132\ttotal 625 iterations\n",
            "iteration: 133\ttotal 625 iterations\n",
            "iteration: 134\ttotal 625 iterations\n",
            "iteration: 135\ttotal 625 iterations\n",
            "iteration: 136\ttotal 625 iterations\n",
            "iteration: 137\ttotal 625 iterations\n",
            "iteration: 138\ttotal 625 iterations\n",
            "iteration: 139\ttotal 625 iterations\n",
            "iteration: 140\ttotal 625 iterations\n",
            "iteration: 141\ttotal 625 iterations\n",
            "iteration: 142\ttotal 625 iterations\n",
            "iteration: 143\ttotal 625 iterations\n",
            "iteration: 144\ttotal 625 iterations\n",
            "iteration: 145\ttotal 625 iterations\n",
            "iteration: 146\ttotal 625 iterations\n",
            "iteration: 147\ttotal 625 iterations\n",
            "iteration: 148\ttotal 625 iterations\n",
            "iteration: 149\ttotal 625 iterations\n",
            "iteration: 150\ttotal 625 iterations\n",
            "iteration: 151\ttotal 625 iterations\n",
            "iteration: 152\ttotal 625 iterations\n",
            "iteration: 153\ttotal 625 iterations\n",
            "iteration: 154\ttotal 625 iterations\n",
            "iteration: 155\ttotal 625 iterations\n",
            "iteration: 156\ttotal 625 iterations\n",
            "iteration: 157\ttotal 625 iterations\n",
            "iteration: 158\ttotal 625 iterations\n",
            "iteration: 159\ttotal 625 iterations\n",
            "iteration: 160\ttotal 625 iterations\n",
            "iteration: 161\ttotal 625 iterations\n",
            "iteration: 162\ttotal 625 iterations\n",
            "iteration: 163\ttotal 625 iterations\n",
            "iteration: 164\ttotal 625 iterations\n",
            "iteration: 165\ttotal 625 iterations\n",
            "iteration: 166\ttotal 625 iterations\n",
            "iteration: 167\ttotal 625 iterations\n",
            "iteration: 168\ttotal 625 iterations\n",
            "iteration: 169\ttotal 625 iterations\n",
            "iteration: 170\ttotal 625 iterations\n",
            "iteration: 171\ttotal 625 iterations\n",
            "iteration: 172\ttotal 625 iterations\n",
            "iteration: 173\ttotal 625 iterations\n",
            "iteration: 174\ttotal 625 iterations\n",
            "iteration: 175\ttotal 625 iterations\n",
            "iteration: 176\ttotal 625 iterations\n",
            "iteration: 177\ttotal 625 iterations\n",
            "iteration: 178\ttotal 625 iterations\n",
            "iteration: 179\ttotal 625 iterations\n",
            "iteration: 180\ttotal 625 iterations\n",
            "iteration: 181\ttotal 625 iterations\n",
            "iteration: 182\ttotal 625 iterations\n",
            "iteration: 183\ttotal 625 iterations\n",
            "iteration: 184\ttotal 625 iterations\n",
            "iteration: 185\ttotal 625 iterations\n",
            "iteration: 186\ttotal 625 iterations\n",
            "iteration: 187\ttotal 625 iterations\n",
            "iteration: 188\ttotal 625 iterations\n",
            "iteration: 189\ttotal 625 iterations\n",
            "iteration: 190\ttotal 625 iterations\n",
            "iteration: 191\ttotal 625 iterations\n",
            "iteration: 192\ttotal 625 iterations\n",
            "iteration: 193\ttotal 625 iterations\n",
            "iteration: 194\ttotal 625 iterations\n",
            "iteration: 195\ttotal 625 iterations\n",
            "iteration: 196\ttotal 625 iterations\n",
            "iteration: 197\ttotal 625 iterations\n",
            "iteration: 198\ttotal 625 iterations\n",
            "iteration: 199\ttotal 625 iterations\n",
            "iteration: 200\ttotal 625 iterations\n",
            "iteration: 201\ttotal 625 iterations\n",
            "iteration: 202\ttotal 625 iterations\n",
            "iteration: 203\ttotal 625 iterations\n",
            "iteration: 204\ttotal 625 iterations\n",
            "iteration: 205\ttotal 625 iterations\n",
            "iteration: 206\ttotal 625 iterations\n",
            "iteration: 207\ttotal 625 iterations\n",
            "iteration: 208\ttotal 625 iterations\n",
            "iteration: 209\ttotal 625 iterations\n",
            "iteration: 210\ttotal 625 iterations\n",
            "iteration: 211\ttotal 625 iterations\n",
            "iteration: 212\ttotal 625 iterations\n",
            "iteration: 213\ttotal 625 iterations\n",
            "iteration: 214\ttotal 625 iterations\n",
            "iteration: 215\ttotal 625 iterations\n",
            "iteration: 216\ttotal 625 iterations\n",
            "iteration: 217\ttotal 625 iterations\n",
            "iteration: 218\ttotal 625 iterations\n",
            "iteration: 219\ttotal 625 iterations\n",
            "iteration: 220\ttotal 625 iterations\n",
            "iteration: 221\ttotal 625 iterations\n",
            "iteration: 222\ttotal 625 iterations\n",
            "iteration: 223\ttotal 625 iterations\n",
            "iteration: 224\ttotal 625 iterations\n",
            "iteration: 225\ttotal 625 iterations\n",
            "iteration: 226\ttotal 625 iterations\n",
            "iteration: 227\ttotal 625 iterations\n",
            "iteration: 228\ttotal 625 iterations\n",
            "iteration: 229\ttotal 625 iterations\n",
            "iteration: 230\ttotal 625 iterations\n",
            "iteration: 231\ttotal 625 iterations\n",
            "iteration: 232\ttotal 625 iterations\n",
            "iteration: 233\ttotal 625 iterations\n",
            "iteration: 234\ttotal 625 iterations\n",
            "iteration: 235\ttotal 625 iterations\n",
            "iteration: 236\ttotal 625 iterations\n",
            "iteration: 237\ttotal 625 iterations\n",
            "iteration: 238\ttotal 625 iterations\n",
            "iteration: 239\ttotal 625 iterations\n",
            "iteration: 240\ttotal 625 iterations\n",
            "iteration: 241\ttotal 625 iterations\n",
            "iteration: 242\ttotal 625 iterations\n",
            "iteration: 243\ttotal 625 iterations\n",
            "iteration: 244\ttotal 625 iterations\n",
            "iteration: 245\ttotal 625 iterations\n",
            "iteration: 246\ttotal 625 iterations\n",
            "iteration: 247\ttotal 625 iterations\n",
            "iteration: 248\ttotal 625 iterations\n",
            "iteration: 249\ttotal 625 iterations\n",
            "iteration: 250\ttotal 625 iterations\n",
            "iteration: 251\ttotal 625 iterations\n",
            "iteration: 252\ttotal 625 iterations\n",
            "iteration: 253\ttotal 625 iterations\n",
            "iteration: 254\ttotal 625 iterations\n",
            "iteration: 255\ttotal 625 iterations\n",
            "iteration: 256\ttotal 625 iterations\n",
            "iteration: 257\ttotal 625 iterations\n",
            "iteration: 258\ttotal 625 iterations\n",
            "iteration: 259\ttotal 625 iterations\n",
            "iteration: 260\ttotal 625 iterations\n",
            "iteration: 261\ttotal 625 iterations\n",
            "iteration: 262\ttotal 625 iterations\n",
            "iteration: 263\ttotal 625 iterations\n",
            "iteration: 264\ttotal 625 iterations\n",
            "iteration: 265\ttotal 625 iterations\n",
            "iteration: 266\ttotal 625 iterations\n",
            "iteration: 267\ttotal 625 iterations\n",
            "iteration: 268\ttotal 625 iterations\n",
            "iteration: 269\ttotal 625 iterations\n",
            "iteration: 270\ttotal 625 iterations\n",
            "iteration: 271\ttotal 625 iterations\n",
            "iteration: 272\ttotal 625 iterations\n",
            "iteration: 273\ttotal 625 iterations\n",
            "iteration: 274\ttotal 625 iterations\n",
            "iteration: 275\ttotal 625 iterations\n",
            "iteration: 276\ttotal 625 iterations\n",
            "iteration: 277\ttotal 625 iterations\n",
            "iteration: 278\ttotal 625 iterations\n",
            "iteration: 279\ttotal 625 iterations\n",
            "iteration: 280\ttotal 625 iterations\n",
            "iteration: 281\ttotal 625 iterations\n",
            "iteration: 282\ttotal 625 iterations\n",
            "iteration: 283\ttotal 625 iterations\n",
            "iteration: 284\ttotal 625 iterations\n",
            "iteration: 285\ttotal 625 iterations\n",
            "iteration: 286\ttotal 625 iterations\n",
            "iteration: 287\ttotal 625 iterations\n",
            "iteration: 288\ttotal 625 iterations\n",
            "iteration: 289\ttotal 625 iterations\n",
            "iteration: 290\ttotal 625 iterations\n",
            "iteration: 291\ttotal 625 iterations\n",
            "iteration: 292\ttotal 625 iterations\n",
            "iteration: 293\ttotal 625 iterations\n",
            "iteration: 294\ttotal 625 iterations\n",
            "iteration: 295\ttotal 625 iterations\n",
            "iteration: 296\ttotal 625 iterations\n",
            "iteration: 297\ttotal 625 iterations\n",
            "iteration: 298\ttotal 625 iterations\n",
            "iteration: 299\ttotal 625 iterations\n",
            "iteration: 300\ttotal 625 iterations\n",
            "iteration: 301\ttotal 625 iterations\n",
            "iteration: 302\ttotal 625 iterations\n",
            "iteration: 303\ttotal 625 iterations\n",
            "iteration: 304\ttotal 625 iterations\n",
            "iteration: 305\ttotal 625 iterations\n",
            "iteration: 306\ttotal 625 iterations\n",
            "iteration: 307\ttotal 625 iterations\n",
            "iteration: 308\ttotal 625 iterations\n",
            "iteration: 309\ttotal 625 iterations\n",
            "iteration: 310\ttotal 625 iterations\n",
            "iteration: 311\ttotal 625 iterations\n",
            "iteration: 312\ttotal 625 iterations\n",
            "iteration: 313\ttotal 625 iterations\n",
            "iteration: 314\ttotal 625 iterations\n",
            "iteration: 315\ttotal 625 iterations\n",
            "iteration: 316\ttotal 625 iterations\n",
            "iteration: 317\ttotal 625 iterations\n",
            "iteration: 318\ttotal 625 iterations\n",
            "iteration: 319\ttotal 625 iterations\n",
            "iteration: 320\ttotal 625 iterations\n",
            "iteration: 321\ttotal 625 iterations\n",
            "iteration: 322\ttotal 625 iterations\n",
            "iteration: 323\ttotal 625 iterations\n",
            "iteration: 324\ttotal 625 iterations\n",
            "iteration: 325\ttotal 625 iterations\n",
            "iteration: 326\ttotal 625 iterations\n",
            "iteration: 327\ttotal 625 iterations\n",
            "iteration: 328\ttotal 625 iterations\n",
            "iteration: 329\ttotal 625 iterations\n",
            "iteration: 330\ttotal 625 iterations\n",
            "iteration: 331\ttotal 625 iterations\n",
            "iteration: 332\ttotal 625 iterations\n",
            "iteration: 333\ttotal 625 iterations\n",
            "iteration: 334\ttotal 625 iterations\n",
            "iteration: 335\ttotal 625 iterations\n",
            "iteration: 336\ttotal 625 iterations\n",
            "iteration: 337\ttotal 625 iterations\n",
            "iteration: 338\ttotal 625 iterations\n",
            "iteration: 339\ttotal 625 iterations\n",
            "iteration: 340\ttotal 625 iterations\n",
            "iteration: 341\ttotal 625 iterations\n",
            "iteration: 342\ttotal 625 iterations\n",
            "iteration: 343\ttotal 625 iterations\n",
            "iteration: 344\ttotal 625 iterations\n",
            "iteration: 345\ttotal 625 iterations\n",
            "iteration: 346\ttotal 625 iterations\n",
            "iteration: 347\ttotal 625 iterations\n",
            "iteration: 348\ttotal 625 iterations\n",
            "iteration: 349\ttotal 625 iterations\n",
            "iteration: 350\ttotal 625 iterations\n",
            "iteration: 351\ttotal 625 iterations\n",
            "iteration: 352\ttotal 625 iterations\n",
            "iteration: 353\ttotal 625 iterations\n",
            "iteration: 354\ttotal 625 iterations\n",
            "iteration: 355\ttotal 625 iterations\n",
            "iteration: 356\ttotal 625 iterations\n",
            "iteration: 357\ttotal 625 iterations\n",
            "iteration: 358\ttotal 625 iterations\n",
            "iteration: 359\ttotal 625 iterations\n",
            "iteration: 360\ttotal 625 iterations\n",
            "iteration: 361\ttotal 625 iterations\n",
            "iteration: 362\ttotal 625 iterations\n",
            "iteration: 363\ttotal 625 iterations\n",
            "iteration: 364\ttotal 625 iterations\n",
            "iteration: 365\ttotal 625 iterations\n",
            "iteration: 366\ttotal 625 iterations\n",
            "iteration: 367\ttotal 625 iterations\n",
            "iteration: 368\ttotal 625 iterations\n",
            "iteration: 369\ttotal 625 iterations\n",
            "iteration: 370\ttotal 625 iterations\n",
            "iteration: 371\ttotal 625 iterations\n",
            "iteration: 372\ttotal 625 iterations\n",
            "iteration: 373\ttotal 625 iterations\n",
            "iteration: 374\ttotal 625 iterations\n",
            "iteration: 375\ttotal 625 iterations\n",
            "iteration: 376\ttotal 625 iterations\n",
            "iteration: 377\ttotal 625 iterations\n",
            "iteration: 378\ttotal 625 iterations\n",
            "iteration: 379\ttotal 625 iterations\n",
            "iteration: 380\ttotal 625 iterations\n",
            "iteration: 381\ttotal 625 iterations\n",
            "iteration: 382\ttotal 625 iterations\n",
            "iteration: 383\ttotal 625 iterations\n",
            "iteration: 384\ttotal 625 iterations\n",
            "iteration: 385\ttotal 625 iterations\n",
            "iteration: 386\ttotal 625 iterations\n",
            "iteration: 387\ttotal 625 iterations\n",
            "iteration: 388\ttotal 625 iterations\n",
            "iteration: 389\ttotal 625 iterations\n",
            "iteration: 390\ttotal 625 iterations\n",
            "iteration: 391\ttotal 625 iterations\n",
            "iteration: 392\ttotal 625 iterations\n",
            "iteration: 393\ttotal 625 iterations\n",
            "iteration: 394\ttotal 625 iterations\n",
            "iteration: 395\ttotal 625 iterations\n",
            "iteration: 396\ttotal 625 iterations\n",
            "iteration: 397\ttotal 625 iterations\n",
            "iteration: 398\ttotal 625 iterations\n",
            "iteration: 399\ttotal 625 iterations\n",
            "iteration: 400\ttotal 625 iterations\n",
            "iteration: 401\ttotal 625 iterations\n",
            "iteration: 402\ttotal 625 iterations\n",
            "iteration: 403\ttotal 625 iterations\n",
            "iteration: 404\ttotal 625 iterations\n",
            "iteration: 405\ttotal 625 iterations\n",
            "iteration: 406\ttotal 625 iterations\n",
            "iteration: 407\ttotal 625 iterations\n",
            "iteration: 408\ttotal 625 iterations\n",
            "iteration: 409\ttotal 625 iterations\n",
            "iteration: 410\ttotal 625 iterations\n",
            "iteration: 411\ttotal 625 iterations\n",
            "iteration: 412\ttotal 625 iterations\n",
            "iteration: 413\ttotal 625 iterations\n",
            "iteration: 414\ttotal 625 iterations\n",
            "iteration: 415\ttotal 625 iterations\n",
            "iteration: 416\ttotal 625 iterations\n",
            "iteration: 417\ttotal 625 iterations\n",
            "iteration: 418\ttotal 625 iterations\n",
            "iteration: 419\ttotal 625 iterations\n",
            "iteration: 420\ttotal 625 iterations\n",
            "iteration: 421\ttotal 625 iterations\n",
            "iteration: 422\ttotal 625 iterations\n",
            "iteration: 423\ttotal 625 iterations\n",
            "iteration: 424\ttotal 625 iterations\n",
            "iteration: 425\ttotal 625 iterations\n",
            "iteration: 426\ttotal 625 iterations\n",
            "iteration: 427\ttotal 625 iterations\n",
            "iteration: 428\ttotal 625 iterations\n",
            "iteration: 429\ttotal 625 iterations\n",
            "iteration: 430\ttotal 625 iterations\n",
            "iteration: 431\ttotal 625 iterations\n",
            "iteration: 432\ttotal 625 iterations\n",
            "iteration: 433\ttotal 625 iterations\n",
            "iteration: 434\ttotal 625 iterations\n",
            "iteration: 435\ttotal 625 iterations\n",
            "iteration: 436\ttotal 625 iterations\n",
            "iteration: 437\ttotal 625 iterations\n",
            "iteration: 438\ttotal 625 iterations\n",
            "iteration: 439\ttotal 625 iterations\n",
            "iteration: 440\ttotal 625 iterations\n",
            "iteration: 441\ttotal 625 iterations\n",
            "iteration: 442\ttotal 625 iterations\n",
            "iteration: 443\ttotal 625 iterations\n",
            "iteration: 444\ttotal 625 iterations\n",
            "iteration: 445\ttotal 625 iterations\n",
            "iteration: 446\ttotal 625 iterations\n",
            "iteration: 447\ttotal 625 iterations\n",
            "iteration: 448\ttotal 625 iterations\n",
            "iteration: 449\ttotal 625 iterations\n",
            "iteration: 450\ttotal 625 iterations\n",
            "iteration: 451\ttotal 625 iterations\n",
            "iteration: 452\ttotal 625 iterations\n",
            "iteration: 453\ttotal 625 iterations\n",
            "iteration: 454\ttotal 625 iterations\n",
            "iteration: 455\ttotal 625 iterations\n",
            "iteration: 456\ttotal 625 iterations\n",
            "iteration: 457\ttotal 625 iterations\n",
            "iteration: 458\ttotal 625 iterations\n",
            "iteration: 459\ttotal 625 iterations\n",
            "iteration: 460\ttotal 625 iterations\n",
            "iteration: 461\ttotal 625 iterations\n",
            "iteration: 462\ttotal 625 iterations\n",
            "iteration: 463\ttotal 625 iterations\n",
            "iteration: 464\ttotal 625 iterations\n",
            "iteration: 465\ttotal 625 iterations\n",
            "iteration: 466\ttotal 625 iterations\n",
            "iteration: 467\ttotal 625 iterations\n",
            "iteration: 468\ttotal 625 iterations\n",
            "iteration: 469\ttotal 625 iterations\n",
            "iteration: 470\ttotal 625 iterations\n",
            "iteration: 471\ttotal 625 iterations\n",
            "iteration: 472\ttotal 625 iterations\n",
            "iteration: 473\ttotal 625 iterations\n",
            "iteration: 474\ttotal 625 iterations\n",
            "iteration: 475\ttotal 625 iterations\n",
            "iteration: 476\ttotal 625 iterations\n",
            "iteration: 477\ttotal 625 iterations\n",
            "iteration: 478\ttotal 625 iterations\n",
            "iteration: 479\ttotal 625 iterations\n",
            "iteration: 480\ttotal 625 iterations\n",
            "iteration: 481\ttotal 625 iterations\n",
            "iteration: 482\ttotal 625 iterations\n",
            "iteration: 483\ttotal 625 iterations\n",
            "iteration: 484\ttotal 625 iterations\n",
            "iteration: 485\ttotal 625 iterations\n",
            "iteration: 486\ttotal 625 iterations\n",
            "iteration: 487\ttotal 625 iterations\n",
            "iteration: 488\ttotal 625 iterations\n",
            "iteration: 489\ttotal 625 iterations\n",
            "iteration: 490\ttotal 625 iterations\n",
            "iteration: 491\ttotal 625 iterations\n",
            "iteration: 492\ttotal 625 iterations\n",
            "iteration: 493\ttotal 625 iterations\n",
            "iteration: 494\ttotal 625 iterations\n",
            "iteration: 495\ttotal 625 iterations\n",
            "iteration: 496\ttotal 625 iterations\n",
            "iteration: 497\ttotal 625 iterations\n",
            "iteration: 498\ttotal 625 iterations\n",
            "iteration: 499\ttotal 625 iterations\n",
            "iteration: 500\ttotal 625 iterations\n",
            "iteration: 501\ttotal 625 iterations\n",
            "iteration: 502\ttotal 625 iterations\n",
            "iteration: 503\ttotal 625 iterations\n",
            "iteration: 504\ttotal 625 iterations\n",
            "iteration: 505\ttotal 625 iterations\n",
            "iteration: 506\ttotal 625 iterations\n",
            "iteration: 507\ttotal 625 iterations\n",
            "iteration: 508\ttotal 625 iterations\n",
            "iteration: 509\ttotal 625 iterations\n",
            "iteration: 510\ttotal 625 iterations\n",
            "iteration: 511\ttotal 625 iterations\n",
            "iteration: 512\ttotal 625 iterations\n",
            "iteration: 513\ttotal 625 iterations\n",
            "iteration: 514\ttotal 625 iterations\n",
            "iteration: 515\ttotal 625 iterations\n",
            "iteration: 516\ttotal 625 iterations\n",
            "iteration: 517\ttotal 625 iterations\n",
            "iteration: 518\ttotal 625 iterations\n",
            "iteration: 519\ttotal 625 iterations\n",
            "iteration: 520\ttotal 625 iterations\n",
            "iteration: 521\ttotal 625 iterations\n",
            "iteration: 522\ttotal 625 iterations\n",
            "iteration: 523\ttotal 625 iterations\n",
            "iteration: 524\ttotal 625 iterations\n",
            "iteration: 525\ttotal 625 iterations\n",
            "iteration: 526\ttotal 625 iterations\n",
            "iteration: 527\ttotal 625 iterations\n",
            "iteration: 528\ttotal 625 iterations\n",
            "iteration: 529\ttotal 625 iterations\n",
            "iteration: 530\ttotal 625 iterations\n",
            "iteration: 531\ttotal 625 iterations\n",
            "iteration: 532\ttotal 625 iterations\n",
            "iteration: 533\ttotal 625 iterations\n",
            "iteration: 534\ttotal 625 iterations\n",
            "iteration: 535\ttotal 625 iterations\n",
            "iteration: 536\ttotal 625 iterations\n",
            "iteration: 537\ttotal 625 iterations\n",
            "iteration: 538\ttotal 625 iterations\n",
            "iteration: 539\ttotal 625 iterations\n",
            "iteration: 540\ttotal 625 iterations\n",
            "iteration: 541\ttotal 625 iterations\n",
            "iteration: 542\ttotal 625 iterations\n",
            "iteration: 543\ttotal 625 iterations\n",
            "iteration: 544\ttotal 625 iterations\n",
            "iteration: 545\ttotal 625 iterations\n",
            "iteration: 546\ttotal 625 iterations\n",
            "iteration: 547\ttotal 625 iterations\n",
            "iteration: 548\ttotal 625 iterations\n",
            "iteration: 549\ttotal 625 iterations\n",
            "iteration: 550\ttotal 625 iterations\n",
            "iteration: 551\ttotal 625 iterations\n",
            "iteration: 552\ttotal 625 iterations\n",
            "iteration: 553\ttotal 625 iterations\n",
            "iteration: 554\ttotal 625 iterations\n",
            "iteration: 555\ttotal 625 iterations\n",
            "iteration: 556\ttotal 625 iterations\n",
            "iteration: 557\ttotal 625 iterations\n",
            "iteration: 558\ttotal 625 iterations\n",
            "iteration: 559\ttotal 625 iterations\n",
            "iteration: 560\ttotal 625 iterations\n",
            "iteration: 561\ttotal 625 iterations\n",
            "iteration: 562\ttotal 625 iterations\n",
            "iteration: 563\ttotal 625 iterations\n",
            "iteration: 564\ttotal 625 iterations\n",
            "iteration: 565\ttotal 625 iterations\n",
            "iteration: 566\ttotal 625 iterations\n",
            "iteration: 567\ttotal 625 iterations\n",
            "iteration: 568\ttotal 625 iterations\n",
            "iteration: 569\ttotal 625 iterations\n",
            "iteration: 570\ttotal 625 iterations\n",
            "iteration: 571\ttotal 625 iterations\n",
            "iteration: 572\ttotal 625 iterations\n",
            "iteration: 573\ttotal 625 iterations\n",
            "iteration: 574\ttotal 625 iterations\n",
            "iteration: 575\ttotal 625 iterations\n",
            "iteration: 576\ttotal 625 iterations\n",
            "iteration: 577\ttotal 625 iterations\n",
            "iteration: 578\ttotal 625 iterations\n",
            "iteration: 579\ttotal 625 iterations\n",
            "iteration: 580\ttotal 625 iterations\n",
            "iteration: 581\ttotal 625 iterations\n",
            "iteration: 582\ttotal 625 iterations\n",
            "iteration: 583\ttotal 625 iterations\n",
            "iteration: 584\ttotal 625 iterations\n",
            "iteration: 585\ttotal 625 iterations\n",
            "iteration: 586\ttotal 625 iterations\n",
            "iteration: 587\ttotal 625 iterations\n",
            "iteration: 588\ttotal 625 iterations\n",
            "iteration: 589\ttotal 625 iterations\n",
            "iteration: 590\ttotal 625 iterations\n",
            "iteration: 591\ttotal 625 iterations\n",
            "iteration: 592\ttotal 625 iterations\n",
            "iteration: 593\ttotal 625 iterations\n",
            "iteration: 594\ttotal 625 iterations\n",
            "iteration: 595\ttotal 625 iterations\n",
            "iteration: 596\ttotal 625 iterations\n",
            "iteration: 597\ttotal 625 iterations\n",
            "iteration: 598\ttotal 625 iterations\n",
            "iteration: 599\ttotal 625 iterations\n",
            "iteration: 600\ttotal 625 iterations\n",
            "iteration: 601\ttotal 625 iterations\n",
            "iteration: 602\ttotal 625 iterations\n",
            "iteration: 603\ttotal 625 iterations\n",
            "iteration: 604\ttotal 625 iterations\n",
            "iteration: 605\ttotal 625 iterations\n",
            "iteration: 606\ttotal 625 iterations\n",
            "iteration: 607\ttotal 625 iterations\n",
            "iteration: 608\ttotal 625 iterations\n",
            "iteration: 609\ttotal 625 iterations\n",
            "iteration: 610\ttotal 625 iterations\n",
            "iteration: 611\ttotal 625 iterations\n",
            "iteration: 612\ttotal 625 iterations\n",
            "iteration: 613\ttotal 625 iterations\n",
            "iteration: 614\ttotal 625 iterations\n",
            "iteration: 615\ttotal 625 iterations\n",
            "iteration: 616\ttotal 625 iterations\n",
            "iteration: 617\ttotal 625 iterations\n",
            "iteration: 618\ttotal 625 iterations\n",
            "iteration: 619\ttotal 625 iterations\n",
            "iteration: 620\ttotal 625 iterations\n",
            "iteration: 621\ttotal 625 iterations\n",
            "iteration: 622\ttotal 625 iterations\n",
            "iteration: 623\ttotal 625 iterations\n",
            "iteration: 624\ttotal 625 iterations\n",
            "iteration: 625\ttotal 625 iterations\n",
            "\n",
            "Top 1 err:  tensor(0.9900)\n",
            "Top 5 err:  tensor(0.9500)\n",
            "Parameter numbers: 6386500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -net xception -gpu"
      ],
      "metadata": {
        "id": "juJYALkJDMhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a7a19b-a567-4998-f633-cde607cfe40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 9 [40704/50000]\tLoss: 1.3190\tLR: 0.010000\n",
            "Training Epoch: 9 [40832/50000]\tLoss: 1.3979\tLR: 0.010000\n",
            "Training Epoch: 9 [40960/50000]\tLoss: 1.4878\tLR: 0.010000\n",
            "Training Epoch: 9 [41088/50000]\tLoss: 1.5323\tLR: 0.010000\n",
            "Training Epoch: 9 [41216/50000]\tLoss: 1.5315\tLR: 0.010000\n",
            "Training Epoch: 9 [41344/50000]\tLoss: 1.4169\tLR: 0.010000\n",
            "Training Epoch: 9 [41472/50000]\tLoss: 1.5597\tLR: 0.010000\n",
            "Training Epoch: 9 [41600/50000]\tLoss: 1.5074\tLR: 0.010000\n",
            "Training Epoch: 9 [41728/50000]\tLoss: 1.4383\tLR: 0.010000\n",
            "Training Epoch: 9 [41856/50000]\tLoss: 1.6493\tLR: 0.010000\n",
            "Training Epoch: 9 [41984/50000]\tLoss: 1.6612\tLR: 0.010000\n",
            "Training Epoch: 9 [42112/50000]\tLoss: 1.4091\tLR: 0.010000\n",
            "Training Epoch: 9 [42240/50000]\tLoss: 1.2151\tLR: 0.010000\n",
            "Training Epoch: 9 [42368/50000]\tLoss: 1.3127\tLR: 0.010000\n",
            "Training Epoch: 9 [42496/50000]\tLoss: 1.4121\tLR: 0.010000\n",
            "Training Epoch: 9 [42624/50000]\tLoss: 1.3585\tLR: 0.010000\n",
            "Training Epoch: 9 [42752/50000]\tLoss: 1.6406\tLR: 0.010000\n",
            "Training Epoch: 9 [42880/50000]\tLoss: 1.4577\tLR: 0.010000\n",
            "Training Epoch: 9 [43008/50000]\tLoss: 1.4610\tLR: 0.010000\n",
            "Training Epoch: 9 [43136/50000]\tLoss: 1.3257\tLR: 0.010000\n",
            "Training Epoch: 9 [43264/50000]\tLoss: 1.4837\tLR: 0.010000\n",
            "Training Epoch: 9 [43392/50000]\tLoss: 1.5997\tLR: 0.010000\n",
            "Training Epoch: 9 [43520/50000]\tLoss: 1.1898\tLR: 0.010000\n",
            "Training Epoch: 9 [43648/50000]\tLoss: 1.3865\tLR: 0.010000\n",
            "Training Epoch: 9 [43776/50000]\tLoss: 1.4903\tLR: 0.010000\n",
            "Training Epoch: 9 [43904/50000]\tLoss: 1.7676\tLR: 0.010000\n",
            "Training Epoch: 9 [44032/50000]\tLoss: 1.7377\tLR: 0.010000\n",
            "Training Epoch: 9 [44160/50000]\tLoss: 1.2658\tLR: 0.010000\n",
            "Training Epoch: 9 [44288/50000]\tLoss: 1.5215\tLR: 0.010000\n",
            "Training Epoch: 9 [44416/50000]\tLoss: 1.7255\tLR: 0.010000\n",
            "Training Epoch: 9 [44544/50000]\tLoss: 1.3834\tLR: 0.010000\n",
            "Training Epoch: 9 [44672/50000]\tLoss: 1.5857\tLR: 0.010000\n",
            "Training Epoch: 9 [44800/50000]\tLoss: 1.2427\tLR: 0.010000\n",
            "Training Epoch: 9 [44928/50000]\tLoss: 1.3716\tLR: 0.010000\n",
            "Training Epoch: 9 [45056/50000]\tLoss: 1.6310\tLR: 0.010000\n",
            "Training Epoch: 9 [45184/50000]\tLoss: 1.1785\tLR: 0.010000\n",
            "Training Epoch: 9 [45312/50000]\tLoss: 1.4905\tLR: 0.010000\n",
            "Training Epoch: 9 [45440/50000]\tLoss: 1.5686\tLR: 0.010000\n",
            "Training Epoch: 9 [45568/50000]\tLoss: 1.5487\tLR: 0.010000\n",
            "Training Epoch: 9 [45696/50000]\tLoss: 1.2169\tLR: 0.010000\n",
            "Training Epoch: 9 [45824/50000]\tLoss: 1.5590\tLR: 0.010000\n",
            "Training Epoch: 9 [45952/50000]\tLoss: 1.6019\tLR: 0.010000\n",
            "Training Epoch: 9 [46080/50000]\tLoss: 1.4974\tLR: 0.010000\n",
            "Training Epoch: 9 [46208/50000]\tLoss: 1.5505\tLR: 0.010000\n",
            "Training Epoch: 9 [46336/50000]\tLoss: 1.3648\tLR: 0.010000\n",
            "Training Epoch: 9 [46464/50000]\tLoss: 1.6906\tLR: 0.010000\n",
            "Training Epoch: 9 [46592/50000]\tLoss: 1.1416\tLR: 0.010000\n",
            "Training Epoch: 9 [46720/50000]\tLoss: 1.3718\tLR: 0.010000\n",
            "Training Epoch: 9 [46848/50000]\tLoss: 1.3448\tLR: 0.010000\n",
            "Training Epoch: 9 [46976/50000]\tLoss: 1.6564\tLR: 0.010000\n",
            "Training Epoch: 9 [47104/50000]\tLoss: 1.5284\tLR: 0.010000\n",
            "Training Epoch: 9 [47232/50000]\tLoss: 1.4972\tLR: 0.010000\n",
            "Training Epoch: 9 [47360/50000]\tLoss: 1.5005\tLR: 0.010000\n",
            "Training Epoch: 9 [47488/50000]\tLoss: 1.5229\tLR: 0.010000\n",
            "Training Epoch: 9 [47616/50000]\tLoss: 1.3362\tLR: 0.010000\n",
            "Training Epoch: 9 [47744/50000]\tLoss: 1.4943\tLR: 0.010000\n",
            "Training Epoch: 9 [47872/50000]\tLoss: 1.3615\tLR: 0.010000\n",
            "Training Epoch: 9 [48000/50000]\tLoss: 1.6973\tLR: 0.010000\n",
            "Training Epoch: 9 [48128/50000]\tLoss: 1.4455\tLR: 0.010000\n",
            "Training Epoch: 9 [48256/50000]\tLoss: 1.4751\tLR: 0.010000\n",
            "Training Epoch: 9 [48384/50000]\tLoss: 1.4246\tLR: 0.010000\n",
            "Training Epoch: 9 [48512/50000]\tLoss: 1.4640\tLR: 0.010000\n",
            "Training Epoch: 9 [48640/50000]\tLoss: 1.5091\tLR: 0.010000\n",
            "Training Epoch: 9 [48768/50000]\tLoss: 1.3331\tLR: 0.010000\n",
            "Training Epoch: 9 [48896/50000]\tLoss: 1.6400\tLR: 0.010000\n",
            "Training Epoch: 9 [49024/50000]\tLoss: 1.4954\tLR: 0.010000\n",
            "Training Epoch: 9 [49152/50000]\tLoss: 1.4642\tLR: 0.010000\n",
            "Training Epoch: 9 [49280/50000]\tLoss: 1.7451\tLR: 0.010000\n",
            "Training Epoch: 9 [49408/50000]\tLoss: 1.4832\tLR: 0.010000\n",
            "Training Epoch: 9 [49536/50000]\tLoss: 1.3775\tLR: 0.010000\n",
            "Training Epoch: 9 [49664/50000]\tLoss: 1.3849\tLR: 0.010000\n",
            "Training Epoch: 9 [49792/50000]\tLoss: 1.2747\tLR: 0.010000\n",
            "Training Epoch: 9 [49920/50000]\tLoss: 1.3340\tLR: 0.010000\n",
            "Training Epoch: 9 [50000/50000]\tLoss: 1.4073\tLR: 0.010000\n",
            "epoch 9 training time consumed: 205.95s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB |  75609 GiB |  75609 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  75554 GiB |  75554 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     54 GiB |     54 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB |  75609 GiB |  75609 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  75554 GiB |  75554 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     54 GiB |     54 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB |  75601 GiB |  75601 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB |  75547 GiB |  75547 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     54 GiB |     54 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  54903 GiB |  54902 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  54846 GiB |  54846 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     56 GiB |     56 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    3002 K  |    3001 K  |\n",
            "|       from large pool |      92    |     184    |    1692 K  |    1692 K  |\n",
            "|       from small pool |     519    |     646    |    1309 K  |    1309 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    3002 K  |    3001 K  |\n",
            "|       from large pool |      92    |     184    |    1692 K  |    1692 K  |\n",
            "|       from small pool |     519    |     646    |    1309 K  |    1309 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |     122    |    1266 K  |    1266 K  |\n",
            "|       from large pool |      42    |     111    |     999 K  |     999 K  |\n",
            "|       from small pool |       9    |      21    |     267 K  |     267 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0130, Accuracy: 0.5552, Time consumed:11.28s\n",
            "\n",
            "Training Epoch: 10 [128/50000]\tLoss: 1.2788\tLR: 0.010000\n",
            "Training Epoch: 10 [256/50000]\tLoss: 1.2538\tLR: 0.010000\n",
            "Training Epoch: 10 [384/50000]\tLoss: 1.5509\tLR: 0.010000\n",
            "Training Epoch: 10 [512/50000]\tLoss: 1.4669\tLR: 0.010000\n",
            "Training Epoch: 10 [640/50000]\tLoss: 1.3960\tLR: 0.010000\n",
            "Training Epoch: 10 [768/50000]\tLoss: 1.3430\tLR: 0.010000\n",
            "Training Epoch: 10 [896/50000]\tLoss: 1.1176\tLR: 0.010000\n",
            "Training Epoch: 10 [1024/50000]\tLoss: 1.2230\tLR: 0.010000\n",
            "Training Epoch: 10 [1152/50000]\tLoss: 1.4752\tLR: 0.010000\n",
            "Training Epoch: 10 [1280/50000]\tLoss: 1.1402\tLR: 0.010000\n",
            "Training Epoch: 10 [1408/50000]\tLoss: 1.1637\tLR: 0.010000\n",
            "Training Epoch: 10 [1536/50000]\tLoss: 1.1482\tLR: 0.010000\n",
            "Training Epoch: 10 [1664/50000]\tLoss: 1.1926\tLR: 0.010000\n",
            "Training Epoch: 10 [1792/50000]\tLoss: 1.4254\tLR: 0.010000\n",
            "Training Epoch: 10 [1920/50000]\tLoss: 1.2772\tLR: 0.010000\n",
            "Training Epoch: 10 [2048/50000]\tLoss: 1.3633\tLR: 0.010000\n",
            "Training Epoch: 10 [2176/50000]\tLoss: 1.4951\tLR: 0.010000\n",
            "Training Epoch: 10 [2304/50000]\tLoss: 1.3407\tLR: 0.010000\n",
            "Training Epoch: 10 [2432/50000]\tLoss: 1.3313\tLR: 0.010000\n",
            "Training Epoch: 10 [2560/50000]\tLoss: 1.3124\tLR: 0.010000\n",
            "Training Epoch: 10 [2688/50000]\tLoss: 1.1786\tLR: 0.010000\n",
            "Training Epoch: 10 [2816/50000]\tLoss: 1.4066\tLR: 0.010000\n",
            "Training Epoch: 10 [2944/50000]\tLoss: 1.2182\tLR: 0.010000\n",
            "Training Epoch: 10 [3072/50000]\tLoss: 1.3816\tLR: 0.010000\n",
            "Training Epoch: 10 [3200/50000]\tLoss: 1.2850\tLR: 0.010000\n",
            "Training Epoch: 10 [3328/50000]\tLoss: 1.3185\tLR: 0.010000\n",
            "Training Epoch: 10 [3456/50000]\tLoss: 1.4837\tLR: 0.010000\n",
            "Training Epoch: 10 [3584/50000]\tLoss: 1.3799\tLR: 0.010000\n",
            "Training Epoch: 10 [3712/50000]\tLoss: 1.3416\tLR: 0.010000\n",
            "Training Epoch: 10 [3840/50000]\tLoss: 1.3097\tLR: 0.010000\n",
            "Training Epoch: 10 [3968/50000]\tLoss: 1.3131\tLR: 0.010000\n",
            "Training Epoch: 10 [4096/50000]\tLoss: 1.1969\tLR: 0.010000\n",
            "Training Epoch: 10 [4224/50000]\tLoss: 0.9316\tLR: 0.010000\n",
            "Training Epoch: 10 [4352/50000]\tLoss: 1.1923\tLR: 0.010000\n",
            "Training Epoch: 10 [4480/50000]\tLoss: 1.3428\tLR: 0.010000\n",
            "Training Epoch: 10 [4608/50000]\tLoss: 1.2558\tLR: 0.010000\n",
            "Training Epoch: 10 [4736/50000]\tLoss: 1.3355\tLR: 0.010000\n",
            "Training Epoch: 10 [4864/50000]\tLoss: 1.3083\tLR: 0.010000\n",
            "Training Epoch: 10 [4992/50000]\tLoss: 1.3472\tLR: 0.010000\n",
            "Training Epoch: 10 [5120/50000]\tLoss: 1.2829\tLR: 0.010000\n",
            "Training Epoch: 10 [5248/50000]\tLoss: 1.5086\tLR: 0.010000\n",
            "Training Epoch: 10 [5376/50000]\tLoss: 1.3047\tLR: 0.010000\n",
            "Training Epoch: 10 [5504/50000]\tLoss: 1.3236\tLR: 0.010000\n",
            "Training Epoch: 10 [5632/50000]\tLoss: 1.5220\tLR: 0.010000\n",
            "Training Epoch: 10 [5760/50000]\tLoss: 1.2057\tLR: 0.010000\n",
            "Training Epoch: 10 [5888/50000]\tLoss: 1.3153\tLR: 0.010000\n",
            "Training Epoch: 10 [6016/50000]\tLoss: 1.5290\tLR: 0.010000\n",
            "Training Epoch: 10 [6144/50000]\tLoss: 1.4750\tLR: 0.010000\n",
            "Training Epoch: 10 [6272/50000]\tLoss: 1.2684\tLR: 0.010000\n",
            "Training Epoch: 10 [6400/50000]\tLoss: 1.2092\tLR: 0.010000\n",
            "Training Epoch: 10 [6528/50000]\tLoss: 1.1677\tLR: 0.010000\n",
            "Training Epoch: 10 [6656/50000]\tLoss: 1.2942\tLR: 0.010000\n",
            "Training Epoch: 10 [6784/50000]\tLoss: 1.3267\tLR: 0.010000\n",
            "Training Epoch: 10 [6912/50000]\tLoss: 1.3273\tLR: 0.010000\n",
            "Training Epoch: 10 [7040/50000]\tLoss: 1.3168\tLR: 0.010000\n",
            "Training Epoch: 10 [7168/50000]\tLoss: 1.3880\tLR: 0.010000\n",
            "Training Epoch: 10 [7296/50000]\tLoss: 1.2424\tLR: 0.010000\n",
            "Training Epoch: 10 [7424/50000]\tLoss: 1.5234\tLR: 0.010000\n",
            "Training Epoch: 10 [7552/50000]\tLoss: 1.2810\tLR: 0.010000\n",
            "Training Epoch: 10 [7680/50000]\tLoss: 1.3358\tLR: 0.010000\n",
            "Training Epoch: 10 [7808/50000]\tLoss: 1.1982\tLR: 0.010000\n",
            "Training Epoch: 10 [7936/50000]\tLoss: 1.3969\tLR: 0.010000\n",
            "Training Epoch: 10 [8064/50000]\tLoss: 1.2336\tLR: 0.010000\n",
            "Training Epoch: 10 [8192/50000]\tLoss: 1.3202\tLR: 0.010000\n",
            "Training Epoch: 10 [8320/50000]\tLoss: 1.2023\tLR: 0.010000\n",
            "Training Epoch: 10 [8448/50000]\tLoss: 1.2621\tLR: 0.010000\n",
            "Training Epoch: 10 [8576/50000]\tLoss: 1.0999\tLR: 0.010000\n",
            "Training Epoch: 10 [8704/50000]\tLoss: 1.5981\tLR: 0.010000\n",
            "Training Epoch: 10 [8832/50000]\tLoss: 1.4478\tLR: 0.010000\n",
            "Training Epoch: 10 [8960/50000]\tLoss: 1.3618\tLR: 0.010000\n",
            "Training Epoch: 10 [9088/50000]\tLoss: 1.5360\tLR: 0.010000\n",
            "Training Epoch: 10 [9216/50000]\tLoss: 1.4272\tLR: 0.010000\n",
            "Training Epoch: 10 [9344/50000]\tLoss: 1.1718\tLR: 0.010000\n",
            "Training Epoch: 10 [9472/50000]\tLoss: 1.3028\tLR: 0.010000\n",
            "Training Epoch: 10 [9600/50000]\tLoss: 1.3629\tLR: 0.010000\n",
            "Training Epoch: 10 [9728/50000]\tLoss: 1.4523\tLR: 0.010000\n",
            "Training Epoch: 10 [9856/50000]\tLoss: 1.1690\tLR: 0.010000\n",
            "Training Epoch: 10 [9984/50000]\tLoss: 0.9658\tLR: 0.010000\n",
            "Training Epoch: 10 [10112/50000]\tLoss: 1.3444\tLR: 0.010000\n",
            "Training Epoch: 10 [10240/50000]\tLoss: 1.4356\tLR: 0.010000\n",
            "Training Epoch: 10 [10368/50000]\tLoss: 1.4959\tLR: 0.010000\n",
            "Training Epoch: 10 [10496/50000]\tLoss: 1.1804\tLR: 0.010000\n",
            "Training Epoch: 10 [10624/50000]\tLoss: 1.2424\tLR: 0.010000\n",
            "Training Epoch: 10 [10752/50000]\tLoss: 1.3933\tLR: 0.010000\n",
            "Training Epoch: 10 [10880/50000]\tLoss: 1.1697\tLR: 0.010000\n",
            "Training Epoch: 10 [11008/50000]\tLoss: 1.3425\tLR: 0.010000\n",
            "Training Epoch: 10 [11136/50000]\tLoss: 1.2552\tLR: 0.010000\n",
            "Training Epoch: 10 [11264/50000]\tLoss: 1.3390\tLR: 0.010000\n",
            "Training Epoch: 10 [11392/50000]\tLoss: 1.5164\tLR: 0.010000\n",
            "Training Epoch: 10 [11520/50000]\tLoss: 1.2796\tLR: 0.010000\n",
            "Training Epoch: 10 [11648/50000]\tLoss: 1.3083\tLR: 0.010000\n",
            "Training Epoch: 10 [11776/50000]\tLoss: 1.1089\tLR: 0.010000\n",
            "Training Epoch: 10 [11904/50000]\tLoss: 1.1607\tLR: 0.010000\n",
            "Training Epoch: 10 [12032/50000]\tLoss: 1.5709\tLR: 0.010000\n",
            "Training Epoch: 10 [12160/50000]\tLoss: 1.2596\tLR: 0.010000\n",
            "Training Epoch: 10 [12288/50000]\tLoss: 1.5348\tLR: 0.010000\n",
            "Training Epoch: 10 [12416/50000]\tLoss: 1.4831\tLR: 0.010000\n",
            "Training Epoch: 10 [12544/50000]\tLoss: 1.2683\tLR: 0.010000\n",
            "Training Epoch: 10 [12672/50000]\tLoss: 1.3697\tLR: 0.010000\n",
            "Training Epoch: 10 [12800/50000]\tLoss: 1.2058\tLR: 0.010000\n",
            "Training Epoch: 10 [12928/50000]\tLoss: 1.2604\tLR: 0.010000\n",
            "Training Epoch: 10 [13056/50000]\tLoss: 1.2437\tLR: 0.010000\n",
            "Training Epoch: 10 [13184/50000]\tLoss: 1.4620\tLR: 0.010000\n",
            "Training Epoch: 10 [13312/50000]\tLoss: 1.4275\tLR: 0.010000\n",
            "Training Epoch: 10 [13440/50000]\tLoss: 1.2089\tLR: 0.010000\n",
            "Training Epoch: 10 [13568/50000]\tLoss: 1.1794\tLR: 0.010000\n",
            "Training Epoch: 10 [13696/50000]\tLoss: 1.4655\tLR: 0.010000\n",
            "Training Epoch: 10 [13824/50000]\tLoss: 1.0769\tLR: 0.010000\n",
            "Training Epoch: 10 [13952/50000]\tLoss: 1.1692\tLR: 0.010000\n",
            "Training Epoch: 10 [14080/50000]\tLoss: 1.4188\tLR: 0.010000\n",
            "Training Epoch: 10 [14208/50000]\tLoss: 1.4352\tLR: 0.010000\n",
            "Training Epoch: 10 [14336/50000]\tLoss: 1.5052\tLR: 0.010000\n",
            "Training Epoch: 10 [14464/50000]\tLoss: 1.1996\tLR: 0.010000\n",
            "Training Epoch: 10 [14592/50000]\tLoss: 1.1942\tLR: 0.010000\n",
            "Training Epoch: 10 [14720/50000]\tLoss: 1.3850\tLR: 0.010000\n",
            "Training Epoch: 10 [14848/50000]\tLoss: 1.3946\tLR: 0.010000\n",
            "Training Epoch: 10 [14976/50000]\tLoss: 1.6701\tLR: 0.010000\n",
            "Training Epoch: 10 [15104/50000]\tLoss: 1.6122\tLR: 0.010000\n",
            "Training Epoch: 10 [15232/50000]\tLoss: 1.2846\tLR: 0.010000\n",
            "Training Epoch: 10 [15360/50000]\tLoss: 1.3524\tLR: 0.010000\n",
            "Training Epoch: 10 [15488/50000]\tLoss: 1.3014\tLR: 0.010000\n",
            "Training Epoch: 10 [15616/50000]\tLoss: 1.1471\tLR: 0.010000\n",
            "Training Epoch: 10 [15744/50000]\tLoss: 1.3278\tLR: 0.010000\n",
            "Training Epoch: 10 [15872/50000]\tLoss: 1.2548\tLR: 0.010000\n",
            "Training Epoch: 10 [16000/50000]\tLoss: 1.2310\tLR: 0.010000\n",
            "Training Epoch: 10 [16128/50000]\tLoss: 1.1185\tLR: 0.010000\n",
            "Training Epoch: 10 [16256/50000]\tLoss: 1.3077\tLR: 0.010000\n",
            "Training Epoch: 10 [16384/50000]\tLoss: 1.2696\tLR: 0.010000\n",
            "Training Epoch: 10 [16512/50000]\tLoss: 1.2794\tLR: 0.010000\n",
            "Training Epoch: 10 [16640/50000]\tLoss: 1.4721\tLR: 0.010000\n",
            "Training Epoch: 10 [16768/50000]\tLoss: 1.3910\tLR: 0.010000\n",
            "Training Epoch: 10 [16896/50000]\tLoss: 1.2719\tLR: 0.010000\n",
            "Training Epoch: 10 [17024/50000]\tLoss: 1.4192\tLR: 0.010000\n",
            "Training Epoch: 10 [17152/50000]\tLoss: 1.2644\tLR: 0.010000\n",
            "Training Epoch: 10 [17280/50000]\tLoss: 1.3069\tLR: 0.010000\n",
            "Training Epoch: 10 [17408/50000]\tLoss: 1.2939\tLR: 0.010000\n",
            "Training Epoch: 10 [17536/50000]\tLoss: 1.3733\tLR: 0.010000\n",
            "Training Epoch: 10 [17664/50000]\tLoss: 1.4894\tLR: 0.010000\n",
            "Training Epoch: 10 [17792/50000]\tLoss: 1.4604\tLR: 0.010000\n",
            "Training Epoch: 10 [17920/50000]\tLoss: 1.2127\tLR: 0.010000\n",
            "Training Epoch: 10 [18048/50000]\tLoss: 1.5116\tLR: 0.010000\n",
            "Training Epoch: 10 [18176/50000]\tLoss: 1.3841\tLR: 0.010000\n",
            "Training Epoch: 10 [18304/50000]\tLoss: 1.3063\tLR: 0.010000\n",
            "Training Epoch: 10 [18432/50000]\tLoss: 1.2738\tLR: 0.010000\n",
            "Training Epoch: 10 [18560/50000]\tLoss: 1.3433\tLR: 0.010000\n",
            "Training Epoch: 10 [18688/50000]\tLoss: 1.2813\tLR: 0.010000\n",
            "Training Epoch: 10 [18816/50000]\tLoss: 1.3043\tLR: 0.010000\n",
            "Training Epoch: 10 [18944/50000]\tLoss: 1.2256\tLR: 0.010000\n",
            "Training Epoch: 10 [19072/50000]\tLoss: 1.3891\tLR: 0.010000\n",
            "Training Epoch: 10 [19200/50000]\tLoss: 1.4719\tLR: 0.010000\n",
            "Training Epoch: 10 [19328/50000]\tLoss: 1.3863\tLR: 0.010000\n",
            "Training Epoch: 10 [19456/50000]\tLoss: 1.3157\tLR: 0.010000\n",
            "Training Epoch: 10 [19584/50000]\tLoss: 1.2851\tLR: 0.010000\n",
            "Training Epoch: 10 [19712/50000]\tLoss: 1.3269\tLR: 0.010000\n",
            "Training Epoch: 10 [19840/50000]\tLoss: 1.4859\tLR: 0.010000\n",
            "Training Epoch: 10 [19968/50000]\tLoss: 1.2635\tLR: 0.010000\n",
            "Training Epoch: 10 [20096/50000]\tLoss: 1.3933\tLR: 0.010000\n",
            "Training Epoch: 10 [20224/50000]\tLoss: 1.7217\tLR: 0.010000\n",
            "Training Epoch: 10 [20352/50000]\tLoss: 1.5271\tLR: 0.010000\n",
            "Training Epoch: 10 [20480/50000]\tLoss: 1.3777\tLR: 0.010000\n",
            "Training Epoch: 10 [20608/50000]\tLoss: 1.2678\tLR: 0.010000\n",
            "Training Epoch: 10 [20736/50000]\tLoss: 1.4268\tLR: 0.010000\n",
            "Training Epoch: 10 [20864/50000]\tLoss: 1.1912\tLR: 0.010000\n",
            "Training Epoch: 10 [20992/50000]\tLoss: 1.5873\tLR: 0.010000\n",
            "Training Epoch: 10 [21120/50000]\tLoss: 1.4228\tLR: 0.010000\n",
            "Training Epoch: 10 [21248/50000]\tLoss: 1.5233\tLR: 0.010000\n",
            "Training Epoch: 10 [21376/50000]\tLoss: 1.2574\tLR: 0.010000\n",
            "Training Epoch: 10 [21504/50000]\tLoss: 1.3456\tLR: 0.010000\n",
            "Training Epoch: 10 [21632/50000]\tLoss: 1.2719\tLR: 0.010000\n",
            "Training Epoch: 10 [21760/50000]\tLoss: 1.3090\tLR: 0.010000\n",
            "Training Epoch: 10 [21888/50000]\tLoss: 1.3913\tLR: 0.010000\n",
            "Training Epoch: 10 [22016/50000]\tLoss: 1.3712\tLR: 0.010000\n",
            "Training Epoch: 10 [22144/50000]\tLoss: 1.3049\tLR: 0.010000\n",
            "Training Epoch: 10 [22272/50000]\tLoss: 1.4582\tLR: 0.010000\n",
            "Training Epoch: 10 [22400/50000]\tLoss: 1.2937\tLR: 0.010000\n",
            "Training Epoch: 10 [22528/50000]\tLoss: 1.4101\tLR: 0.010000\n",
            "Training Epoch: 10 [22656/50000]\tLoss: 1.3172\tLR: 0.010000\n",
            "Training Epoch: 10 [22784/50000]\tLoss: 1.4438\tLR: 0.010000\n",
            "Training Epoch: 10 [22912/50000]\tLoss: 1.5187\tLR: 0.010000\n",
            "Training Epoch: 10 [23040/50000]\tLoss: 1.2594\tLR: 0.010000\n",
            "Training Epoch: 10 [23168/50000]\tLoss: 1.3624\tLR: 0.010000\n",
            "Training Epoch: 10 [23296/50000]\tLoss: 1.3525\tLR: 0.010000\n",
            "Training Epoch: 10 [23424/50000]\tLoss: 1.5076\tLR: 0.010000\n",
            "Training Epoch: 10 [23552/50000]\tLoss: 1.4940\tLR: 0.010000\n",
            "Training Epoch: 10 [23680/50000]\tLoss: 1.2260\tLR: 0.010000\n",
            "Training Epoch: 10 [23808/50000]\tLoss: 1.3863\tLR: 0.010000\n",
            "Training Epoch: 10 [23936/50000]\tLoss: 1.4880\tLR: 0.010000\n",
            "Training Epoch: 10 [24064/50000]\tLoss: 1.4188\tLR: 0.010000\n",
            "Training Epoch: 10 [24192/50000]\tLoss: 1.3702\tLR: 0.010000\n",
            "Training Epoch: 10 [24320/50000]\tLoss: 1.3770\tLR: 0.010000\n",
            "Training Epoch: 10 [24448/50000]\tLoss: 1.4748\tLR: 0.010000\n",
            "Training Epoch: 10 [24576/50000]\tLoss: 1.3327\tLR: 0.010000\n",
            "Training Epoch: 10 [24704/50000]\tLoss: 1.2994\tLR: 0.010000\n",
            "Training Epoch: 10 [24832/50000]\tLoss: 1.1405\tLR: 0.010000\n",
            "Training Epoch: 10 [24960/50000]\tLoss: 1.3270\tLR: 0.010000\n",
            "Training Epoch: 10 [25088/50000]\tLoss: 1.1813\tLR: 0.010000\n",
            "Training Epoch: 10 [25216/50000]\tLoss: 1.3436\tLR: 0.010000\n",
            "Training Epoch: 10 [25344/50000]\tLoss: 1.2345\tLR: 0.010000\n",
            "Training Epoch: 10 [25472/50000]\tLoss: 1.2114\tLR: 0.010000\n",
            "Training Epoch: 10 [25600/50000]\tLoss: 1.6443\tLR: 0.010000\n",
            "Training Epoch: 10 [25728/50000]\tLoss: 1.4005\tLR: 0.010000\n",
            "Training Epoch: 10 [25856/50000]\tLoss: 1.3246\tLR: 0.010000\n",
            "Training Epoch: 10 [25984/50000]\tLoss: 1.4967\tLR: 0.010000\n",
            "Training Epoch: 10 [26112/50000]\tLoss: 1.1915\tLR: 0.010000\n",
            "Training Epoch: 10 [26240/50000]\tLoss: 1.4800\tLR: 0.010000\n",
            "Training Epoch: 10 [26368/50000]\tLoss: 1.4401\tLR: 0.010000\n",
            "Training Epoch: 10 [26496/50000]\tLoss: 1.7339\tLR: 0.010000\n",
            "Training Epoch: 10 [26624/50000]\tLoss: 1.2096\tLR: 0.010000\n",
            "Training Epoch: 10 [26752/50000]\tLoss: 1.2825\tLR: 0.010000\n",
            "Training Epoch: 10 [26880/50000]\tLoss: 1.3070\tLR: 0.010000\n",
            "Training Epoch: 10 [27008/50000]\tLoss: 1.2713\tLR: 0.010000\n",
            "Training Epoch: 10 [27136/50000]\tLoss: 1.2582\tLR: 0.010000\n",
            "Training Epoch: 10 [27264/50000]\tLoss: 1.1199\tLR: 0.010000\n",
            "Training Epoch: 10 [27392/50000]\tLoss: 1.4888\tLR: 0.010000\n",
            "Training Epoch: 10 [27520/50000]\tLoss: 1.4427\tLR: 0.010000\n",
            "Training Epoch: 10 [27648/50000]\tLoss: 1.5139\tLR: 0.010000\n",
            "Training Epoch: 10 [27776/50000]\tLoss: 1.3481\tLR: 0.010000\n",
            "Training Epoch: 10 [27904/50000]\tLoss: 1.3061\tLR: 0.010000\n",
            "Training Epoch: 10 [28032/50000]\tLoss: 1.1276\tLR: 0.010000\n",
            "Training Epoch: 10 [28160/50000]\tLoss: 1.2439\tLR: 0.010000\n",
            "Training Epoch: 10 [28288/50000]\tLoss: 1.5497\tLR: 0.010000\n",
            "Training Epoch: 10 [28416/50000]\tLoss: 1.3111\tLR: 0.010000\n",
            "Training Epoch: 10 [28544/50000]\tLoss: 1.6443\tLR: 0.010000\n",
            "Training Epoch: 10 [28672/50000]\tLoss: 1.2816\tLR: 0.010000\n",
            "Training Epoch: 10 [28800/50000]\tLoss: 1.4589\tLR: 0.010000\n",
            "Training Epoch: 10 [28928/50000]\tLoss: 1.4438\tLR: 0.010000\n",
            "Training Epoch: 10 [29056/50000]\tLoss: 1.4404\tLR: 0.010000\n",
            "Training Epoch: 10 [29184/50000]\tLoss: 1.2757\tLR: 0.010000\n",
            "Training Epoch: 10 [29312/50000]\tLoss: 1.4699\tLR: 0.010000\n",
            "Training Epoch: 10 [29440/50000]\tLoss: 1.5462\tLR: 0.010000\n",
            "Training Epoch: 10 [29568/50000]\tLoss: 1.3995\tLR: 0.010000\n",
            "Training Epoch: 10 [29696/50000]\tLoss: 1.4257\tLR: 0.010000\n",
            "Training Epoch: 10 [29824/50000]\tLoss: 1.3007\tLR: 0.010000\n",
            "Training Epoch: 10 [29952/50000]\tLoss: 1.3712\tLR: 0.010000\n",
            "Training Epoch: 10 [30080/50000]\tLoss: 1.3767\tLR: 0.010000\n",
            "Training Epoch: 10 [30208/50000]\tLoss: 1.1820\tLR: 0.010000\n",
            "Training Epoch: 10 [30336/50000]\tLoss: 1.6145\tLR: 0.010000\n",
            "Training Epoch: 10 [30464/50000]\tLoss: 1.6326\tLR: 0.010000\n",
            "Training Epoch: 10 [30592/50000]\tLoss: 1.3253\tLR: 0.010000\n",
            "Training Epoch: 10 [30720/50000]\tLoss: 1.2936\tLR: 0.010000\n",
            "Training Epoch: 10 [30848/50000]\tLoss: 1.6677\tLR: 0.010000\n",
            "Training Epoch: 10 [30976/50000]\tLoss: 1.3910\tLR: 0.010000\n",
            "Training Epoch: 10 [31104/50000]\tLoss: 1.4307\tLR: 0.010000\n",
            "Training Epoch: 10 [31232/50000]\tLoss: 1.4427\tLR: 0.010000\n",
            "Training Epoch: 10 [31360/50000]\tLoss: 1.2213\tLR: 0.010000\n",
            "Training Epoch: 10 [31488/50000]\tLoss: 1.4022\tLR: 0.010000\n",
            "Training Epoch: 10 [31616/50000]\tLoss: 1.4356\tLR: 0.010000\n",
            "Training Epoch: 10 [31744/50000]\tLoss: 1.4872\tLR: 0.010000\n",
            "Training Epoch: 10 [31872/50000]\tLoss: 1.3570\tLR: 0.010000\n",
            "Training Epoch: 10 [32000/50000]\tLoss: 1.3480\tLR: 0.010000\n",
            "Training Epoch: 10 [32128/50000]\tLoss: 1.4914\tLR: 0.010000\n",
            "Training Epoch: 10 [32256/50000]\tLoss: 1.2651\tLR: 0.010000\n",
            "Training Epoch: 10 [32384/50000]\tLoss: 1.3125\tLR: 0.010000\n",
            "Training Epoch: 10 [32512/50000]\tLoss: 1.3732\tLR: 0.010000\n",
            "Training Epoch: 10 [32640/50000]\tLoss: 1.2853\tLR: 0.010000\n",
            "Training Epoch: 10 [32768/50000]\tLoss: 1.4788\tLR: 0.010000\n",
            "Training Epoch: 10 [32896/50000]\tLoss: 1.1216\tLR: 0.010000\n",
            "Training Epoch: 10 [33024/50000]\tLoss: 1.1906\tLR: 0.010000\n",
            "Training Epoch: 10 [33152/50000]\tLoss: 1.3776\tLR: 0.010000\n",
            "Training Epoch: 10 [33280/50000]\tLoss: 1.4936\tLR: 0.010000\n",
            "Training Epoch: 10 [33408/50000]\tLoss: 1.4216\tLR: 0.010000\n",
            "Training Epoch: 10 [33536/50000]\tLoss: 1.4964\tLR: 0.010000\n",
            "Training Epoch: 10 [33664/50000]\tLoss: 1.4254\tLR: 0.010000\n",
            "Training Epoch: 10 [33792/50000]\tLoss: 1.5490\tLR: 0.010000\n",
            "Training Epoch: 10 [33920/50000]\tLoss: 1.3494\tLR: 0.010000\n",
            "Training Epoch: 10 [34048/50000]\tLoss: 1.4586\tLR: 0.010000\n",
            "Training Epoch: 10 [34176/50000]\tLoss: 1.1290\tLR: 0.010000\n",
            "Training Epoch: 10 [34304/50000]\tLoss: 1.2194\tLR: 0.010000\n",
            "Training Epoch: 10 [34432/50000]\tLoss: 1.3579\tLR: 0.010000\n",
            "Training Epoch: 10 [34560/50000]\tLoss: 1.5529\tLR: 0.010000\n",
            "Training Epoch: 10 [34688/50000]\tLoss: 1.2461\tLR: 0.010000\n",
            "Training Epoch: 10 [34816/50000]\tLoss: 1.4130\tLR: 0.010000\n",
            "Training Epoch: 10 [34944/50000]\tLoss: 1.2805\tLR: 0.010000\n",
            "Training Epoch: 10 [35072/50000]\tLoss: 1.5763\tLR: 0.010000\n",
            "Training Epoch: 10 [35200/50000]\tLoss: 1.4163\tLR: 0.010000\n",
            "Training Epoch: 10 [35328/50000]\tLoss: 1.2801\tLR: 0.010000\n",
            "Training Epoch: 10 [35456/50000]\tLoss: 1.3127\tLR: 0.010000\n",
            "Training Epoch: 10 [35584/50000]\tLoss: 1.2264\tLR: 0.010000\n",
            "Training Epoch: 10 [35712/50000]\tLoss: 1.3016\tLR: 0.010000\n",
            "Training Epoch: 10 [35840/50000]\tLoss: 1.4139\tLR: 0.010000\n",
            "Training Epoch: 10 [35968/50000]\tLoss: 1.6155\tLR: 0.010000\n",
            "Training Epoch: 10 [36096/50000]\tLoss: 1.3555\tLR: 0.010000\n",
            "Training Epoch: 10 [36224/50000]\tLoss: 1.5179\tLR: 0.010000\n",
            "Training Epoch: 10 [36352/50000]\tLoss: 1.4031\tLR: 0.010000\n",
            "Training Epoch: 10 [36480/50000]\tLoss: 1.4642\tLR: 0.010000\n",
            "Training Epoch: 10 [36608/50000]\tLoss: 1.3639\tLR: 0.010000\n",
            "Training Epoch: 10 [36736/50000]\tLoss: 1.2958\tLR: 0.010000\n",
            "Training Epoch: 10 [36864/50000]\tLoss: 1.2748\tLR: 0.010000\n",
            "Training Epoch: 10 [36992/50000]\tLoss: 1.4607\tLR: 0.010000\n",
            "Training Epoch: 10 [37120/50000]\tLoss: 1.3758\tLR: 0.010000\n",
            "Training Epoch: 10 [37248/50000]\tLoss: 1.5470\tLR: 0.010000\n",
            "Training Epoch: 10 [37376/50000]\tLoss: 1.5553\tLR: 0.010000\n",
            "Training Epoch: 10 [37504/50000]\tLoss: 1.2657\tLR: 0.010000\n",
            "Training Epoch: 10 [37632/50000]\tLoss: 1.2580\tLR: 0.010000\n",
            "Training Epoch: 10 [37760/50000]\tLoss: 1.3894\tLR: 0.010000\n",
            "Training Epoch: 10 [37888/50000]\tLoss: 1.4632\tLR: 0.010000\n",
            "Training Epoch: 10 [38016/50000]\tLoss: 1.2333\tLR: 0.010000\n",
            "Training Epoch: 10 [38144/50000]\tLoss: 1.3638\tLR: 0.010000\n",
            "Training Epoch: 10 [38272/50000]\tLoss: 1.9019\tLR: 0.010000\n",
            "Training Epoch: 10 [38400/50000]\tLoss: 1.3724\tLR: 0.010000\n",
            "Training Epoch: 10 [38528/50000]\tLoss: 1.3017\tLR: 0.010000\n",
            "Training Epoch: 10 [38656/50000]\tLoss: 1.4709\tLR: 0.010000\n",
            "Training Epoch: 10 [38784/50000]\tLoss: 1.5704\tLR: 0.010000\n",
            "Training Epoch: 10 [38912/50000]\tLoss: 1.2497\tLR: 0.010000\n",
            "Training Epoch: 10 [39040/50000]\tLoss: 1.2352\tLR: 0.010000\n",
            "Training Epoch: 10 [39168/50000]\tLoss: 1.6407\tLR: 0.010000\n",
            "Training Epoch: 10 [39296/50000]\tLoss: 1.3189\tLR: 0.010000\n",
            "Training Epoch: 10 [39424/50000]\tLoss: 1.3765\tLR: 0.010000\n",
            "Training Epoch: 10 [39552/50000]\tLoss: 1.2949\tLR: 0.010000\n",
            "Training Epoch: 10 [39680/50000]\tLoss: 1.2968\tLR: 0.010000\n",
            "Training Epoch: 10 [39808/50000]\tLoss: 1.4765\tLR: 0.010000\n",
            "Training Epoch: 10 [39936/50000]\tLoss: 1.3507\tLR: 0.010000\n",
            "Training Epoch: 10 [40064/50000]\tLoss: 1.5396\tLR: 0.010000\n",
            "Training Epoch: 10 [40192/50000]\tLoss: 1.6719\tLR: 0.010000\n",
            "Training Epoch: 10 [40320/50000]\tLoss: 1.5895\tLR: 0.010000\n",
            "Training Epoch: 10 [40448/50000]\tLoss: 1.1712\tLR: 0.010000\n",
            "Training Epoch: 10 [40576/50000]\tLoss: 1.3813\tLR: 0.010000\n",
            "Training Epoch: 10 [40704/50000]\tLoss: 1.5683\tLR: 0.010000\n",
            "Training Epoch: 10 [40832/50000]\tLoss: 1.2797\tLR: 0.010000\n",
            "Training Epoch: 10 [40960/50000]\tLoss: 1.5473\tLR: 0.010000\n",
            "Training Epoch: 10 [41088/50000]\tLoss: 1.4322\tLR: 0.010000\n",
            "Training Epoch: 10 [41216/50000]\tLoss: 1.4509\tLR: 0.010000\n",
            "Training Epoch: 10 [41344/50000]\tLoss: 1.3255\tLR: 0.010000\n",
            "Training Epoch: 10 [41472/50000]\tLoss: 1.4367\tLR: 0.010000\n",
            "Training Epoch: 10 [41600/50000]\tLoss: 1.3232\tLR: 0.010000\n",
            "Training Epoch: 10 [41728/50000]\tLoss: 1.5988\tLR: 0.010000\n",
            "Training Epoch: 10 [41856/50000]\tLoss: 1.4637\tLR: 0.010000\n",
            "Training Epoch: 10 [41984/50000]\tLoss: 1.4851\tLR: 0.010000\n",
            "Training Epoch: 10 [42112/50000]\tLoss: 1.3259\tLR: 0.010000\n",
            "Training Epoch: 10 [42240/50000]\tLoss: 1.2097\tLR: 0.010000\n",
            "Training Epoch: 10 [42368/50000]\tLoss: 1.3660\tLR: 0.010000\n",
            "Training Epoch: 10 [42496/50000]\tLoss: 1.4753\tLR: 0.010000\n",
            "Training Epoch: 10 [42624/50000]\tLoss: 1.3473\tLR: 0.010000\n",
            "Training Epoch: 10 [42752/50000]\tLoss: 1.3011\tLR: 0.010000\n",
            "Training Epoch: 10 [42880/50000]\tLoss: 1.6155\tLR: 0.010000\n",
            "Training Epoch: 10 [43008/50000]\tLoss: 1.4411\tLR: 0.010000\n",
            "Training Epoch: 10 [43136/50000]\tLoss: 1.3952\tLR: 0.010000\n",
            "Training Epoch: 10 [43264/50000]\tLoss: 1.4128\tLR: 0.010000\n",
            "Training Epoch: 10 [43392/50000]\tLoss: 1.6394\tLR: 0.010000\n",
            "Training Epoch: 10 [43520/50000]\tLoss: 1.4518\tLR: 0.010000\n",
            "Training Epoch: 10 [43648/50000]\tLoss: 1.3058\tLR: 0.010000\n",
            "Training Epoch: 10 [43776/50000]\tLoss: 1.3221\tLR: 0.010000\n",
            "Training Epoch: 10 [43904/50000]\tLoss: 1.5926\tLR: 0.010000\n",
            "Training Epoch: 10 [44032/50000]\tLoss: 1.3607\tLR: 0.010000\n",
            "Training Epoch: 10 [44160/50000]\tLoss: 1.1174\tLR: 0.010000\n",
            "Training Epoch: 10 [44288/50000]\tLoss: 1.3129\tLR: 0.010000\n",
            "Training Epoch: 10 [44416/50000]\tLoss: 1.3224\tLR: 0.010000\n",
            "Training Epoch: 10 [44544/50000]\tLoss: 1.5820\tLR: 0.010000\n",
            "Training Epoch: 10 [44672/50000]\tLoss: 1.2674\tLR: 0.010000\n",
            "Training Epoch: 10 [44800/50000]\tLoss: 1.4211\tLR: 0.010000\n",
            "Training Epoch: 10 [44928/50000]\tLoss: 1.6883\tLR: 0.010000\n",
            "Training Epoch: 10 [45056/50000]\tLoss: 1.5936\tLR: 0.010000\n",
            "Training Epoch: 10 [45184/50000]\tLoss: 1.6108\tLR: 0.010000\n",
            "Training Epoch: 10 [45312/50000]\tLoss: 1.3139\tLR: 0.010000\n",
            "Training Epoch: 10 [45440/50000]\tLoss: 1.4807\tLR: 0.010000\n",
            "Training Epoch: 10 [45568/50000]\tLoss: 1.3773\tLR: 0.010000\n",
            "Training Epoch: 10 [45696/50000]\tLoss: 1.5348\tLR: 0.010000\n",
            "Training Epoch: 10 [45824/50000]\tLoss: 1.3536\tLR: 0.010000\n",
            "Training Epoch: 10 [45952/50000]\tLoss: 1.2990\tLR: 0.010000\n",
            "Training Epoch: 10 [46080/50000]\tLoss: 1.4707\tLR: 0.010000\n",
            "Training Epoch: 10 [46208/50000]\tLoss: 1.4762\tLR: 0.010000\n",
            "Training Epoch: 10 [46336/50000]\tLoss: 1.3338\tLR: 0.010000\n",
            "Training Epoch: 10 [46464/50000]\tLoss: 1.2065\tLR: 0.010000\n",
            "Training Epoch: 10 [46592/50000]\tLoss: 1.3852\tLR: 0.010000\n",
            "Training Epoch: 10 [46720/50000]\tLoss: 1.5464\tLR: 0.010000\n",
            "Training Epoch: 10 [46848/50000]\tLoss: 1.2985\tLR: 0.010000\n",
            "Training Epoch: 10 [46976/50000]\tLoss: 1.2254\tLR: 0.010000\n",
            "Training Epoch: 10 [47104/50000]\tLoss: 1.2466\tLR: 0.010000\n",
            "Training Epoch: 10 [47232/50000]\tLoss: 1.4315\tLR: 0.010000\n",
            "Training Epoch: 10 [47360/50000]\tLoss: 1.3825\tLR: 0.010000\n",
            "Training Epoch: 10 [47488/50000]\tLoss: 1.2901\tLR: 0.010000\n",
            "Training Epoch: 10 [47616/50000]\tLoss: 1.0714\tLR: 0.010000\n",
            "Training Epoch: 10 [47744/50000]\tLoss: 1.1306\tLR: 0.010000\n",
            "Training Epoch: 10 [47872/50000]\tLoss: 1.3510\tLR: 0.010000\n",
            "Training Epoch: 10 [48000/50000]\tLoss: 1.2994\tLR: 0.010000\n",
            "Training Epoch: 10 [48128/50000]\tLoss: 1.2085\tLR: 0.010000\n",
            "Training Epoch: 10 [48256/50000]\tLoss: 1.4635\tLR: 0.010000\n",
            "Training Epoch: 10 [48384/50000]\tLoss: 1.4503\tLR: 0.010000\n",
            "Training Epoch: 10 [48512/50000]\tLoss: 1.3292\tLR: 0.010000\n",
            "Training Epoch: 10 [48640/50000]\tLoss: 1.4523\tLR: 0.010000\n",
            "Training Epoch: 10 [48768/50000]\tLoss: 1.5559\tLR: 0.010000\n",
            "Training Epoch: 10 [48896/50000]\tLoss: 1.5394\tLR: 0.010000\n",
            "Training Epoch: 10 [49024/50000]\tLoss: 1.4396\tLR: 0.010000\n",
            "Training Epoch: 10 [49152/50000]\tLoss: 1.1546\tLR: 0.010000\n",
            "Training Epoch: 10 [49280/50000]\tLoss: 1.2371\tLR: 0.010000\n",
            "Training Epoch: 10 [49408/50000]\tLoss: 1.3479\tLR: 0.010000\n",
            "Training Epoch: 10 [49536/50000]\tLoss: 1.7028\tLR: 0.010000\n",
            "Training Epoch: 10 [49664/50000]\tLoss: 1.2853\tLR: 0.010000\n",
            "Training Epoch: 10 [49792/50000]\tLoss: 1.4973\tLR: 0.010000\n",
            "Training Epoch: 10 [49920/50000]\tLoss: 1.2122\tLR: 0.010000\n",
            "Training Epoch: 10 [50000/50000]\tLoss: 1.4712\tLR: 0.010000\n",
            "epoch 10 training time consumed: 206.49s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB |  84008 GiB |  84008 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  83947 GiB |  83947 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB |  84008 GiB |  84008 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  83947 GiB |  83947 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB |  83999 GiB |  83999 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB |  83939 GiB |  83939 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     60 GiB |     60 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  61003 GiB |  61002 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  60940 GiB |  60940 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     62 GiB |     62 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    3335 K  |    3335 K  |\n",
            "|       from large pool |      92    |     184    |    1880 K  |    1880 K  |\n",
            "|       from small pool |     519    |     646    |    1455 K  |    1454 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    3335 K  |    3335 K  |\n",
            "|       from large pool |      92    |     184    |    1880 K  |    1880 K  |\n",
            "|       from small pool |     519    |     646    |    1455 K  |    1454 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |     122    |    1407 K  |    1407 K  |\n",
            "|       from large pool |      42    |     111    |    1110 K  |    1110 K  |\n",
            "|       from small pool |       8    |      21    |     297 K  |     297 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0123, Accuracy: 0.5732, Time consumed:11.36s\n",
            "\n",
            "saving weights file to checkpoint/xception/Tuesday_25_July_2023_11h_34m_35s/xception-10-regular.pth\n",
            "Training Epoch: 11 [128/50000]\tLoss: 1.3526\tLR: 0.010000\n",
            "Training Epoch: 11 [256/50000]\tLoss: 1.3637\tLR: 0.010000\n",
            "Training Epoch: 11 [384/50000]\tLoss: 1.4879\tLR: 0.010000\n",
            "Training Epoch: 11 [512/50000]\tLoss: 1.2353\tLR: 0.010000\n",
            "Training Epoch: 11 [640/50000]\tLoss: 1.0088\tLR: 0.010000\n",
            "Training Epoch: 11 [768/50000]\tLoss: 1.1917\tLR: 0.010000\n",
            "Training Epoch: 11 [896/50000]\tLoss: 1.3140\tLR: 0.010000\n",
            "Training Epoch: 11 [1024/50000]\tLoss: 1.3700\tLR: 0.010000\n",
            "Training Epoch: 11 [1152/50000]\tLoss: 1.4645\tLR: 0.010000\n",
            "Training Epoch: 11 [1280/50000]\tLoss: 1.1433\tLR: 0.010000\n",
            "Training Epoch: 11 [1408/50000]\tLoss: 1.3808\tLR: 0.010000\n",
            "Training Epoch: 11 [1536/50000]\tLoss: 1.1651\tLR: 0.010000\n",
            "Training Epoch: 11 [1664/50000]\tLoss: 1.1531\tLR: 0.010000\n",
            "Training Epoch: 11 [1792/50000]\tLoss: 1.3324\tLR: 0.010000\n",
            "Training Epoch: 11 [1920/50000]\tLoss: 1.2874\tLR: 0.010000\n",
            "Training Epoch: 11 [2048/50000]\tLoss: 1.0046\tLR: 0.010000\n",
            "Training Epoch: 11 [2176/50000]\tLoss: 1.5131\tLR: 0.010000\n",
            "Training Epoch: 11 [2304/50000]\tLoss: 1.0456\tLR: 0.010000\n",
            "Training Epoch: 11 [2432/50000]\tLoss: 1.0527\tLR: 0.010000\n",
            "Training Epoch: 11 [2560/50000]\tLoss: 1.2124\tLR: 0.010000\n",
            "Training Epoch: 11 [2688/50000]\tLoss: 1.2105\tLR: 0.010000\n",
            "Training Epoch: 11 [2816/50000]\tLoss: 1.1394\tLR: 0.010000\n",
            "Training Epoch: 11 [2944/50000]\tLoss: 1.1194\tLR: 0.010000\n",
            "Training Epoch: 11 [3072/50000]\tLoss: 1.3108\tLR: 0.010000\n",
            "Training Epoch: 11 [3200/50000]\tLoss: 1.3345\tLR: 0.010000\n",
            "Training Epoch: 11 [3328/50000]\tLoss: 1.1973\tLR: 0.010000\n",
            "Training Epoch: 11 [3456/50000]\tLoss: 1.2199\tLR: 0.010000\n",
            "Training Epoch: 11 [3584/50000]\tLoss: 1.3266\tLR: 0.010000\n",
            "Training Epoch: 11 [3712/50000]\tLoss: 1.2119\tLR: 0.010000\n",
            "Training Epoch: 11 [3840/50000]\tLoss: 1.3162\tLR: 0.010000\n",
            "Training Epoch: 11 [3968/50000]\tLoss: 1.3073\tLR: 0.010000\n",
            "Training Epoch: 11 [4096/50000]\tLoss: 1.1862\tLR: 0.010000\n",
            "Training Epoch: 11 [4224/50000]\tLoss: 1.4491\tLR: 0.010000\n",
            "Training Epoch: 11 [4352/50000]\tLoss: 1.1184\tLR: 0.010000\n",
            "Training Epoch: 11 [4480/50000]\tLoss: 1.0942\tLR: 0.010000\n",
            "Training Epoch: 11 [4608/50000]\tLoss: 1.1756\tLR: 0.010000\n",
            "Training Epoch: 11 [4736/50000]\tLoss: 1.3812\tLR: 0.010000\n",
            "Training Epoch: 11 [4864/50000]\tLoss: 1.4001\tLR: 0.010000\n",
            "Training Epoch: 11 [4992/50000]\tLoss: 1.3457\tLR: 0.010000\n",
            "Training Epoch: 11 [5120/50000]\tLoss: 1.1476\tLR: 0.010000\n",
            "Training Epoch: 11 [5248/50000]\tLoss: 1.3906\tLR: 0.010000\n",
            "Training Epoch: 11 [5376/50000]\tLoss: 1.1702\tLR: 0.010000\n",
            "Training Epoch: 11 [5504/50000]\tLoss: 1.3951\tLR: 0.010000\n",
            "Training Epoch: 11 [5632/50000]\tLoss: 1.1995\tLR: 0.010000\n",
            "Training Epoch: 11 [5760/50000]\tLoss: 1.0365\tLR: 0.010000\n",
            "Training Epoch: 11 [5888/50000]\tLoss: 1.2033\tLR: 0.010000\n",
            "Training Epoch: 11 [6016/50000]\tLoss: 1.2113\tLR: 0.010000\n",
            "Training Epoch: 11 [6144/50000]\tLoss: 1.3618\tLR: 0.010000\n",
            "Training Epoch: 11 [6272/50000]\tLoss: 1.1719\tLR: 0.010000\n",
            "Training Epoch: 11 [6400/50000]\tLoss: 1.1256\tLR: 0.010000\n",
            "Training Epoch: 11 [6528/50000]\tLoss: 1.4124\tLR: 0.010000\n",
            "Training Epoch: 11 [6656/50000]\tLoss: 1.2966\tLR: 0.010000\n",
            "Training Epoch: 11 [6784/50000]\tLoss: 1.2208\tLR: 0.010000\n",
            "Training Epoch: 11 [6912/50000]\tLoss: 1.2524\tLR: 0.010000\n",
            "Training Epoch: 11 [7040/50000]\tLoss: 1.1695\tLR: 0.010000\n",
            "Training Epoch: 11 [7168/50000]\tLoss: 1.4900\tLR: 0.010000\n",
            "Training Epoch: 11 [7296/50000]\tLoss: 1.2797\tLR: 0.010000\n",
            "Training Epoch: 11 [7424/50000]\tLoss: 1.3467\tLR: 0.010000\n",
            "Training Epoch: 11 [7552/50000]\tLoss: 1.3783\tLR: 0.010000\n",
            "Training Epoch: 11 [7680/50000]\tLoss: 1.0727\tLR: 0.010000\n",
            "Training Epoch: 11 [7808/50000]\tLoss: 1.1021\tLR: 0.010000\n",
            "Training Epoch: 11 [7936/50000]\tLoss: 1.4455\tLR: 0.010000\n",
            "Training Epoch: 11 [8064/50000]\tLoss: 1.3190\tLR: 0.010000\n",
            "Training Epoch: 11 [8192/50000]\tLoss: 1.4369\tLR: 0.010000\n",
            "Training Epoch: 11 [8320/50000]\tLoss: 1.2507\tLR: 0.010000\n",
            "Training Epoch: 11 [8448/50000]\tLoss: 1.2321\tLR: 0.010000\n",
            "Training Epoch: 11 [8576/50000]\tLoss: 1.1625\tLR: 0.010000\n",
            "Training Epoch: 11 [8704/50000]\tLoss: 1.1070\tLR: 0.010000\n",
            "Training Epoch: 11 [8832/50000]\tLoss: 1.4426\tLR: 0.010000\n",
            "Training Epoch: 11 [8960/50000]\tLoss: 1.1453\tLR: 0.010000\n",
            "Training Epoch: 11 [9088/50000]\tLoss: 1.0537\tLR: 0.010000\n",
            "Training Epoch: 11 [9216/50000]\tLoss: 1.2977\tLR: 0.010000\n",
            "Training Epoch: 11 [9344/50000]\tLoss: 1.1764\tLR: 0.010000\n",
            "Training Epoch: 11 [9472/50000]\tLoss: 1.4362\tLR: 0.010000\n",
            "Training Epoch: 11 [9600/50000]\tLoss: 1.4283\tLR: 0.010000\n",
            "Training Epoch: 11 [9728/50000]\tLoss: 1.1598\tLR: 0.010000\n",
            "Training Epoch: 11 [9856/50000]\tLoss: 1.2214\tLR: 0.010000\n",
            "Training Epoch: 11 [9984/50000]\tLoss: 1.2921\tLR: 0.010000\n",
            "Training Epoch: 11 [10112/50000]\tLoss: 1.2580\tLR: 0.010000\n",
            "Training Epoch: 11 [10240/50000]\tLoss: 1.3744\tLR: 0.010000\n",
            "Training Epoch: 11 [10368/50000]\tLoss: 1.6154\tLR: 0.010000\n",
            "Training Epoch: 11 [10496/50000]\tLoss: 1.1417\tLR: 0.010000\n",
            "Training Epoch: 11 [10624/50000]\tLoss: 1.1965\tLR: 0.010000\n",
            "Training Epoch: 11 [10752/50000]\tLoss: 1.1412\tLR: 0.010000\n",
            "Training Epoch: 11 [10880/50000]\tLoss: 1.2840\tLR: 0.010000\n",
            "Training Epoch: 11 [11008/50000]\tLoss: 1.3555\tLR: 0.010000\n",
            "Training Epoch: 11 [11136/50000]\tLoss: 1.2325\tLR: 0.010000\n",
            "Training Epoch: 11 [11264/50000]\tLoss: 1.2469\tLR: 0.010000\n",
            "Training Epoch: 11 [11392/50000]\tLoss: 1.2122\tLR: 0.010000\n",
            "Training Epoch: 11 [11520/50000]\tLoss: 1.2675\tLR: 0.010000\n",
            "Training Epoch: 11 [11648/50000]\tLoss: 1.3536\tLR: 0.010000\n",
            "Training Epoch: 11 [11776/50000]\tLoss: 1.2227\tLR: 0.010000\n",
            "Training Epoch: 11 [11904/50000]\tLoss: 1.3368\tLR: 0.010000\n",
            "Training Epoch: 11 [12032/50000]\tLoss: 1.4362\tLR: 0.010000\n",
            "Training Epoch: 11 [12160/50000]\tLoss: 1.3957\tLR: 0.010000\n",
            "Training Epoch: 11 [12288/50000]\tLoss: 1.4099\tLR: 0.010000\n",
            "Training Epoch: 11 [12416/50000]\tLoss: 1.3369\tLR: 0.010000\n",
            "Training Epoch: 11 [12544/50000]\tLoss: 1.2798\tLR: 0.010000\n",
            "Training Epoch: 11 [12672/50000]\tLoss: 1.2175\tLR: 0.010000\n",
            "Training Epoch: 11 [12800/50000]\tLoss: 1.3019\tLR: 0.010000\n",
            "Training Epoch: 11 [12928/50000]\tLoss: 1.0265\tLR: 0.010000\n",
            "Training Epoch: 11 [13056/50000]\tLoss: 1.1402\tLR: 0.010000\n",
            "Training Epoch: 11 [13184/50000]\tLoss: 1.3075\tLR: 0.010000\n",
            "Training Epoch: 11 [13312/50000]\tLoss: 1.3314\tLR: 0.010000\n",
            "Training Epoch: 11 [13440/50000]\tLoss: 1.2844\tLR: 0.010000\n",
            "Training Epoch: 11 [13568/50000]\tLoss: 1.1672\tLR: 0.010000\n",
            "Training Epoch: 11 [13696/50000]\tLoss: 1.1699\tLR: 0.010000\n",
            "Training Epoch: 11 [13824/50000]\tLoss: 1.2369\tLR: 0.010000\n",
            "Training Epoch: 11 [13952/50000]\tLoss: 1.0978\tLR: 0.010000\n",
            "Training Epoch: 11 [14080/50000]\tLoss: 1.5153\tLR: 0.010000\n",
            "Training Epoch: 11 [14208/50000]\tLoss: 0.9874\tLR: 0.010000\n",
            "Training Epoch: 11 [14336/50000]\tLoss: 1.2866\tLR: 0.010000\n",
            "Training Epoch: 11 [14464/50000]\tLoss: 1.3164\tLR: 0.010000\n",
            "Training Epoch: 11 [14592/50000]\tLoss: 1.3871\tLR: 0.010000\n",
            "Training Epoch: 11 [14720/50000]\tLoss: 1.0527\tLR: 0.010000\n",
            "Training Epoch: 11 [14848/50000]\tLoss: 1.3923\tLR: 0.010000\n",
            "Training Epoch: 11 [14976/50000]\tLoss: 0.9735\tLR: 0.010000\n",
            "Training Epoch: 11 [15104/50000]\tLoss: 1.2537\tLR: 0.010000\n",
            "Training Epoch: 11 [15232/50000]\tLoss: 1.4969\tLR: 0.010000\n",
            "Training Epoch: 11 [15360/50000]\tLoss: 1.5039\tLR: 0.010000\n",
            "Training Epoch: 11 [15488/50000]\tLoss: 1.0559\tLR: 0.010000\n",
            "Training Epoch: 11 [15616/50000]\tLoss: 1.1690\tLR: 0.010000\n",
            "Training Epoch: 11 [15744/50000]\tLoss: 1.2347\tLR: 0.010000\n",
            "Training Epoch: 11 [15872/50000]\tLoss: 1.1957\tLR: 0.010000\n",
            "Training Epoch: 11 [16000/50000]\tLoss: 1.1475\tLR: 0.010000\n",
            "Training Epoch: 11 [16128/50000]\tLoss: 1.3654\tLR: 0.010000\n",
            "Training Epoch: 11 [16256/50000]\tLoss: 0.9217\tLR: 0.010000\n",
            "Training Epoch: 11 [16384/50000]\tLoss: 1.2577\tLR: 0.010000\n",
            "Training Epoch: 11 [16512/50000]\tLoss: 0.9073\tLR: 0.010000\n",
            "Training Epoch: 11 [16640/50000]\tLoss: 1.3689\tLR: 0.010000\n",
            "Training Epoch: 11 [16768/50000]\tLoss: 1.1785\tLR: 0.010000\n",
            "Training Epoch: 11 [16896/50000]\tLoss: 1.3661\tLR: 0.010000\n",
            "Training Epoch: 11 [17024/50000]\tLoss: 1.4953\tLR: 0.010000\n",
            "Training Epoch: 11 [17152/50000]\tLoss: 0.9971\tLR: 0.010000\n",
            "Training Epoch: 11 [17280/50000]\tLoss: 1.0455\tLR: 0.010000\n",
            "Training Epoch: 11 [17408/50000]\tLoss: 1.0399\tLR: 0.010000\n",
            "Training Epoch: 11 [17536/50000]\tLoss: 1.0768\tLR: 0.010000\n",
            "Training Epoch: 11 [17664/50000]\tLoss: 1.2553\tLR: 0.010000\n",
            "Training Epoch: 11 [17792/50000]\tLoss: 1.2214\tLR: 0.010000\n",
            "Training Epoch: 11 [17920/50000]\tLoss: 1.1103\tLR: 0.010000\n",
            "Training Epoch: 11 [18048/50000]\tLoss: 1.0211\tLR: 0.010000\n",
            "Training Epoch: 11 [18176/50000]\tLoss: 1.3277\tLR: 0.010000\n",
            "Training Epoch: 11 [18304/50000]\tLoss: 1.3212\tLR: 0.010000\n",
            "Training Epoch: 11 [18432/50000]\tLoss: 1.1959\tLR: 0.010000\n",
            "Training Epoch: 11 [18560/50000]\tLoss: 1.1875\tLR: 0.010000\n",
            "Training Epoch: 11 [18688/50000]\tLoss: 1.0107\tLR: 0.010000\n",
            "Training Epoch: 11 [18816/50000]\tLoss: 1.2177\tLR: 0.010000\n",
            "Training Epoch: 11 [18944/50000]\tLoss: 1.0221\tLR: 0.010000\n",
            "Training Epoch: 11 [19072/50000]\tLoss: 1.1536\tLR: 0.010000\n",
            "Training Epoch: 11 [19200/50000]\tLoss: 1.1979\tLR: 0.010000\n",
            "Training Epoch: 11 [19328/50000]\tLoss: 1.4144\tLR: 0.010000\n",
            "Training Epoch: 11 [19456/50000]\tLoss: 1.0774\tLR: 0.010000\n",
            "Training Epoch: 11 [19584/50000]\tLoss: 1.4821\tLR: 0.010000\n",
            "Training Epoch: 11 [19712/50000]\tLoss: 1.3567\tLR: 0.010000\n",
            "Training Epoch: 11 [19840/50000]\tLoss: 1.4541\tLR: 0.010000\n",
            "Training Epoch: 11 [19968/50000]\tLoss: 1.2644\tLR: 0.010000\n",
            "Training Epoch: 11 [20096/50000]\tLoss: 1.2958\tLR: 0.010000\n",
            "Training Epoch: 11 [20224/50000]\tLoss: 1.5217\tLR: 0.010000\n",
            "Training Epoch: 11 [20352/50000]\tLoss: 1.2928\tLR: 0.010000\n",
            "Training Epoch: 11 [20480/50000]\tLoss: 0.9098\tLR: 0.010000\n",
            "Training Epoch: 11 [20608/50000]\tLoss: 1.3678\tLR: 0.010000\n",
            "Training Epoch: 11 [20736/50000]\tLoss: 1.1950\tLR: 0.010000\n",
            "Training Epoch: 11 [20864/50000]\tLoss: 1.2233\tLR: 0.010000\n",
            "Training Epoch: 11 [20992/50000]\tLoss: 1.4215\tLR: 0.010000\n",
            "Training Epoch: 11 [21120/50000]\tLoss: 1.3943\tLR: 0.010000\n",
            "Training Epoch: 11 [21248/50000]\tLoss: 1.3807\tLR: 0.010000\n",
            "Training Epoch: 11 [21376/50000]\tLoss: 1.3347\tLR: 0.010000\n",
            "Training Epoch: 11 [21504/50000]\tLoss: 1.2856\tLR: 0.010000\n",
            "Training Epoch: 11 [21632/50000]\tLoss: 1.4108\tLR: 0.010000\n",
            "Training Epoch: 11 [21760/50000]\tLoss: 1.3397\tLR: 0.010000\n",
            "Training Epoch: 11 [21888/50000]\tLoss: 1.3217\tLR: 0.010000\n",
            "Training Epoch: 11 [22016/50000]\tLoss: 1.4898\tLR: 0.010000\n",
            "Training Epoch: 11 [22144/50000]\tLoss: 1.2951\tLR: 0.010000\n",
            "Training Epoch: 11 [22272/50000]\tLoss: 1.1067\tLR: 0.010000\n",
            "Training Epoch: 11 [22400/50000]\tLoss: 1.3325\tLR: 0.010000\n",
            "Training Epoch: 11 [22528/50000]\tLoss: 1.2754\tLR: 0.010000\n",
            "Training Epoch: 11 [22656/50000]\tLoss: 1.3239\tLR: 0.010000\n",
            "Training Epoch: 11 [22784/50000]\tLoss: 1.3845\tLR: 0.010000\n",
            "Training Epoch: 11 [22912/50000]\tLoss: 1.5738\tLR: 0.010000\n",
            "Training Epoch: 11 [23040/50000]\tLoss: 1.2556\tLR: 0.010000\n",
            "Training Epoch: 11 [23168/50000]\tLoss: 1.7317\tLR: 0.010000\n",
            "Training Epoch: 11 [23296/50000]\tLoss: 1.2594\tLR: 0.010000\n",
            "Training Epoch: 11 [23424/50000]\tLoss: 1.2687\tLR: 0.010000\n",
            "Training Epoch: 11 [23552/50000]\tLoss: 1.2807\tLR: 0.010000\n",
            "Training Epoch: 11 [23680/50000]\tLoss: 1.3254\tLR: 0.010000\n",
            "Training Epoch: 11 [23808/50000]\tLoss: 1.2283\tLR: 0.010000\n",
            "Training Epoch: 11 [23936/50000]\tLoss: 1.4615\tLR: 0.010000\n",
            "Training Epoch: 11 [24064/50000]\tLoss: 1.4416\tLR: 0.010000\n",
            "Training Epoch: 11 [24192/50000]\tLoss: 1.5217\tLR: 0.010000\n",
            "Training Epoch: 11 [24320/50000]\tLoss: 1.2677\tLR: 0.010000\n",
            "Training Epoch: 11 [24448/50000]\tLoss: 1.2524\tLR: 0.010000\n",
            "Training Epoch: 11 [24576/50000]\tLoss: 1.3718\tLR: 0.010000\n",
            "Training Epoch: 11 [24704/50000]\tLoss: 1.3438\tLR: 0.010000\n",
            "Training Epoch: 11 [24832/50000]\tLoss: 1.1916\tLR: 0.010000\n",
            "Training Epoch: 11 [24960/50000]\tLoss: 1.1607\tLR: 0.010000\n",
            "Training Epoch: 11 [25088/50000]\tLoss: 1.3846\tLR: 0.010000\n",
            "Training Epoch: 11 [25216/50000]\tLoss: 1.3444\tLR: 0.010000\n",
            "Training Epoch: 11 [25344/50000]\tLoss: 1.4809\tLR: 0.010000\n",
            "Training Epoch: 11 [25472/50000]\tLoss: 1.2602\tLR: 0.010000\n",
            "Training Epoch: 11 [25600/50000]\tLoss: 1.2522\tLR: 0.010000\n",
            "Training Epoch: 11 [25728/50000]\tLoss: 1.2245\tLR: 0.010000\n",
            "Training Epoch: 11 [25856/50000]\tLoss: 1.1259\tLR: 0.010000\n",
            "Training Epoch: 11 [25984/50000]\tLoss: 1.4033\tLR: 0.010000\n",
            "Training Epoch: 11 [26112/50000]\tLoss: 1.0620\tLR: 0.010000\n",
            "Training Epoch: 11 [26240/50000]\tLoss: 1.1309\tLR: 0.010000\n",
            "Training Epoch: 11 [26368/50000]\tLoss: 1.3290\tLR: 0.010000\n",
            "Training Epoch: 11 [26496/50000]\tLoss: 1.2969\tLR: 0.010000\n",
            "Training Epoch: 11 [26624/50000]\tLoss: 1.2443\tLR: 0.010000\n",
            "Training Epoch: 11 [26752/50000]\tLoss: 1.2572\tLR: 0.010000\n",
            "Training Epoch: 11 [26880/50000]\tLoss: 1.6337\tLR: 0.010000\n",
            "Training Epoch: 11 [27008/50000]\tLoss: 1.4885\tLR: 0.010000\n",
            "Training Epoch: 11 [27136/50000]\tLoss: 1.2640\tLR: 0.010000\n",
            "Training Epoch: 11 [27264/50000]\tLoss: 1.2813\tLR: 0.010000\n",
            "Training Epoch: 11 [27392/50000]\tLoss: 1.2108\tLR: 0.010000\n",
            "Training Epoch: 11 [27520/50000]\tLoss: 1.2610\tLR: 0.010000\n",
            "Training Epoch: 11 [27648/50000]\tLoss: 1.3638\tLR: 0.010000\n",
            "Training Epoch: 11 [27776/50000]\tLoss: 1.3580\tLR: 0.010000\n",
            "Training Epoch: 11 [27904/50000]\tLoss: 1.1577\tLR: 0.010000\n",
            "Training Epoch: 11 [28032/50000]\tLoss: 1.1888\tLR: 0.010000\n",
            "Training Epoch: 11 [28160/50000]\tLoss: 1.5050\tLR: 0.010000\n",
            "Training Epoch: 11 [28288/50000]\tLoss: 1.1484\tLR: 0.010000\n",
            "Training Epoch: 11 [28416/50000]\tLoss: 1.2607\tLR: 0.010000\n",
            "Training Epoch: 11 [28544/50000]\tLoss: 1.3444\tLR: 0.010000\n",
            "Training Epoch: 11 [28672/50000]\tLoss: 0.9893\tLR: 0.010000\n",
            "Training Epoch: 11 [28800/50000]\tLoss: 1.1445\tLR: 0.010000\n",
            "Training Epoch: 11 [28928/50000]\tLoss: 1.2274\tLR: 0.010000\n",
            "Training Epoch: 11 [29056/50000]\tLoss: 1.5345\tLR: 0.010000\n",
            "Training Epoch: 11 [29184/50000]\tLoss: 1.2985\tLR: 0.010000\n",
            "Training Epoch: 11 [29312/50000]\tLoss: 1.4775\tLR: 0.010000\n",
            "Training Epoch: 11 [29440/50000]\tLoss: 1.0706\tLR: 0.010000\n",
            "Training Epoch: 11 [29568/50000]\tLoss: 1.1451\tLR: 0.010000\n",
            "Training Epoch: 11 [29696/50000]\tLoss: 1.2764\tLR: 0.010000\n",
            "Training Epoch: 11 [29824/50000]\tLoss: 1.2150\tLR: 0.010000\n",
            "Training Epoch: 11 [29952/50000]\tLoss: 1.3220\tLR: 0.010000\n",
            "Training Epoch: 11 [30080/50000]\tLoss: 1.3837\tLR: 0.010000\n",
            "Training Epoch: 11 [30208/50000]\tLoss: 1.0661\tLR: 0.010000\n",
            "Training Epoch: 11 [30336/50000]\tLoss: 1.2890\tLR: 0.010000\n",
            "Training Epoch: 11 [30464/50000]\tLoss: 1.3489\tLR: 0.010000\n",
            "Training Epoch: 11 [30592/50000]\tLoss: 1.0834\tLR: 0.010000\n",
            "Training Epoch: 11 [30720/50000]\tLoss: 1.3707\tLR: 0.010000\n",
            "Training Epoch: 11 [30848/50000]\tLoss: 1.2874\tLR: 0.010000\n",
            "Training Epoch: 11 [30976/50000]\tLoss: 1.3116\tLR: 0.010000\n",
            "Training Epoch: 11 [31104/50000]\tLoss: 1.1982\tLR: 0.010000\n",
            "Training Epoch: 11 [31232/50000]\tLoss: 1.2179\tLR: 0.010000\n",
            "Training Epoch: 11 [31360/50000]\tLoss: 1.3464\tLR: 0.010000\n",
            "Training Epoch: 11 [31488/50000]\tLoss: 1.1583\tLR: 0.010000\n",
            "Training Epoch: 11 [31616/50000]\tLoss: 1.4045\tLR: 0.010000\n",
            "Training Epoch: 11 [31744/50000]\tLoss: 1.4675\tLR: 0.010000\n",
            "Training Epoch: 11 [31872/50000]\tLoss: 1.2034\tLR: 0.010000\n",
            "Training Epoch: 11 [32000/50000]\tLoss: 1.1960\tLR: 0.010000\n",
            "Training Epoch: 11 [32128/50000]\tLoss: 1.1215\tLR: 0.010000\n",
            "Training Epoch: 11 [32256/50000]\tLoss: 1.1956\tLR: 0.010000\n",
            "Training Epoch: 11 [32384/50000]\tLoss: 1.1516\tLR: 0.010000\n",
            "Training Epoch: 11 [32512/50000]\tLoss: 1.3034\tLR: 0.010000\n",
            "Training Epoch: 11 [32640/50000]\tLoss: 1.1943\tLR: 0.010000\n",
            "Training Epoch: 11 [32768/50000]\tLoss: 1.4955\tLR: 0.010000\n",
            "Training Epoch: 11 [32896/50000]\tLoss: 1.2520\tLR: 0.010000\n",
            "Training Epoch: 11 [33024/50000]\tLoss: 1.1376\tLR: 0.010000\n",
            "Training Epoch: 11 [33152/50000]\tLoss: 1.3015\tLR: 0.010000\n",
            "Training Epoch: 11 [33280/50000]\tLoss: 1.3193\tLR: 0.010000\n",
            "Training Epoch: 11 [33408/50000]\tLoss: 1.5383\tLR: 0.010000\n",
            "Training Epoch: 11 [33536/50000]\tLoss: 1.1384\tLR: 0.010000\n",
            "Training Epoch: 11 [33664/50000]\tLoss: 1.2353\tLR: 0.010000\n",
            "Training Epoch: 11 [33792/50000]\tLoss: 1.5220\tLR: 0.010000\n",
            "Training Epoch: 11 [33920/50000]\tLoss: 1.2958\tLR: 0.010000\n",
            "Training Epoch: 11 [34048/50000]\tLoss: 1.2705\tLR: 0.010000\n",
            "Training Epoch: 11 [34176/50000]\tLoss: 1.2122\tLR: 0.010000\n",
            "Training Epoch: 11 [34304/50000]\tLoss: 1.2700\tLR: 0.010000\n",
            "Training Epoch: 11 [34432/50000]\tLoss: 1.3776\tLR: 0.010000\n",
            "Training Epoch: 11 [34560/50000]\tLoss: 1.3467\tLR: 0.010000\n",
            "Training Epoch: 11 [34688/50000]\tLoss: 1.0771\tLR: 0.010000\n",
            "Training Epoch: 11 [34816/50000]\tLoss: 1.2207\tLR: 0.010000\n",
            "Training Epoch: 11 [34944/50000]\tLoss: 1.4173\tLR: 0.010000\n",
            "Training Epoch: 11 [35072/50000]\tLoss: 1.1439\tLR: 0.010000\n",
            "Training Epoch: 11 [35200/50000]\tLoss: 1.2124\tLR: 0.010000\n",
            "Training Epoch: 11 [35328/50000]\tLoss: 1.3472\tLR: 0.010000\n",
            "Training Epoch: 11 [35456/50000]\tLoss: 1.4918\tLR: 0.010000\n",
            "Training Epoch: 11 [35584/50000]\tLoss: 1.2876\tLR: 0.010000\n",
            "Training Epoch: 11 [35712/50000]\tLoss: 1.3040\tLR: 0.010000\n",
            "Training Epoch: 11 [35840/50000]\tLoss: 1.2306\tLR: 0.010000\n",
            "Training Epoch: 11 [35968/50000]\tLoss: 1.2349\tLR: 0.010000\n",
            "Training Epoch: 11 [36096/50000]\tLoss: 1.2617\tLR: 0.010000\n",
            "Training Epoch: 11 [36224/50000]\tLoss: 1.1205\tLR: 0.010000\n",
            "Training Epoch: 11 [36352/50000]\tLoss: 1.2296\tLR: 0.010000\n",
            "Training Epoch: 11 [36480/50000]\tLoss: 1.3596\tLR: 0.010000\n",
            "Training Epoch: 11 [36608/50000]\tLoss: 1.3055\tLR: 0.010000\n",
            "Training Epoch: 11 [36736/50000]\tLoss: 1.3537\tLR: 0.010000\n",
            "Training Epoch: 11 [36864/50000]\tLoss: 1.1706\tLR: 0.010000\n",
            "Training Epoch: 11 [36992/50000]\tLoss: 1.3953\tLR: 0.010000\n",
            "Training Epoch: 11 [37120/50000]\tLoss: 1.1930\tLR: 0.010000\n",
            "Training Epoch: 11 [37248/50000]\tLoss: 1.3942\tLR: 0.010000\n",
            "Training Epoch: 11 [37376/50000]\tLoss: 1.2605\tLR: 0.010000\n",
            "Training Epoch: 11 [37504/50000]\tLoss: 1.3153\tLR: 0.010000\n",
            "Training Epoch: 11 [37632/50000]\tLoss: 1.2362\tLR: 0.010000\n",
            "Training Epoch: 11 [37760/50000]\tLoss: 1.5722\tLR: 0.010000\n",
            "Training Epoch: 11 [37888/50000]\tLoss: 1.1308\tLR: 0.010000\n",
            "Training Epoch: 11 [38016/50000]\tLoss: 1.3510\tLR: 0.010000\n",
            "Training Epoch: 11 [38144/50000]\tLoss: 1.3171\tLR: 0.010000\n",
            "Training Epoch: 11 [38272/50000]\tLoss: 1.2450\tLR: 0.010000\n",
            "Training Epoch: 11 [38400/50000]\tLoss: 1.2869\tLR: 0.010000\n",
            "Training Epoch: 11 [38528/50000]\tLoss: 1.4107\tLR: 0.010000\n",
            "Training Epoch: 11 [38656/50000]\tLoss: 1.4562\tLR: 0.010000\n",
            "Training Epoch: 11 [38784/50000]\tLoss: 1.2053\tLR: 0.010000\n",
            "Training Epoch: 11 [38912/50000]\tLoss: 1.4825\tLR: 0.010000\n",
            "Training Epoch: 11 [39040/50000]\tLoss: 1.1951\tLR: 0.010000\n",
            "Training Epoch: 11 [39168/50000]\tLoss: 1.3186\tLR: 0.010000\n",
            "Training Epoch: 11 [39296/50000]\tLoss: 1.1060\tLR: 0.010000\n",
            "Training Epoch: 11 [39424/50000]\tLoss: 1.4432\tLR: 0.010000\n",
            "Training Epoch: 11 [39552/50000]\tLoss: 1.5169\tLR: 0.010000\n",
            "Training Epoch: 11 [39680/50000]\tLoss: 1.3467\tLR: 0.010000\n",
            "Training Epoch: 11 [39808/50000]\tLoss: 1.1211\tLR: 0.010000\n",
            "Training Epoch: 11 [39936/50000]\tLoss: 1.3953\tLR: 0.010000\n",
            "Training Epoch: 11 [40064/50000]\tLoss: 1.4387\tLR: 0.010000\n",
            "Training Epoch: 11 [40192/50000]\tLoss: 1.2114\tLR: 0.010000\n",
            "Training Epoch: 11 [40320/50000]\tLoss: 1.2267\tLR: 0.010000\n",
            "Training Epoch: 11 [40448/50000]\tLoss: 1.3859\tLR: 0.010000\n",
            "Training Epoch: 11 [40576/50000]\tLoss: 1.3985\tLR: 0.010000\n",
            "Training Epoch: 11 [40704/50000]\tLoss: 1.2645\tLR: 0.010000\n",
            "Training Epoch: 11 [40832/50000]\tLoss: 1.4751\tLR: 0.010000\n",
            "Training Epoch: 11 [40960/50000]\tLoss: 1.3414\tLR: 0.010000\n",
            "Training Epoch: 11 [41088/50000]\tLoss: 1.0781\tLR: 0.010000\n",
            "Training Epoch: 11 [41216/50000]\tLoss: 1.5183\tLR: 0.010000\n",
            "Training Epoch: 11 [41344/50000]\tLoss: 1.0795\tLR: 0.010000\n",
            "Training Epoch: 11 [41472/50000]\tLoss: 1.4878\tLR: 0.010000\n",
            "Training Epoch: 11 [41600/50000]\tLoss: 1.1596\tLR: 0.010000\n",
            "Training Epoch: 11 [41728/50000]\tLoss: 1.2374\tLR: 0.010000\n",
            "Training Epoch: 11 [41856/50000]\tLoss: 1.0668\tLR: 0.010000\n",
            "Training Epoch: 11 [41984/50000]\tLoss: 1.1430\tLR: 0.010000\n",
            "Training Epoch: 11 [42112/50000]\tLoss: 1.2324\tLR: 0.010000\n",
            "Training Epoch: 11 [42240/50000]\tLoss: 1.3450\tLR: 0.010000\n",
            "Training Epoch: 11 [42368/50000]\tLoss: 1.2173\tLR: 0.010000\n",
            "Training Epoch: 11 [42496/50000]\tLoss: 1.1629\tLR: 0.010000\n",
            "Training Epoch: 11 [42624/50000]\tLoss: 1.4274\tLR: 0.010000\n",
            "Training Epoch: 11 [42752/50000]\tLoss: 1.2562\tLR: 0.010000\n",
            "Training Epoch: 11 [42880/50000]\tLoss: 1.3960\tLR: 0.010000\n",
            "Training Epoch: 11 [43008/50000]\tLoss: 1.2173\tLR: 0.010000\n",
            "Training Epoch: 11 [43136/50000]\tLoss: 1.3098\tLR: 0.010000\n",
            "Training Epoch: 11 [43264/50000]\tLoss: 1.3913\tLR: 0.010000\n",
            "Training Epoch: 11 [43392/50000]\tLoss: 1.4353\tLR: 0.010000\n",
            "Training Epoch: 11 [43520/50000]\tLoss: 1.4069\tLR: 0.010000\n",
            "Training Epoch: 11 [43648/50000]\tLoss: 1.3337\tLR: 0.010000\n",
            "Training Epoch: 11 [43776/50000]\tLoss: 1.3331\tLR: 0.010000\n",
            "Training Epoch: 11 [43904/50000]\tLoss: 1.3605\tLR: 0.010000\n",
            "Training Epoch: 11 [44032/50000]\tLoss: 1.3505\tLR: 0.010000\n",
            "Training Epoch: 11 [44160/50000]\tLoss: 1.2369\tLR: 0.010000\n",
            "Training Epoch: 11 [44288/50000]\tLoss: 1.4255\tLR: 0.010000\n",
            "Training Epoch: 11 [44416/50000]\tLoss: 1.3064\tLR: 0.010000\n",
            "Training Epoch: 11 [44544/50000]\tLoss: 1.3668\tLR: 0.010000\n",
            "Training Epoch: 11 [44672/50000]\tLoss: 1.2098\tLR: 0.010000\n",
            "Training Epoch: 11 [44800/50000]\tLoss: 1.3229\tLR: 0.010000\n",
            "Training Epoch: 11 [44928/50000]\tLoss: 1.3008\tLR: 0.010000\n",
            "Training Epoch: 11 [45056/50000]\tLoss: 1.3471\tLR: 0.010000\n",
            "Training Epoch: 11 [45184/50000]\tLoss: 1.2704\tLR: 0.010000\n",
            "Training Epoch: 11 [45312/50000]\tLoss: 1.3832\tLR: 0.010000\n",
            "Training Epoch: 11 [45440/50000]\tLoss: 1.3184\tLR: 0.010000\n",
            "Training Epoch: 11 [45568/50000]\tLoss: 1.3162\tLR: 0.010000\n",
            "Training Epoch: 11 [45696/50000]\tLoss: 1.3916\tLR: 0.010000\n",
            "Training Epoch: 11 [45824/50000]\tLoss: 1.1656\tLR: 0.010000\n",
            "Training Epoch: 11 [45952/50000]\tLoss: 1.1900\tLR: 0.010000\n",
            "Training Epoch: 11 [46080/50000]\tLoss: 1.3693\tLR: 0.010000\n",
            "Training Epoch: 11 [46208/50000]\tLoss: 1.2216\tLR: 0.010000\n",
            "Training Epoch: 11 [46336/50000]\tLoss: 1.3597\tLR: 0.010000\n",
            "Training Epoch: 11 [46464/50000]\tLoss: 1.1035\tLR: 0.010000\n",
            "Training Epoch: 11 [46592/50000]\tLoss: 1.3073\tLR: 0.010000\n",
            "Training Epoch: 11 [46720/50000]\tLoss: 1.2264\tLR: 0.010000\n",
            "Training Epoch: 11 [46848/50000]\tLoss: 1.3431\tLR: 0.010000\n",
            "Training Epoch: 11 [46976/50000]\tLoss: 1.3952\tLR: 0.010000\n",
            "Training Epoch: 11 [47104/50000]\tLoss: 1.0828\tLR: 0.010000\n",
            "Training Epoch: 11 [47232/50000]\tLoss: 1.4019\tLR: 0.010000\n",
            "Training Epoch: 11 [47360/50000]\tLoss: 1.1974\tLR: 0.010000\n",
            "Training Epoch: 11 [47488/50000]\tLoss: 1.3886\tLR: 0.010000\n",
            "Training Epoch: 11 [47616/50000]\tLoss: 1.3846\tLR: 0.010000\n",
            "Training Epoch: 11 [47744/50000]\tLoss: 1.0493\tLR: 0.010000\n",
            "Training Epoch: 11 [47872/50000]\tLoss: 1.3620\tLR: 0.010000\n",
            "Training Epoch: 11 [48000/50000]\tLoss: 1.5297\tLR: 0.010000\n",
            "Training Epoch: 11 [48128/50000]\tLoss: 1.0469\tLR: 0.010000\n",
            "Training Epoch: 11 [48256/50000]\tLoss: 1.1427\tLR: 0.010000\n",
            "Training Epoch: 11 [48384/50000]\tLoss: 1.3633\tLR: 0.010000\n",
            "Training Epoch: 11 [48512/50000]\tLoss: 1.3441\tLR: 0.010000\n",
            "Training Epoch: 11 [48640/50000]\tLoss: 1.4563\tLR: 0.010000\n",
            "Training Epoch: 11 [48768/50000]\tLoss: 1.2834\tLR: 0.010000\n",
            "Training Epoch: 11 [48896/50000]\tLoss: 1.4196\tLR: 0.010000\n",
            "Training Epoch: 11 [49024/50000]\tLoss: 1.2085\tLR: 0.010000\n",
            "Training Epoch: 11 [49152/50000]\tLoss: 1.2157\tLR: 0.010000\n",
            "Training Epoch: 11 [49280/50000]\tLoss: 1.4603\tLR: 0.010000\n",
            "Training Epoch: 11 [49408/50000]\tLoss: 1.3752\tLR: 0.010000\n",
            "Training Epoch: 11 [49536/50000]\tLoss: 1.1510\tLR: 0.010000\n",
            "Training Epoch: 11 [49664/50000]\tLoss: 1.3528\tLR: 0.010000\n",
            "Training Epoch: 11 [49792/50000]\tLoss: 1.1687\tLR: 0.010000\n",
            "Training Epoch: 11 [49920/50000]\tLoss: 1.2274\tLR: 0.010000\n",
            "Training Epoch: 11 [50000/50000]\tLoss: 1.4787\tLR: 0.010000\n",
            "epoch 11 training time consumed: 205.55s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB |  92407 GiB |  92407 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  92340 GiB |  92340 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB |  92407 GiB |  92407 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB |  92340 GiB |  92340 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB |  92397 GiB |  92397 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB |  92331 GiB |  92331 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     66 GiB |     66 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  67103 GiB |  67102 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  67034 GiB |  67033 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     68 GiB |     68 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    3669 K  |    3668 K  |\n",
            "|       from large pool |      92    |     184    |    2068 K  |    2068 K  |\n",
            "|       from small pool |     519    |     646    |    1600 K  |    1600 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    3669 K  |    3668 K  |\n",
            "|       from large pool |      92    |     184    |    2068 K  |    2068 K  |\n",
            "|       from small pool |     519    |     646    |    1600 K  |    1600 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |     122    |    1548 K  |    1548 K  |\n",
            "|       from large pool |      42    |     111    |    1221 K  |    1221 K  |\n",
            "|       from small pool |       9    |      21    |     327 K  |     327 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0120, Accuracy: 0.5811, Time consumed:11.22s\n",
            "\n",
            "Training Epoch: 12 [128/50000]\tLoss: 0.9884\tLR: 0.010000\n",
            "Training Epoch: 12 [256/50000]\tLoss: 1.1331\tLR: 0.010000\n",
            "Training Epoch: 12 [384/50000]\tLoss: 1.3948\tLR: 0.010000\n",
            "Training Epoch: 12 [512/50000]\tLoss: 1.1575\tLR: 0.010000\n",
            "Training Epoch: 12 [640/50000]\tLoss: 1.0495\tLR: 0.010000\n",
            "Training Epoch: 12 [768/50000]\tLoss: 1.0589\tLR: 0.010000\n",
            "Training Epoch: 12 [896/50000]\tLoss: 1.2435\tLR: 0.010000\n",
            "Training Epoch: 12 [1024/50000]\tLoss: 1.3937\tLR: 0.010000\n",
            "Training Epoch: 12 [1152/50000]\tLoss: 1.0388\tLR: 0.010000\n",
            "Training Epoch: 12 [1280/50000]\tLoss: 1.2303\tLR: 0.010000\n",
            "Training Epoch: 12 [1408/50000]\tLoss: 1.3391\tLR: 0.010000\n",
            "Training Epoch: 12 [1536/50000]\tLoss: 0.8802\tLR: 0.010000\n",
            "Training Epoch: 12 [1664/50000]\tLoss: 0.9852\tLR: 0.010000\n",
            "Training Epoch: 12 [1792/50000]\tLoss: 1.2111\tLR: 0.010000\n",
            "Training Epoch: 12 [1920/50000]\tLoss: 1.0542\tLR: 0.010000\n",
            "Training Epoch: 12 [2048/50000]\tLoss: 1.0807\tLR: 0.010000\n",
            "Training Epoch: 12 [2176/50000]\tLoss: 1.0964\tLR: 0.010000\n",
            "Training Epoch: 12 [2304/50000]\tLoss: 1.0631\tLR: 0.010000\n",
            "Training Epoch: 12 [2432/50000]\tLoss: 1.0998\tLR: 0.010000\n",
            "Training Epoch: 12 [2560/50000]\tLoss: 1.0581\tLR: 0.010000\n",
            "Training Epoch: 12 [2688/50000]\tLoss: 1.2187\tLR: 0.010000\n",
            "Training Epoch: 12 [2816/50000]\tLoss: 1.2296\tLR: 0.010000\n",
            "Training Epoch: 12 [2944/50000]\tLoss: 1.1236\tLR: 0.010000\n",
            "Training Epoch: 12 [3072/50000]\tLoss: 1.1208\tLR: 0.010000\n",
            "Training Epoch: 12 [3200/50000]\tLoss: 1.1557\tLR: 0.010000\n",
            "Training Epoch: 12 [3328/50000]\tLoss: 1.4181\tLR: 0.010000\n",
            "Training Epoch: 12 [3456/50000]\tLoss: 1.1237\tLR: 0.010000\n",
            "Training Epoch: 12 [3584/50000]\tLoss: 1.0274\tLR: 0.010000\n",
            "Training Epoch: 12 [3712/50000]\tLoss: 1.0773\tLR: 0.010000\n",
            "Training Epoch: 12 [3840/50000]\tLoss: 1.0602\tLR: 0.010000\n",
            "Training Epoch: 12 [3968/50000]\tLoss: 0.9878\tLR: 0.010000\n",
            "Training Epoch: 12 [4096/50000]\tLoss: 1.2280\tLR: 0.010000\n",
            "Training Epoch: 12 [4224/50000]\tLoss: 1.1527\tLR: 0.010000\n",
            "Training Epoch: 12 [4352/50000]\tLoss: 1.1453\tLR: 0.010000\n",
            "Training Epoch: 12 [4480/50000]\tLoss: 0.8869\tLR: 0.010000\n",
            "Training Epoch: 12 [4608/50000]\tLoss: 1.1150\tLR: 0.010000\n",
            "Training Epoch: 12 [4736/50000]\tLoss: 1.2317\tLR: 0.010000\n",
            "Training Epoch: 12 [4864/50000]\tLoss: 1.2708\tLR: 0.010000\n",
            "Training Epoch: 12 [4992/50000]\tLoss: 0.9590\tLR: 0.010000\n",
            "Training Epoch: 12 [5120/50000]\tLoss: 1.2380\tLR: 0.010000\n",
            "Training Epoch: 12 [5248/50000]\tLoss: 1.1143\tLR: 0.010000\n",
            "Training Epoch: 12 [5376/50000]\tLoss: 1.0546\tLR: 0.010000\n",
            "Training Epoch: 12 [5504/50000]\tLoss: 1.3465\tLR: 0.010000\n",
            "Training Epoch: 12 [5632/50000]\tLoss: 1.2488\tLR: 0.010000\n",
            "Training Epoch: 12 [5760/50000]\tLoss: 1.2379\tLR: 0.010000\n",
            "Training Epoch: 12 [5888/50000]\tLoss: 1.2247\tLR: 0.010000\n",
            "Training Epoch: 12 [6016/50000]\tLoss: 1.3148\tLR: 0.010000\n",
            "Training Epoch: 12 [6144/50000]\tLoss: 1.0898\tLR: 0.010000\n",
            "Training Epoch: 12 [6272/50000]\tLoss: 1.2437\tLR: 0.010000\n",
            "Training Epoch: 12 [6400/50000]\tLoss: 1.0621\tLR: 0.010000\n",
            "Training Epoch: 12 [6528/50000]\tLoss: 1.2602\tLR: 0.010000\n",
            "Training Epoch: 12 [6656/50000]\tLoss: 1.0982\tLR: 0.010000\n",
            "Training Epoch: 12 [6784/50000]\tLoss: 1.2076\tLR: 0.010000\n",
            "Training Epoch: 12 [6912/50000]\tLoss: 1.0281\tLR: 0.010000\n",
            "Training Epoch: 12 [7040/50000]\tLoss: 0.9616\tLR: 0.010000\n",
            "Training Epoch: 12 [7168/50000]\tLoss: 1.2642\tLR: 0.010000\n",
            "Training Epoch: 12 [7296/50000]\tLoss: 0.8754\tLR: 0.010000\n",
            "Training Epoch: 12 [7424/50000]\tLoss: 1.2505\tLR: 0.010000\n",
            "Training Epoch: 12 [7552/50000]\tLoss: 1.0842\tLR: 0.010000\n",
            "Training Epoch: 12 [7680/50000]\tLoss: 1.1804\tLR: 0.010000\n",
            "Training Epoch: 12 [7808/50000]\tLoss: 1.0766\tLR: 0.010000\n",
            "Training Epoch: 12 [7936/50000]\tLoss: 1.1999\tLR: 0.010000\n",
            "Training Epoch: 12 [8064/50000]\tLoss: 1.2314\tLR: 0.010000\n",
            "Training Epoch: 12 [8192/50000]\tLoss: 1.2887\tLR: 0.010000\n",
            "Training Epoch: 12 [8320/50000]\tLoss: 1.1338\tLR: 0.010000\n",
            "Training Epoch: 12 [8448/50000]\tLoss: 1.2327\tLR: 0.010000\n",
            "Training Epoch: 12 [8576/50000]\tLoss: 1.2744\tLR: 0.010000\n",
            "Training Epoch: 12 [8704/50000]\tLoss: 1.1470\tLR: 0.010000\n",
            "Training Epoch: 12 [8832/50000]\tLoss: 1.0672\tLR: 0.010000\n",
            "Training Epoch: 12 [8960/50000]\tLoss: 1.1586\tLR: 0.010000\n",
            "Training Epoch: 12 [9088/50000]\tLoss: 1.0252\tLR: 0.010000\n",
            "Training Epoch: 12 [9216/50000]\tLoss: 1.2594\tLR: 0.010000\n",
            "Training Epoch: 12 [9344/50000]\tLoss: 1.2574\tLR: 0.010000\n",
            "Training Epoch: 12 [9472/50000]\tLoss: 1.1350\tLR: 0.010000\n",
            "Training Epoch: 12 [9600/50000]\tLoss: 1.0397\tLR: 0.010000\n",
            "Training Epoch: 12 [9728/50000]\tLoss: 1.1134\tLR: 0.010000\n",
            "Training Epoch: 12 [9856/50000]\tLoss: 1.1743\tLR: 0.010000\n",
            "Training Epoch: 12 [9984/50000]\tLoss: 1.2357\tLR: 0.010000\n",
            "Training Epoch: 12 [10112/50000]\tLoss: 1.1529\tLR: 0.010000\n",
            "Training Epoch: 12 [10240/50000]\tLoss: 1.0115\tLR: 0.010000\n",
            "Training Epoch: 12 [10368/50000]\tLoss: 1.1064\tLR: 0.010000\n",
            "Training Epoch: 12 [10496/50000]\tLoss: 1.5618\tLR: 0.010000\n",
            "Training Epoch: 12 [10624/50000]\tLoss: 1.1190\tLR: 0.010000\n",
            "Training Epoch: 12 [10752/50000]\tLoss: 1.2492\tLR: 0.010000\n",
            "Training Epoch: 12 [10880/50000]\tLoss: 0.8892\tLR: 0.010000\n",
            "Training Epoch: 12 [11008/50000]\tLoss: 1.2884\tLR: 0.010000\n",
            "Training Epoch: 12 [11136/50000]\tLoss: 1.1534\tLR: 0.010000\n",
            "Training Epoch: 12 [11264/50000]\tLoss: 1.0632\tLR: 0.010000\n",
            "Training Epoch: 12 [11392/50000]\tLoss: 1.2275\tLR: 0.010000\n",
            "Training Epoch: 12 [11520/50000]\tLoss: 1.1704\tLR: 0.010000\n",
            "Training Epoch: 12 [11648/50000]\tLoss: 1.1256\tLR: 0.010000\n",
            "Training Epoch: 12 [11776/50000]\tLoss: 1.4330\tLR: 0.010000\n",
            "Training Epoch: 12 [11904/50000]\tLoss: 1.1006\tLR: 0.010000\n",
            "Training Epoch: 12 [12032/50000]\tLoss: 1.4066\tLR: 0.010000\n",
            "Training Epoch: 12 [12160/50000]\tLoss: 1.0449\tLR: 0.010000\n",
            "Training Epoch: 12 [12288/50000]\tLoss: 1.0901\tLR: 0.010000\n",
            "Training Epoch: 12 [12416/50000]\tLoss: 1.1037\tLR: 0.010000\n",
            "Training Epoch: 12 [12544/50000]\tLoss: 1.0781\tLR: 0.010000\n",
            "Training Epoch: 12 [12672/50000]\tLoss: 1.3188\tLR: 0.010000\n",
            "Training Epoch: 12 [12800/50000]\tLoss: 1.2510\tLR: 0.010000\n",
            "Training Epoch: 12 [12928/50000]\tLoss: 1.1546\tLR: 0.010000\n",
            "Training Epoch: 12 [13056/50000]\tLoss: 1.1320\tLR: 0.010000\n",
            "Training Epoch: 12 [13184/50000]\tLoss: 1.3116\tLR: 0.010000\n",
            "Training Epoch: 12 [13312/50000]\tLoss: 1.2607\tLR: 0.010000\n",
            "Training Epoch: 12 [13440/50000]\tLoss: 1.2771\tLR: 0.010000\n",
            "Training Epoch: 12 [13568/50000]\tLoss: 1.2133\tLR: 0.010000\n",
            "Training Epoch: 12 [13696/50000]\tLoss: 1.0136\tLR: 0.010000\n",
            "Training Epoch: 12 [13824/50000]\tLoss: 1.1752\tLR: 0.010000\n",
            "Training Epoch: 12 [13952/50000]\tLoss: 1.1127\tLR: 0.010000\n",
            "Training Epoch: 12 [14080/50000]\tLoss: 1.1743\tLR: 0.010000\n",
            "Training Epoch: 12 [14208/50000]\tLoss: 1.2037\tLR: 0.010000\n",
            "Training Epoch: 12 [14336/50000]\tLoss: 1.3809\tLR: 0.010000\n",
            "Training Epoch: 12 [14464/50000]\tLoss: 1.0178\tLR: 0.010000\n",
            "Training Epoch: 12 [14592/50000]\tLoss: 1.0927\tLR: 0.010000\n",
            "Training Epoch: 12 [14720/50000]\tLoss: 1.0878\tLR: 0.010000\n",
            "Training Epoch: 12 [14848/50000]\tLoss: 1.3988\tLR: 0.010000\n",
            "Training Epoch: 12 [14976/50000]\tLoss: 1.2059\tLR: 0.010000\n",
            "Training Epoch: 12 [15104/50000]\tLoss: 1.0337\tLR: 0.010000\n",
            "Training Epoch: 12 [15232/50000]\tLoss: 1.2751\tLR: 0.010000\n",
            "Training Epoch: 12 [15360/50000]\tLoss: 1.4635\tLR: 0.010000\n",
            "Training Epoch: 12 [15488/50000]\tLoss: 1.3476\tLR: 0.010000\n",
            "Training Epoch: 12 [15616/50000]\tLoss: 1.1661\tLR: 0.010000\n",
            "Training Epoch: 12 [15744/50000]\tLoss: 1.0206\tLR: 0.010000\n",
            "Training Epoch: 12 [15872/50000]\tLoss: 1.4220\tLR: 0.010000\n",
            "Training Epoch: 12 [16000/50000]\tLoss: 1.2238\tLR: 0.010000\n",
            "Training Epoch: 12 [16128/50000]\tLoss: 0.9127\tLR: 0.010000\n",
            "Training Epoch: 12 [16256/50000]\tLoss: 1.0340\tLR: 0.010000\n",
            "Training Epoch: 12 [16384/50000]\tLoss: 1.0286\tLR: 0.010000\n",
            "Training Epoch: 12 [16512/50000]\tLoss: 1.2666\tLR: 0.010000\n",
            "Training Epoch: 12 [16640/50000]\tLoss: 1.2324\tLR: 0.010000\n",
            "Training Epoch: 12 [16768/50000]\tLoss: 1.0325\tLR: 0.010000\n",
            "Training Epoch: 12 [16896/50000]\tLoss: 1.3783\tLR: 0.010000\n",
            "Training Epoch: 12 [17024/50000]\tLoss: 1.0703\tLR: 0.010000\n",
            "Training Epoch: 12 [17152/50000]\tLoss: 1.0691\tLR: 0.010000\n",
            "Training Epoch: 12 [17280/50000]\tLoss: 1.1302\tLR: 0.010000\n",
            "Training Epoch: 12 [17408/50000]\tLoss: 1.1818\tLR: 0.010000\n",
            "Training Epoch: 12 [17536/50000]\tLoss: 0.9641\tLR: 0.010000\n",
            "Training Epoch: 12 [17664/50000]\tLoss: 1.2080\tLR: 0.010000\n",
            "Training Epoch: 12 [17792/50000]\tLoss: 1.3886\tLR: 0.010000\n",
            "Training Epoch: 12 [17920/50000]\tLoss: 1.0706\tLR: 0.010000\n",
            "Training Epoch: 12 [18048/50000]\tLoss: 1.1777\tLR: 0.010000\n",
            "Training Epoch: 12 [18176/50000]\tLoss: 1.0371\tLR: 0.010000\n",
            "Training Epoch: 12 [18304/50000]\tLoss: 1.4161\tLR: 0.010000\n",
            "Training Epoch: 12 [18432/50000]\tLoss: 1.2135\tLR: 0.010000\n",
            "Training Epoch: 12 [18560/50000]\tLoss: 1.2089\tLR: 0.010000\n",
            "Training Epoch: 12 [18688/50000]\tLoss: 1.2994\tLR: 0.010000\n",
            "Training Epoch: 12 [18816/50000]\tLoss: 0.9548\tLR: 0.010000\n",
            "Training Epoch: 12 [18944/50000]\tLoss: 1.1568\tLR: 0.010000\n",
            "Training Epoch: 12 [19072/50000]\tLoss: 1.3465\tLR: 0.010000\n",
            "Training Epoch: 12 [19200/50000]\tLoss: 1.1007\tLR: 0.010000\n",
            "Training Epoch: 12 [19328/50000]\tLoss: 1.0835\tLR: 0.010000\n",
            "Training Epoch: 12 [19456/50000]\tLoss: 1.3920\tLR: 0.010000\n",
            "Training Epoch: 12 [19584/50000]\tLoss: 1.3883\tLR: 0.010000\n",
            "Training Epoch: 12 [19712/50000]\tLoss: 1.1749\tLR: 0.010000\n",
            "Training Epoch: 12 [19840/50000]\tLoss: 1.2250\tLR: 0.010000\n",
            "Training Epoch: 12 [19968/50000]\tLoss: 1.0879\tLR: 0.010000\n",
            "Training Epoch: 12 [20096/50000]\tLoss: 1.3860\tLR: 0.010000\n",
            "Training Epoch: 12 [20224/50000]\tLoss: 1.1668\tLR: 0.010000\n",
            "Training Epoch: 12 [20352/50000]\tLoss: 1.2434\tLR: 0.010000\n",
            "Training Epoch: 12 [20480/50000]\tLoss: 1.2225\tLR: 0.010000\n",
            "Training Epoch: 12 [20608/50000]\tLoss: 1.3479\tLR: 0.010000\n",
            "Training Epoch: 12 [20736/50000]\tLoss: 1.0696\tLR: 0.010000\n",
            "Training Epoch: 12 [20864/50000]\tLoss: 1.3920\tLR: 0.010000\n",
            "Training Epoch: 12 [20992/50000]\tLoss: 1.0629\tLR: 0.010000\n",
            "Training Epoch: 12 [21120/50000]\tLoss: 1.2765\tLR: 0.010000\n",
            "Training Epoch: 12 [21248/50000]\tLoss: 1.1943\tLR: 0.010000\n",
            "Training Epoch: 12 [21376/50000]\tLoss: 1.1826\tLR: 0.010000\n",
            "Training Epoch: 12 [21504/50000]\tLoss: 1.3396\tLR: 0.010000\n",
            "Training Epoch: 12 [21632/50000]\tLoss: 1.2289\tLR: 0.010000\n",
            "Training Epoch: 12 [21760/50000]\tLoss: 1.0999\tLR: 0.010000\n",
            "Training Epoch: 12 [21888/50000]\tLoss: 1.4202\tLR: 0.010000\n",
            "Training Epoch: 12 [22016/50000]\tLoss: 1.3335\tLR: 0.010000\n",
            "Training Epoch: 12 [22144/50000]\tLoss: 1.2334\tLR: 0.010000\n",
            "Training Epoch: 12 [22272/50000]\tLoss: 1.0612\tLR: 0.010000\n",
            "Training Epoch: 12 [22400/50000]\tLoss: 1.1937\tLR: 0.010000\n",
            "Training Epoch: 12 [22528/50000]\tLoss: 1.3978\tLR: 0.010000\n",
            "Training Epoch: 12 [22656/50000]\tLoss: 0.9011\tLR: 0.010000\n",
            "Training Epoch: 12 [22784/50000]\tLoss: 1.0478\tLR: 0.010000\n",
            "Training Epoch: 12 [22912/50000]\tLoss: 1.1162\tLR: 0.010000\n",
            "Training Epoch: 12 [23040/50000]\tLoss: 1.3600\tLR: 0.010000\n",
            "Training Epoch: 12 [23168/50000]\tLoss: 1.1919\tLR: 0.010000\n",
            "Training Epoch: 12 [23296/50000]\tLoss: 1.1876\tLR: 0.010000\n",
            "Training Epoch: 12 [23424/50000]\tLoss: 1.3678\tLR: 0.010000\n",
            "Training Epoch: 12 [23552/50000]\tLoss: 1.0644\tLR: 0.010000\n",
            "Training Epoch: 12 [23680/50000]\tLoss: 1.4929\tLR: 0.010000\n",
            "Training Epoch: 12 [23808/50000]\tLoss: 1.0631\tLR: 0.010000\n",
            "Training Epoch: 12 [23936/50000]\tLoss: 1.1490\tLR: 0.010000\n",
            "Training Epoch: 12 [24064/50000]\tLoss: 1.3824\tLR: 0.010000\n",
            "Training Epoch: 12 [24192/50000]\tLoss: 1.2622\tLR: 0.010000\n",
            "Training Epoch: 12 [24320/50000]\tLoss: 1.1593\tLR: 0.010000\n",
            "Training Epoch: 12 [24448/50000]\tLoss: 1.1984\tLR: 0.010000\n",
            "Training Epoch: 12 [24576/50000]\tLoss: 1.2105\tLR: 0.010000\n",
            "Training Epoch: 12 [24704/50000]\tLoss: 0.9584\tLR: 0.010000\n",
            "Training Epoch: 12 [24832/50000]\tLoss: 1.2367\tLR: 0.010000\n",
            "Training Epoch: 12 [24960/50000]\tLoss: 1.2807\tLR: 0.010000\n",
            "Training Epoch: 12 [25088/50000]\tLoss: 1.0419\tLR: 0.010000\n",
            "Training Epoch: 12 [25216/50000]\tLoss: 1.1140\tLR: 0.010000\n",
            "Training Epoch: 12 [25344/50000]\tLoss: 1.1651\tLR: 0.010000\n",
            "Training Epoch: 12 [25472/50000]\tLoss: 1.2941\tLR: 0.010000\n",
            "Training Epoch: 12 [25600/50000]\tLoss: 1.0820\tLR: 0.010000\n",
            "Training Epoch: 12 [25728/50000]\tLoss: 1.3603\tLR: 0.010000\n",
            "Training Epoch: 12 [25856/50000]\tLoss: 1.0266\tLR: 0.010000\n",
            "Training Epoch: 12 [25984/50000]\tLoss: 1.0043\tLR: 0.010000\n",
            "Training Epoch: 12 [26112/50000]\tLoss: 1.1589\tLR: 0.010000\n",
            "Training Epoch: 12 [26240/50000]\tLoss: 1.1601\tLR: 0.010000\n",
            "Training Epoch: 12 [26368/50000]\tLoss: 1.2707\tLR: 0.010000\n",
            "Training Epoch: 12 [26496/50000]\tLoss: 0.9281\tLR: 0.010000\n",
            "Training Epoch: 12 [26624/50000]\tLoss: 1.0637\tLR: 0.010000\n",
            "Training Epoch: 12 [26752/50000]\tLoss: 1.1006\tLR: 0.010000\n",
            "Training Epoch: 12 [26880/50000]\tLoss: 1.1469\tLR: 0.010000\n",
            "Training Epoch: 12 [27008/50000]\tLoss: 1.0312\tLR: 0.010000\n",
            "Training Epoch: 12 [27136/50000]\tLoss: 1.1177\tLR: 0.010000\n",
            "Training Epoch: 12 [27264/50000]\tLoss: 1.3974\tLR: 0.010000\n",
            "Training Epoch: 12 [27392/50000]\tLoss: 1.0146\tLR: 0.010000\n",
            "Training Epoch: 12 [27520/50000]\tLoss: 1.2895\tLR: 0.010000\n",
            "Training Epoch: 12 [27648/50000]\tLoss: 1.1481\tLR: 0.010000\n",
            "Training Epoch: 12 [27776/50000]\tLoss: 1.2441\tLR: 0.010000\n",
            "Training Epoch: 12 [27904/50000]\tLoss: 1.2571\tLR: 0.010000\n",
            "Training Epoch: 12 [28032/50000]\tLoss: 1.4270\tLR: 0.010000\n",
            "Training Epoch: 12 [28160/50000]\tLoss: 1.3607\tLR: 0.010000\n",
            "Training Epoch: 12 [28288/50000]\tLoss: 1.0315\tLR: 0.010000\n",
            "Training Epoch: 12 [28416/50000]\tLoss: 1.3256\tLR: 0.010000\n",
            "Training Epoch: 12 [28544/50000]\tLoss: 1.3450\tLR: 0.010000\n",
            "Training Epoch: 12 [28672/50000]\tLoss: 1.2748\tLR: 0.010000\n",
            "Training Epoch: 12 [28800/50000]\tLoss: 1.3043\tLR: 0.010000\n",
            "Training Epoch: 12 [28928/50000]\tLoss: 0.9545\tLR: 0.010000\n",
            "Training Epoch: 12 [29056/50000]\tLoss: 1.3827\tLR: 0.010000\n",
            "Training Epoch: 12 [29184/50000]\tLoss: 1.3694\tLR: 0.010000\n",
            "Training Epoch: 12 [29312/50000]\tLoss: 1.2111\tLR: 0.010000\n",
            "Training Epoch: 12 [29440/50000]\tLoss: 1.1175\tLR: 0.010000\n",
            "Training Epoch: 12 [29568/50000]\tLoss: 1.1687\tLR: 0.010000\n",
            "Training Epoch: 12 [29696/50000]\tLoss: 1.2046\tLR: 0.010000\n",
            "Training Epoch: 12 [29824/50000]\tLoss: 1.3765\tLR: 0.010000\n",
            "Training Epoch: 12 [29952/50000]\tLoss: 1.2169\tLR: 0.010000\n",
            "Training Epoch: 12 [30080/50000]\tLoss: 1.3290\tLR: 0.010000\n",
            "Training Epoch: 12 [30208/50000]\tLoss: 1.4582\tLR: 0.010000\n",
            "Training Epoch: 12 [30336/50000]\tLoss: 1.1909\tLR: 0.010000\n",
            "Training Epoch: 12 [30464/50000]\tLoss: 1.3918\tLR: 0.010000\n",
            "Training Epoch: 12 [30592/50000]\tLoss: 1.2038\tLR: 0.010000\n",
            "Training Epoch: 12 [30720/50000]\tLoss: 1.0429\tLR: 0.010000\n",
            "Training Epoch: 12 [30848/50000]\tLoss: 1.1489\tLR: 0.010000\n",
            "Training Epoch: 12 [30976/50000]\tLoss: 0.9610\tLR: 0.010000\n",
            "Training Epoch: 12 [31104/50000]\tLoss: 1.3050\tLR: 0.010000\n",
            "Training Epoch: 12 [31232/50000]\tLoss: 1.2891\tLR: 0.010000\n",
            "Training Epoch: 12 [31360/50000]\tLoss: 1.1722\tLR: 0.010000\n",
            "Training Epoch: 12 [31488/50000]\tLoss: 1.2352\tLR: 0.010000\n",
            "Training Epoch: 12 [31616/50000]\tLoss: 1.1923\tLR: 0.010000\n",
            "Training Epoch: 12 [31744/50000]\tLoss: 1.2323\tLR: 0.010000\n",
            "Training Epoch: 12 [31872/50000]\tLoss: 1.5353\tLR: 0.010000\n",
            "Training Epoch: 12 [32000/50000]\tLoss: 1.5613\tLR: 0.010000\n",
            "Training Epoch: 12 [32128/50000]\tLoss: 1.0747\tLR: 0.010000\n",
            "Training Epoch: 12 [32256/50000]\tLoss: 1.2309\tLR: 0.010000\n",
            "Training Epoch: 12 [32384/50000]\tLoss: 1.3587\tLR: 0.010000\n",
            "Training Epoch: 12 [32512/50000]\tLoss: 1.2590\tLR: 0.010000\n",
            "Training Epoch: 12 [32640/50000]\tLoss: 1.2901\tLR: 0.010000\n",
            "Training Epoch: 12 [32768/50000]\tLoss: 1.2571\tLR: 0.010000\n",
            "Training Epoch: 12 [32896/50000]\tLoss: 1.3003\tLR: 0.010000\n",
            "Training Epoch: 12 [33024/50000]\tLoss: 1.1039\tLR: 0.010000\n",
            "Training Epoch: 12 [33152/50000]\tLoss: 0.9268\tLR: 0.010000\n",
            "Training Epoch: 12 [33280/50000]\tLoss: 1.2254\tLR: 0.010000\n",
            "Training Epoch: 12 [33408/50000]\tLoss: 1.2087\tLR: 0.010000\n",
            "Training Epoch: 12 [33536/50000]\tLoss: 1.3021\tLR: 0.010000\n",
            "Training Epoch: 12 [33664/50000]\tLoss: 1.0372\tLR: 0.010000\n",
            "Training Epoch: 12 [33792/50000]\tLoss: 1.1173\tLR: 0.010000\n",
            "Training Epoch: 12 [33920/50000]\tLoss: 1.3440\tLR: 0.010000\n",
            "Training Epoch: 12 [34048/50000]\tLoss: 1.2329\tLR: 0.010000\n",
            "Training Epoch: 12 [34176/50000]\tLoss: 1.0305\tLR: 0.010000\n",
            "Training Epoch: 12 [34304/50000]\tLoss: 1.2807\tLR: 0.010000\n",
            "Training Epoch: 12 [34432/50000]\tLoss: 1.0509\tLR: 0.010000\n",
            "Training Epoch: 12 [34560/50000]\tLoss: 1.2354\tLR: 0.010000\n",
            "Training Epoch: 12 [34688/50000]\tLoss: 1.3719\tLR: 0.010000\n",
            "Training Epoch: 12 [34816/50000]\tLoss: 0.9371\tLR: 0.010000\n",
            "Training Epoch: 12 [34944/50000]\tLoss: 1.1188\tLR: 0.010000\n",
            "Training Epoch: 12 [35072/50000]\tLoss: 1.3176\tLR: 0.010000\n",
            "Training Epoch: 12 [35200/50000]\tLoss: 1.1439\tLR: 0.010000\n",
            "Training Epoch: 12 [35328/50000]\tLoss: 1.1379\tLR: 0.010000\n",
            "Training Epoch: 12 [35456/50000]\tLoss: 1.1552\tLR: 0.010000\n",
            "Training Epoch: 12 [35584/50000]\tLoss: 1.0501\tLR: 0.010000\n",
            "Training Epoch: 12 [35712/50000]\tLoss: 1.2304\tLR: 0.010000\n",
            "Training Epoch: 12 [35840/50000]\tLoss: 1.1137\tLR: 0.010000\n",
            "Training Epoch: 12 [35968/50000]\tLoss: 1.3192\tLR: 0.010000\n",
            "Training Epoch: 12 [36096/50000]\tLoss: 1.1552\tLR: 0.010000\n",
            "Training Epoch: 12 [36224/50000]\tLoss: 1.2016\tLR: 0.010000\n",
            "Training Epoch: 12 [36352/50000]\tLoss: 1.3528\tLR: 0.010000\n",
            "Training Epoch: 12 [36480/50000]\tLoss: 1.0709\tLR: 0.010000\n",
            "Training Epoch: 12 [36608/50000]\tLoss: 1.1138\tLR: 0.010000\n",
            "Training Epoch: 12 [36736/50000]\tLoss: 1.0948\tLR: 0.010000\n",
            "Training Epoch: 12 [36864/50000]\tLoss: 1.3247\tLR: 0.010000\n",
            "Training Epoch: 12 [36992/50000]\tLoss: 1.0744\tLR: 0.010000\n",
            "Training Epoch: 12 [37120/50000]\tLoss: 1.1267\tLR: 0.010000\n",
            "Training Epoch: 12 [37248/50000]\tLoss: 1.1300\tLR: 0.010000\n",
            "Training Epoch: 12 [37376/50000]\tLoss: 1.2089\tLR: 0.010000\n",
            "Training Epoch: 12 [37504/50000]\tLoss: 1.3604\tLR: 0.010000\n",
            "Training Epoch: 12 [37632/50000]\tLoss: 1.3354\tLR: 0.010000\n",
            "Training Epoch: 12 [37760/50000]\tLoss: 1.1645\tLR: 0.010000\n",
            "Training Epoch: 12 [37888/50000]\tLoss: 1.2443\tLR: 0.010000\n",
            "Training Epoch: 12 [38016/50000]\tLoss: 1.1070\tLR: 0.010000\n",
            "Training Epoch: 12 [38144/50000]\tLoss: 1.0924\tLR: 0.010000\n",
            "Training Epoch: 12 [38272/50000]\tLoss: 1.3680\tLR: 0.010000\n",
            "Training Epoch: 12 [38400/50000]\tLoss: 1.4698\tLR: 0.010000\n",
            "Training Epoch: 12 [38528/50000]\tLoss: 1.0846\tLR: 0.010000\n",
            "Training Epoch: 12 [38656/50000]\tLoss: 1.3221\tLR: 0.010000\n",
            "Training Epoch: 12 [38784/50000]\tLoss: 1.3905\tLR: 0.010000\n",
            "Training Epoch: 12 [38912/50000]\tLoss: 1.1219\tLR: 0.010000\n",
            "Training Epoch: 12 [39040/50000]\tLoss: 1.1921\tLR: 0.010000\n",
            "Training Epoch: 12 [39168/50000]\tLoss: 1.2898\tLR: 0.010000\n",
            "Training Epoch: 12 [39296/50000]\tLoss: 1.2721\tLR: 0.010000\n",
            "Training Epoch: 12 [39424/50000]\tLoss: 1.3386\tLR: 0.010000\n",
            "Training Epoch: 12 [39552/50000]\tLoss: 1.1530\tLR: 0.010000\n",
            "Training Epoch: 12 [39680/50000]\tLoss: 1.3504\tLR: 0.010000\n",
            "Training Epoch: 12 [39808/50000]\tLoss: 1.2922\tLR: 0.010000\n",
            "Training Epoch: 12 [39936/50000]\tLoss: 1.2527\tLR: 0.010000\n",
            "Training Epoch: 12 [40064/50000]\tLoss: 1.2659\tLR: 0.010000\n",
            "Training Epoch: 12 [40192/50000]\tLoss: 1.2071\tLR: 0.010000\n",
            "Training Epoch: 12 [40320/50000]\tLoss: 1.0007\tLR: 0.010000\n",
            "Training Epoch: 12 [40448/50000]\tLoss: 1.2026\tLR: 0.010000\n",
            "Training Epoch: 12 [40576/50000]\tLoss: 1.4217\tLR: 0.010000\n",
            "Training Epoch: 12 [40704/50000]\tLoss: 1.2133\tLR: 0.010000\n",
            "Training Epoch: 12 [40832/50000]\tLoss: 1.1971\tLR: 0.010000\n",
            "Training Epoch: 12 [40960/50000]\tLoss: 1.0082\tLR: 0.010000\n",
            "Training Epoch: 12 [41088/50000]\tLoss: 1.1076\tLR: 0.010000\n",
            "Training Epoch: 12 [41216/50000]\tLoss: 1.0750\tLR: 0.010000\n",
            "Training Epoch: 12 [41344/50000]\tLoss: 1.3115\tLR: 0.010000\n",
            "Training Epoch: 12 [41472/50000]\tLoss: 1.3494\tLR: 0.010000\n",
            "Training Epoch: 12 [41600/50000]\tLoss: 1.2080\tLR: 0.010000\n",
            "Training Epoch: 12 [41728/50000]\tLoss: 1.2915\tLR: 0.010000\n",
            "Training Epoch: 12 [41856/50000]\tLoss: 1.1382\tLR: 0.010000\n",
            "Training Epoch: 12 [41984/50000]\tLoss: 0.9713\tLR: 0.010000\n",
            "Training Epoch: 12 [42112/50000]\tLoss: 1.1805\tLR: 0.010000\n",
            "Training Epoch: 12 [42240/50000]\tLoss: 1.1032\tLR: 0.010000\n",
            "Training Epoch: 12 [42368/50000]\tLoss: 1.1922\tLR: 0.010000\n",
            "Training Epoch: 12 [42496/50000]\tLoss: 1.2922\tLR: 0.010000\n",
            "Training Epoch: 12 [42624/50000]\tLoss: 1.1748\tLR: 0.010000\n",
            "Training Epoch: 12 [42752/50000]\tLoss: 1.3394\tLR: 0.010000\n",
            "Training Epoch: 12 [42880/50000]\tLoss: 0.9447\tLR: 0.010000\n",
            "Training Epoch: 12 [43008/50000]\tLoss: 1.1128\tLR: 0.010000\n",
            "Training Epoch: 12 [43136/50000]\tLoss: 1.0991\tLR: 0.010000\n",
            "Training Epoch: 12 [43264/50000]\tLoss: 1.2560\tLR: 0.010000\n",
            "Training Epoch: 12 [43392/50000]\tLoss: 1.2159\tLR: 0.010000\n",
            "Training Epoch: 12 [43520/50000]\tLoss: 1.1567\tLR: 0.010000\n",
            "Training Epoch: 12 [43648/50000]\tLoss: 1.1632\tLR: 0.010000\n",
            "Training Epoch: 12 [43776/50000]\tLoss: 1.2589\tLR: 0.010000\n",
            "Training Epoch: 12 [43904/50000]\tLoss: 1.1451\tLR: 0.010000\n",
            "Training Epoch: 12 [44032/50000]\tLoss: 0.8854\tLR: 0.010000\n",
            "Training Epoch: 12 [44160/50000]\tLoss: 1.2210\tLR: 0.010000\n",
            "Training Epoch: 12 [44288/50000]\tLoss: 1.2005\tLR: 0.010000\n",
            "Training Epoch: 12 [44416/50000]\tLoss: 1.3577\tLR: 0.010000\n",
            "Training Epoch: 12 [44544/50000]\tLoss: 1.2174\tLR: 0.010000\n",
            "Training Epoch: 12 [44672/50000]\tLoss: 1.0329\tLR: 0.010000\n",
            "Training Epoch: 12 [44800/50000]\tLoss: 1.2687\tLR: 0.010000\n",
            "Training Epoch: 12 [44928/50000]\tLoss: 1.2266\tLR: 0.010000\n",
            "Training Epoch: 12 [45056/50000]\tLoss: 1.0829\tLR: 0.010000\n",
            "Training Epoch: 12 [45184/50000]\tLoss: 1.3094\tLR: 0.010000\n",
            "Training Epoch: 12 [45312/50000]\tLoss: 1.1450\tLR: 0.010000\n",
            "Training Epoch: 12 [45440/50000]\tLoss: 1.1556\tLR: 0.010000\n",
            "Training Epoch: 12 [45568/50000]\tLoss: 0.7973\tLR: 0.010000\n",
            "Training Epoch: 12 [45696/50000]\tLoss: 1.3754\tLR: 0.010000\n",
            "Training Epoch: 12 [45824/50000]\tLoss: 1.2683\tLR: 0.010000\n",
            "Training Epoch: 12 [45952/50000]\tLoss: 1.2863\tLR: 0.010000\n",
            "Training Epoch: 12 [46080/50000]\tLoss: 1.2686\tLR: 0.010000\n",
            "Training Epoch: 12 [46208/50000]\tLoss: 1.1764\tLR: 0.010000\n",
            "Training Epoch: 12 [46336/50000]\tLoss: 1.2535\tLR: 0.010000\n",
            "Training Epoch: 12 [46464/50000]\tLoss: 1.4021\tLR: 0.010000\n",
            "Training Epoch: 12 [46592/50000]\tLoss: 1.1605\tLR: 0.010000\n",
            "Training Epoch: 12 [46720/50000]\tLoss: 1.2041\tLR: 0.010000\n",
            "Training Epoch: 12 [46848/50000]\tLoss: 1.3140\tLR: 0.010000\n",
            "Training Epoch: 12 [46976/50000]\tLoss: 1.2222\tLR: 0.010000\n",
            "Training Epoch: 12 [47104/50000]\tLoss: 1.2588\tLR: 0.010000\n",
            "Training Epoch: 12 [47232/50000]\tLoss: 1.3126\tLR: 0.010000\n",
            "Training Epoch: 12 [47360/50000]\tLoss: 1.2122\tLR: 0.010000\n",
            "Training Epoch: 12 [47488/50000]\tLoss: 1.1778\tLR: 0.010000\n",
            "Training Epoch: 12 [47616/50000]\tLoss: 1.3886\tLR: 0.010000\n",
            "Training Epoch: 12 [47744/50000]\tLoss: 1.1517\tLR: 0.010000\n",
            "Training Epoch: 12 [47872/50000]\tLoss: 1.1455\tLR: 0.010000\n",
            "Training Epoch: 12 [48000/50000]\tLoss: 1.2513\tLR: 0.010000\n",
            "Training Epoch: 12 [48128/50000]\tLoss: 1.2721\tLR: 0.010000\n",
            "Training Epoch: 12 [48256/50000]\tLoss: 1.2008\tLR: 0.010000\n",
            "Training Epoch: 12 [48384/50000]\tLoss: 1.2288\tLR: 0.010000\n",
            "Training Epoch: 12 [48512/50000]\tLoss: 1.1669\tLR: 0.010000\n",
            "Training Epoch: 12 [48640/50000]\tLoss: 1.1721\tLR: 0.010000\n",
            "Training Epoch: 12 [48768/50000]\tLoss: 1.2896\tLR: 0.010000\n",
            "Training Epoch: 12 [48896/50000]\tLoss: 1.2697\tLR: 0.010000\n",
            "Training Epoch: 12 [49024/50000]\tLoss: 1.3101\tLR: 0.010000\n",
            "Training Epoch: 12 [49152/50000]\tLoss: 1.4138\tLR: 0.010000\n",
            "Training Epoch: 12 [49280/50000]\tLoss: 1.2029\tLR: 0.010000\n",
            "Training Epoch: 12 [49408/50000]\tLoss: 1.3953\tLR: 0.010000\n",
            "Training Epoch: 12 [49536/50000]\tLoss: 1.1934\tLR: 0.010000\n",
            "Training Epoch: 12 [49664/50000]\tLoss: 1.3366\tLR: 0.010000\n",
            "Training Epoch: 12 [49792/50000]\tLoss: 1.1022\tLR: 0.010000\n",
            "Training Epoch: 12 [49920/50000]\tLoss: 0.9475\tLR: 0.010000\n",
            "Training Epoch: 12 [50000/50000]\tLoss: 1.2114\tLR: 0.010000\n",
            "epoch 12 training time consumed: 205.97s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 100806 GiB | 100806 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 100733 GiB | 100733 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     73 GiB |     73 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 100806 GiB | 100806 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 100733 GiB | 100733 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     73 GiB |     73 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 100795 GiB | 100795 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 100723 GiB | 100722 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     72 GiB |     72 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  73203 GiB |  73202 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  73128 GiB |  73127 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     75 GiB |     75 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    4002 K  |    4002 K  |\n",
            "|       from large pool |      92    |     184    |    2256 K  |    2256 K  |\n",
            "|       from small pool |     519    |     646    |    1745 K  |    1745 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    4002 K  |    4002 K  |\n",
            "|       from large pool |      92    |     184    |    2256 K  |    2256 K  |\n",
            "|       from small pool |     519    |     646    |    1745 K  |    1745 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |     122    |    1689 K  |    1688 K  |\n",
            "|       from large pool |      42    |     111    |    1332 K  |    1332 K  |\n",
            "|       from small pool |       7    |      21    |     356 K  |     356 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0117, Accuracy: 0.5949, Time consumed:11.19s\n",
            "\n",
            "Training Epoch: 13 [128/50000]\tLoss: 1.1504\tLR: 0.010000\n",
            "Training Epoch: 13 [256/50000]\tLoss: 1.2472\tLR: 0.010000\n",
            "Training Epoch: 13 [384/50000]\tLoss: 0.9897\tLR: 0.010000\n",
            "Training Epoch: 13 [512/50000]\tLoss: 1.0532\tLR: 0.010000\n",
            "Training Epoch: 13 [640/50000]\tLoss: 1.0771\tLR: 0.010000\n",
            "Training Epoch: 13 [768/50000]\tLoss: 1.0288\tLR: 0.010000\n",
            "Training Epoch: 13 [896/50000]\tLoss: 1.0834\tLR: 0.010000\n",
            "Training Epoch: 13 [1024/50000]\tLoss: 1.1142\tLR: 0.010000\n",
            "Training Epoch: 13 [1152/50000]\tLoss: 1.1012\tLR: 0.010000\n",
            "Training Epoch: 13 [1280/50000]\tLoss: 1.2756\tLR: 0.010000\n",
            "Training Epoch: 13 [1408/50000]\tLoss: 1.2784\tLR: 0.010000\n",
            "Training Epoch: 13 [1536/50000]\tLoss: 0.8924\tLR: 0.010000\n",
            "Training Epoch: 13 [1664/50000]\tLoss: 1.0672\tLR: 0.010000\n",
            "Training Epoch: 13 [1792/50000]\tLoss: 1.2955\tLR: 0.010000\n",
            "Training Epoch: 13 [1920/50000]\tLoss: 1.1086\tLR: 0.010000\n",
            "Training Epoch: 13 [2048/50000]\tLoss: 0.9354\tLR: 0.010000\n",
            "Training Epoch: 13 [2176/50000]\tLoss: 1.2847\tLR: 0.010000\n",
            "Training Epoch: 13 [2304/50000]\tLoss: 1.1735\tLR: 0.010000\n",
            "Training Epoch: 13 [2432/50000]\tLoss: 1.0035\tLR: 0.010000\n",
            "Training Epoch: 13 [2560/50000]\tLoss: 0.9453\tLR: 0.010000\n",
            "Training Epoch: 13 [2688/50000]\tLoss: 1.0871\tLR: 0.010000\n",
            "Training Epoch: 13 [2816/50000]\tLoss: 0.9339\tLR: 0.010000\n",
            "Training Epoch: 13 [2944/50000]\tLoss: 1.1177\tLR: 0.010000\n",
            "Training Epoch: 13 [3072/50000]\tLoss: 1.0698\tLR: 0.010000\n",
            "Training Epoch: 13 [3200/50000]\tLoss: 0.9331\tLR: 0.010000\n",
            "Training Epoch: 13 [3328/50000]\tLoss: 1.0598\tLR: 0.010000\n",
            "Training Epoch: 13 [3456/50000]\tLoss: 0.9381\tLR: 0.010000\n",
            "Training Epoch: 13 [3584/50000]\tLoss: 1.1254\tLR: 0.010000\n",
            "Training Epoch: 13 [3712/50000]\tLoss: 1.1278\tLR: 0.010000\n",
            "Training Epoch: 13 [3840/50000]\tLoss: 1.0775\tLR: 0.010000\n",
            "Training Epoch: 13 [3968/50000]\tLoss: 1.1387\tLR: 0.010000\n",
            "Training Epoch: 13 [4096/50000]\tLoss: 1.2000\tLR: 0.010000\n",
            "Training Epoch: 13 [4224/50000]\tLoss: 0.9423\tLR: 0.010000\n",
            "Training Epoch: 13 [4352/50000]\tLoss: 1.0576\tLR: 0.010000\n",
            "Training Epoch: 13 [4480/50000]\tLoss: 1.1270\tLR: 0.010000\n",
            "Training Epoch: 13 [4608/50000]\tLoss: 1.2114\tLR: 0.010000\n",
            "Training Epoch: 13 [4736/50000]\tLoss: 0.8969\tLR: 0.010000\n",
            "Training Epoch: 13 [4864/50000]\tLoss: 1.2232\tLR: 0.010000\n",
            "Training Epoch: 13 [4992/50000]\tLoss: 0.9525\tLR: 0.010000\n",
            "Training Epoch: 13 [5120/50000]\tLoss: 1.0167\tLR: 0.010000\n",
            "Training Epoch: 13 [5248/50000]\tLoss: 0.9841\tLR: 0.010000\n",
            "Training Epoch: 13 [5376/50000]\tLoss: 1.0626\tLR: 0.010000\n",
            "Training Epoch: 13 [5504/50000]\tLoss: 1.0035\tLR: 0.010000\n",
            "Training Epoch: 13 [5632/50000]\tLoss: 1.1671\tLR: 0.010000\n",
            "Training Epoch: 13 [5760/50000]\tLoss: 1.2041\tLR: 0.010000\n",
            "Training Epoch: 13 [5888/50000]\tLoss: 0.9772\tLR: 0.010000\n",
            "Training Epoch: 13 [6016/50000]\tLoss: 1.0196\tLR: 0.010000\n",
            "Training Epoch: 13 [6144/50000]\tLoss: 1.1221\tLR: 0.010000\n",
            "Training Epoch: 13 [6272/50000]\tLoss: 1.0926\tLR: 0.010000\n",
            "Training Epoch: 13 [6400/50000]\tLoss: 1.1935\tLR: 0.010000\n",
            "Training Epoch: 13 [6528/50000]\tLoss: 1.2763\tLR: 0.010000\n",
            "Training Epoch: 13 [6656/50000]\tLoss: 1.2927\tLR: 0.010000\n",
            "Training Epoch: 13 [6784/50000]\tLoss: 0.9452\tLR: 0.010000\n",
            "Training Epoch: 13 [6912/50000]\tLoss: 0.8721\tLR: 0.010000\n",
            "Training Epoch: 13 [7040/50000]\tLoss: 0.9240\tLR: 0.010000\n",
            "Training Epoch: 13 [7168/50000]\tLoss: 1.2387\tLR: 0.010000\n",
            "Training Epoch: 13 [7296/50000]\tLoss: 1.3008\tLR: 0.010000\n",
            "Training Epoch: 13 [7424/50000]\tLoss: 1.1366\tLR: 0.010000\n",
            "Training Epoch: 13 [7552/50000]\tLoss: 0.9783\tLR: 0.010000\n",
            "Training Epoch: 13 [7680/50000]\tLoss: 1.1628\tLR: 0.010000\n",
            "Training Epoch: 13 [7808/50000]\tLoss: 1.1679\tLR: 0.010000\n",
            "Training Epoch: 13 [7936/50000]\tLoss: 1.0216\tLR: 0.010000\n",
            "Training Epoch: 13 [8064/50000]\tLoss: 0.8651\tLR: 0.010000\n",
            "Training Epoch: 13 [8192/50000]\tLoss: 0.9700\tLR: 0.010000\n",
            "Training Epoch: 13 [8320/50000]\tLoss: 1.0292\tLR: 0.010000\n",
            "Training Epoch: 13 [8448/50000]\tLoss: 0.9949\tLR: 0.010000\n",
            "Training Epoch: 13 [8576/50000]\tLoss: 0.9476\tLR: 0.010000\n",
            "Training Epoch: 13 [8704/50000]\tLoss: 1.0683\tLR: 0.010000\n",
            "Training Epoch: 13 [8832/50000]\tLoss: 0.9430\tLR: 0.010000\n",
            "Training Epoch: 13 [8960/50000]\tLoss: 1.0560\tLR: 0.010000\n",
            "Training Epoch: 13 [9088/50000]\tLoss: 1.1338\tLR: 0.010000\n",
            "Training Epoch: 13 [9216/50000]\tLoss: 1.0327\tLR: 0.010000\n",
            "Training Epoch: 13 [9344/50000]\tLoss: 1.0960\tLR: 0.010000\n",
            "Training Epoch: 13 [9472/50000]\tLoss: 1.0988\tLR: 0.010000\n",
            "Training Epoch: 13 [9600/50000]\tLoss: 1.0116\tLR: 0.010000\n",
            "Training Epoch: 13 [9728/50000]\tLoss: 1.1215\tLR: 0.010000\n",
            "Training Epoch: 13 [9856/50000]\tLoss: 1.1915\tLR: 0.010000\n",
            "Training Epoch: 13 [9984/50000]\tLoss: 1.1541\tLR: 0.010000\n",
            "Training Epoch: 13 [10112/50000]\tLoss: 1.2419\tLR: 0.010000\n",
            "Training Epoch: 13 [10240/50000]\tLoss: 1.0728\tLR: 0.010000\n",
            "Training Epoch: 13 [10368/50000]\tLoss: 1.2532\tLR: 0.010000\n",
            "Training Epoch: 13 [10496/50000]\tLoss: 1.1993\tLR: 0.010000\n",
            "Training Epoch: 13 [10624/50000]\tLoss: 0.9643\tLR: 0.010000\n",
            "Training Epoch: 13 [10752/50000]\tLoss: 1.1786\tLR: 0.010000\n",
            "Training Epoch: 13 [10880/50000]\tLoss: 0.9823\tLR: 0.010000\n",
            "Training Epoch: 13 [11008/50000]\tLoss: 1.1819\tLR: 0.010000\n",
            "Training Epoch: 13 [11136/50000]\tLoss: 1.1936\tLR: 0.010000\n",
            "Training Epoch: 13 [11264/50000]\tLoss: 0.9277\tLR: 0.010000\n",
            "Training Epoch: 13 [11392/50000]\tLoss: 1.1768\tLR: 0.010000\n",
            "Training Epoch: 13 [11520/50000]\tLoss: 0.9976\tLR: 0.010000\n",
            "Training Epoch: 13 [11648/50000]\tLoss: 1.0797\tLR: 0.010000\n",
            "Training Epoch: 13 [11776/50000]\tLoss: 1.0830\tLR: 0.010000\n",
            "Training Epoch: 13 [11904/50000]\tLoss: 1.1260\tLR: 0.010000\n",
            "Training Epoch: 13 [12032/50000]\tLoss: 0.8474\tLR: 0.010000\n",
            "Training Epoch: 13 [12160/50000]\tLoss: 1.1046\tLR: 0.010000\n",
            "Training Epoch: 13 [12288/50000]\tLoss: 1.0642\tLR: 0.010000\n",
            "Training Epoch: 13 [12416/50000]\tLoss: 1.2632\tLR: 0.010000\n",
            "Training Epoch: 13 [12544/50000]\tLoss: 1.0688\tLR: 0.010000\n",
            "Training Epoch: 13 [12672/50000]\tLoss: 0.9256\tLR: 0.010000\n",
            "Training Epoch: 13 [12800/50000]\tLoss: 0.8620\tLR: 0.010000\n",
            "Training Epoch: 13 [12928/50000]\tLoss: 1.1075\tLR: 0.010000\n",
            "Training Epoch: 13 [13056/50000]\tLoss: 1.1043\tLR: 0.010000\n",
            "Training Epoch: 13 [13184/50000]\tLoss: 0.8951\tLR: 0.010000\n",
            "Training Epoch: 13 [13312/50000]\tLoss: 1.1724\tLR: 0.010000\n",
            "Training Epoch: 13 [13440/50000]\tLoss: 1.0129\tLR: 0.010000\n",
            "Training Epoch: 13 [13568/50000]\tLoss: 1.2133\tLR: 0.010000\n",
            "Training Epoch: 13 [13696/50000]\tLoss: 0.9995\tLR: 0.010000\n",
            "Training Epoch: 13 [13824/50000]\tLoss: 1.0158\tLR: 0.010000\n",
            "Training Epoch: 13 [13952/50000]\tLoss: 1.2119\tLR: 0.010000\n",
            "Training Epoch: 13 [14080/50000]\tLoss: 1.0290\tLR: 0.010000\n",
            "Training Epoch: 13 [14208/50000]\tLoss: 0.9404\tLR: 0.010000\n",
            "Training Epoch: 13 [14336/50000]\tLoss: 1.0880\tLR: 0.010000\n",
            "Training Epoch: 13 [14464/50000]\tLoss: 0.9043\tLR: 0.010000\n",
            "Training Epoch: 13 [14592/50000]\tLoss: 1.3313\tLR: 0.010000\n",
            "Training Epoch: 13 [14720/50000]\tLoss: 1.1330\tLR: 0.010000\n",
            "Training Epoch: 13 [14848/50000]\tLoss: 0.9593\tLR: 0.010000\n",
            "Training Epoch: 13 [14976/50000]\tLoss: 1.2648\tLR: 0.010000\n",
            "Training Epoch: 13 [15104/50000]\tLoss: 1.1251\tLR: 0.010000\n",
            "Training Epoch: 13 [15232/50000]\tLoss: 1.0404\tLR: 0.010000\n",
            "Training Epoch: 13 [15360/50000]\tLoss: 1.0894\tLR: 0.010000\n",
            "Training Epoch: 13 [15488/50000]\tLoss: 1.0372\tLR: 0.010000\n",
            "Training Epoch: 13 [15616/50000]\tLoss: 1.1582\tLR: 0.010000\n",
            "Training Epoch: 13 [15744/50000]\tLoss: 1.1372\tLR: 0.010000\n",
            "Training Epoch: 13 [15872/50000]\tLoss: 1.2253\tLR: 0.010000\n",
            "Training Epoch: 13 [16000/50000]\tLoss: 1.2344\tLR: 0.010000\n",
            "Training Epoch: 13 [16128/50000]\tLoss: 1.1145\tLR: 0.010000\n",
            "Training Epoch: 13 [16256/50000]\tLoss: 1.1808\tLR: 0.010000\n",
            "Training Epoch: 13 [16384/50000]\tLoss: 1.2027\tLR: 0.010000\n",
            "Training Epoch: 13 [16512/50000]\tLoss: 0.9961\tLR: 0.010000\n",
            "Training Epoch: 13 [16640/50000]\tLoss: 1.2386\tLR: 0.010000\n",
            "Training Epoch: 13 [16768/50000]\tLoss: 1.1095\tLR: 0.010000\n",
            "Training Epoch: 13 [16896/50000]\tLoss: 1.2154\tLR: 0.010000\n",
            "Training Epoch: 13 [17024/50000]\tLoss: 1.1114\tLR: 0.010000\n",
            "Training Epoch: 13 [17152/50000]\tLoss: 1.1990\tLR: 0.010000\n",
            "Training Epoch: 13 [17280/50000]\tLoss: 1.0394\tLR: 0.010000\n",
            "Training Epoch: 13 [17408/50000]\tLoss: 1.0102\tLR: 0.010000\n",
            "Training Epoch: 13 [17536/50000]\tLoss: 1.3570\tLR: 0.010000\n",
            "Training Epoch: 13 [17664/50000]\tLoss: 1.1063\tLR: 0.010000\n",
            "Training Epoch: 13 [17792/50000]\tLoss: 1.0392\tLR: 0.010000\n",
            "Training Epoch: 13 [17920/50000]\tLoss: 1.2044\tLR: 0.010000\n",
            "Training Epoch: 13 [18048/50000]\tLoss: 1.2210\tLR: 0.010000\n",
            "Training Epoch: 13 [18176/50000]\tLoss: 1.1733\tLR: 0.010000\n",
            "Training Epoch: 13 [18304/50000]\tLoss: 1.2758\tLR: 0.010000\n",
            "Training Epoch: 13 [18432/50000]\tLoss: 0.9143\tLR: 0.010000\n",
            "Training Epoch: 13 [18560/50000]\tLoss: 0.9331\tLR: 0.010000\n",
            "Training Epoch: 13 [18688/50000]\tLoss: 0.8886\tLR: 0.010000\n",
            "Training Epoch: 13 [18816/50000]\tLoss: 1.1316\tLR: 0.010000\n",
            "Training Epoch: 13 [18944/50000]\tLoss: 1.1332\tLR: 0.010000\n",
            "Training Epoch: 13 [19072/50000]\tLoss: 1.1512\tLR: 0.010000\n",
            "Training Epoch: 13 [19200/50000]\tLoss: 1.2154\tLR: 0.010000\n",
            "Training Epoch: 13 [19328/50000]\tLoss: 0.9936\tLR: 0.010000\n",
            "Training Epoch: 13 [19456/50000]\tLoss: 1.1790\tLR: 0.010000\n",
            "Training Epoch: 13 [19584/50000]\tLoss: 1.1472\tLR: 0.010000\n",
            "Training Epoch: 13 [19712/50000]\tLoss: 1.1101\tLR: 0.010000\n",
            "Training Epoch: 13 [19840/50000]\tLoss: 1.1214\tLR: 0.010000\n",
            "Training Epoch: 13 [19968/50000]\tLoss: 1.0738\tLR: 0.010000\n",
            "Training Epoch: 13 [20096/50000]\tLoss: 0.9593\tLR: 0.010000\n",
            "Training Epoch: 13 [20224/50000]\tLoss: 1.0733\tLR: 0.010000\n",
            "Training Epoch: 13 [20352/50000]\tLoss: 0.9706\tLR: 0.010000\n",
            "Training Epoch: 13 [20480/50000]\tLoss: 1.1744\tLR: 0.010000\n",
            "Training Epoch: 13 [20608/50000]\tLoss: 1.0693\tLR: 0.010000\n",
            "Training Epoch: 13 [20736/50000]\tLoss: 1.1187\tLR: 0.010000\n",
            "Training Epoch: 13 [20864/50000]\tLoss: 1.0838\tLR: 0.010000\n",
            "Training Epoch: 13 [20992/50000]\tLoss: 1.1109\tLR: 0.010000\n",
            "Training Epoch: 13 [21120/50000]\tLoss: 1.2801\tLR: 0.010000\n",
            "Training Epoch: 13 [21248/50000]\tLoss: 0.9864\tLR: 0.010000\n",
            "Training Epoch: 13 [21376/50000]\tLoss: 1.0993\tLR: 0.010000\n",
            "Training Epoch: 13 [21504/50000]\tLoss: 1.1859\tLR: 0.010000\n",
            "Training Epoch: 13 [21632/50000]\tLoss: 1.3889\tLR: 0.010000\n",
            "Training Epoch: 13 [21760/50000]\tLoss: 1.1613\tLR: 0.010000\n",
            "Training Epoch: 13 [21888/50000]\tLoss: 0.9588\tLR: 0.010000\n",
            "Training Epoch: 13 [22016/50000]\tLoss: 1.1642\tLR: 0.010000\n",
            "Training Epoch: 13 [22144/50000]\tLoss: 0.9298\tLR: 0.010000\n",
            "Training Epoch: 13 [22272/50000]\tLoss: 1.3934\tLR: 0.010000\n",
            "Training Epoch: 13 [22400/50000]\tLoss: 1.2925\tLR: 0.010000\n",
            "Training Epoch: 13 [22528/50000]\tLoss: 1.1050\tLR: 0.010000\n",
            "Training Epoch: 13 [22656/50000]\tLoss: 1.1951\tLR: 0.010000\n",
            "Training Epoch: 13 [22784/50000]\tLoss: 0.9995\tLR: 0.010000\n",
            "Training Epoch: 13 [22912/50000]\tLoss: 0.9643\tLR: 0.010000\n",
            "Training Epoch: 13 [23040/50000]\tLoss: 1.0946\tLR: 0.010000\n",
            "Training Epoch: 13 [23168/50000]\tLoss: 1.0276\tLR: 0.010000\n",
            "Training Epoch: 13 [23296/50000]\tLoss: 1.1164\tLR: 0.010000\n",
            "Training Epoch: 13 [23424/50000]\tLoss: 1.3155\tLR: 0.010000\n",
            "Training Epoch: 13 [23552/50000]\tLoss: 1.0045\tLR: 0.010000\n",
            "Training Epoch: 13 [23680/50000]\tLoss: 1.0308\tLR: 0.010000\n",
            "Training Epoch: 13 [23808/50000]\tLoss: 1.1767\tLR: 0.010000\n",
            "Training Epoch: 13 [23936/50000]\tLoss: 1.0809\tLR: 0.010000\n",
            "Training Epoch: 13 [24064/50000]\tLoss: 1.2734\tLR: 0.010000\n",
            "Training Epoch: 13 [24192/50000]\tLoss: 1.1580\tLR: 0.010000\n",
            "Training Epoch: 13 [24320/50000]\tLoss: 1.2532\tLR: 0.010000\n",
            "Training Epoch: 13 [24448/50000]\tLoss: 1.0735\tLR: 0.010000\n",
            "Training Epoch: 13 [24576/50000]\tLoss: 1.1543\tLR: 0.010000\n",
            "Training Epoch: 13 [24704/50000]\tLoss: 1.1945\tLR: 0.010000\n",
            "Training Epoch: 13 [24832/50000]\tLoss: 1.2573\tLR: 0.010000\n",
            "Training Epoch: 13 [24960/50000]\tLoss: 1.0439\tLR: 0.010000\n",
            "Training Epoch: 13 [25088/50000]\tLoss: 1.0126\tLR: 0.010000\n",
            "Training Epoch: 13 [25216/50000]\tLoss: 0.9780\tLR: 0.010000\n",
            "Training Epoch: 13 [25344/50000]\tLoss: 1.2781\tLR: 0.010000\n",
            "Training Epoch: 13 [25472/50000]\tLoss: 1.0969\tLR: 0.010000\n",
            "Training Epoch: 13 [25600/50000]\tLoss: 1.2842\tLR: 0.010000\n",
            "Training Epoch: 13 [25728/50000]\tLoss: 1.2671\tLR: 0.010000\n",
            "Training Epoch: 13 [25856/50000]\tLoss: 0.9896\tLR: 0.010000\n",
            "Training Epoch: 13 [25984/50000]\tLoss: 1.1413\tLR: 0.010000\n",
            "Training Epoch: 13 [26112/50000]\tLoss: 1.0410\tLR: 0.010000\n",
            "Training Epoch: 13 [26240/50000]\tLoss: 0.8899\tLR: 0.010000\n",
            "Training Epoch: 13 [26368/50000]\tLoss: 1.0556\tLR: 0.010000\n",
            "Training Epoch: 13 [26496/50000]\tLoss: 1.0263\tLR: 0.010000\n",
            "Training Epoch: 13 [26624/50000]\tLoss: 1.1047\tLR: 0.010000\n",
            "Training Epoch: 13 [26752/50000]\tLoss: 1.1384\tLR: 0.010000\n",
            "Training Epoch: 13 [26880/50000]\tLoss: 1.1326\tLR: 0.010000\n",
            "Training Epoch: 13 [27008/50000]\tLoss: 1.0667\tLR: 0.010000\n",
            "Training Epoch: 13 [27136/50000]\tLoss: 1.0997\tLR: 0.010000\n",
            "Training Epoch: 13 [27264/50000]\tLoss: 0.9372\tLR: 0.010000\n",
            "Training Epoch: 13 [27392/50000]\tLoss: 0.9755\tLR: 0.010000\n",
            "Training Epoch: 13 [27520/50000]\tLoss: 0.9445\tLR: 0.010000\n",
            "Training Epoch: 13 [27648/50000]\tLoss: 1.1026\tLR: 0.010000\n",
            "Training Epoch: 13 [27776/50000]\tLoss: 1.1816\tLR: 0.010000\n",
            "Training Epoch: 13 [27904/50000]\tLoss: 1.0215\tLR: 0.010000\n",
            "Training Epoch: 13 [28032/50000]\tLoss: 1.1780\tLR: 0.010000\n",
            "Training Epoch: 13 [28160/50000]\tLoss: 1.2409\tLR: 0.010000\n",
            "Training Epoch: 13 [28288/50000]\tLoss: 1.2356\tLR: 0.010000\n",
            "Training Epoch: 13 [28416/50000]\tLoss: 1.1279\tLR: 0.010000\n",
            "Training Epoch: 13 [28544/50000]\tLoss: 0.9727\tLR: 0.010000\n",
            "Training Epoch: 13 [28672/50000]\tLoss: 1.0459\tLR: 0.010000\n",
            "Training Epoch: 13 [28800/50000]\tLoss: 1.0090\tLR: 0.010000\n",
            "Training Epoch: 13 [28928/50000]\tLoss: 1.3659\tLR: 0.010000\n",
            "Training Epoch: 13 [29056/50000]\tLoss: 1.0691\tLR: 0.010000\n",
            "Training Epoch: 13 [29184/50000]\tLoss: 1.0019\tLR: 0.010000\n",
            "Training Epoch: 13 [29312/50000]\tLoss: 1.1643\tLR: 0.010000\n",
            "Training Epoch: 13 [29440/50000]\tLoss: 0.9239\tLR: 0.010000\n",
            "Training Epoch: 13 [29568/50000]\tLoss: 1.1543\tLR: 0.010000\n",
            "Training Epoch: 13 [29696/50000]\tLoss: 1.4319\tLR: 0.010000\n",
            "Training Epoch: 13 [29824/50000]\tLoss: 1.1890\tLR: 0.010000\n",
            "Training Epoch: 13 [29952/50000]\tLoss: 1.0742\tLR: 0.010000\n",
            "Training Epoch: 13 [30080/50000]\tLoss: 1.1662\tLR: 0.010000\n",
            "Training Epoch: 13 [30208/50000]\tLoss: 1.1087\tLR: 0.010000\n",
            "Training Epoch: 13 [30464/50000]\tLoss: 1.0616\tLR: 0.010000\n",
            "Training Epoch: 13 [30592/50000]\tLoss: 1.1189\tLR: 0.010000\n",
            "Training Epoch: 13 [30720/50000]\tLoss: 0.9901\tLR: 0.010000\n",
            "Training Epoch: 13 [30848/50000]\tLoss: 1.0261\tLR: 0.010000\n",
            "Training Epoch: 13 [30976/50000]\tLoss: 0.9194\tLR: 0.010000\n",
            "Training Epoch: 13 [31104/50000]\tLoss: 1.0532\tLR: 0.010000\n",
            "Training Epoch: 13 [31232/50000]\tLoss: 1.2452\tLR: 0.010000\n",
            "Training Epoch: 13 [31360/50000]\tLoss: 1.0733\tLR: 0.010000\n",
            "Training Epoch: 13 [31488/50000]\tLoss: 1.1747\tLR: 0.010000\n",
            "Training Epoch: 13 [31616/50000]\tLoss: 1.1346\tLR: 0.010000\n",
            "Training Epoch: 13 [31744/50000]\tLoss: 1.0166\tLR: 0.010000\n",
            "Training Epoch: 13 [31872/50000]\tLoss: 0.7866\tLR: 0.010000\n",
            "Training Epoch: 13 [32000/50000]\tLoss: 1.2348\tLR: 0.010000\n",
            "Training Epoch: 13 [32128/50000]\tLoss: 1.2498\tLR: 0.010000\n",
            "Training Epoch: 13 [32256/50000]\tLoss: 1.2834\tLR: 0.010000\n",
            "Training Epoch: 13 [32384/50000]\tLoss: 1.0935\tLR: 0.010000\n",
            "Training Epoch: 13 [32512/50000]\tLoss: 1.0860\tLR: 0.010000\n",
            "Training Epoch: 13 [32640/50000]\tLoss: 1.0588\tLR: 0.010000\n",
            "Training Epoch: 13 [32768/50000]\tLoss: 1.1771\tLR: 0.010000\n",
            "Training Epoch: 13 [32896/50000]\tLoss: 1.1466\tLR: 0.010000\n",
            "Training Epoch: 13 [33024/50000]\tLoss: 1.1806\tLR: 0.010000\n",
            "Training Epoch: 13 [33152/50000]\tLoss: 1.1696\tLR: 0.010000\n",
            "Training Epoch: 13 [33280/50000]\tLoss: 1.3100\tLR: 0.010000\n",
            "Training Epoch: 13 [33408/50000]\tLoss: 1.2313\tLR: 0.010000\n",
            "Training Epoch: 13 [33536/50000]\tLoss: 1.3681\tLR: 0.010000\n",
            "Training Epoch: 13 [33664/50000]\tLoss: 1.2036\tLR: 0.010000\n",
            "Training Epoch: 13 [33792/50000]\tLoss: 1.2098\tLR: 0.010000\n",
            "Training Epoch: 13 [33920/50000]\tLoss: 1.1386\tLR: 0.010000\n",
            "Training Epoch: 13 [34048/50000]\tLoss: 1.1305\tLR: 0.010000\n",
            "Training Epoch: 13 [34176/50000]\tLoss: 1.1447\tLR: 0.010000\n",
            "Training Epoch: 13 [34304/50000]\tLoss: 0.9562\tLR: 0.010000\n",
            "Training Epoch: 13 [34432/50000]\tLoss: 1.0475\tLR: 0.010000\n",
            "Training Epoch: 13 [34560/50000]\tLoss: 1.2436\tLR: 0.010000\n",
            "Training Epoch: 13 [34688/50000]\tLoss: 1.0391\tLR: 0.010000\n",
            "Training Epoch: 13 [34816/50000]\tLoss: 0.9785\tLR: 0.010000\n",
            "Training Epoch: 13 [34944/50000]\tLoss: 1.2748\tLR: 0.010000\n",
            "Training Epoch: 13 [35072/50000]\tLoss: 0.9671\tLR: 0.010000\n",
            "Training Epoch: 13 [35200/50000]\tLoss: 1.3825\tLR: 0.010000\n",
            "Training Epoch: 13 [35328/50000]\tLoss: 1.3856\tLR: 0.010000\n",
            "Training Epoch: 13 [35456/50000]\tLoss: 1.0376\tLR: 0.010000\n",
            "Training Epoch: 13 [35584/50000]\tLoss: 1.1894\tLR: 0.010000\n",
            "Training Epoch: 13 [35712/50000]\tLoss: 1.1565\tLR: 0.010000\n",
            "Training Epoch: 13 [35840/50000]\tLoss: 0.9806\tLR: 0.010000\n",
            "Training Epoch: 13 [35968/50000]\tLoss: 0.9543\tLR: 0.010000\n",
            "Training Epoch: 13 [36096/50000]\tLoss: 1.2421\tLR: 0.010000\n",
            "Training Epoch: 13 [36224/50000]\tLoss: 1.0903\tLR: 0.010000\n",
            "Training Epoch: 13 [36352/50000]\tLoss: 0.9896\tLR: 0.010000\n",
            "Training Epoch: 13 [36480/50000]\tLoss: 1.0819\tLR: 0.010000\n",
            "Training Epoch: 13 [36608/50000]\tLoss: 1.2865\tLR: 0.010000\n",
            "Training Epoch: 13 [36736/50000]\tLoss: 1.1019\tLR: 0.010000\n",
            "Training Epoch: 13 [36864/50000]\tLoss: 0.9464\tLR: 0.010000\n",
            "Training Epoch: 13 [36992/50000]\tLoss: 1.1741\tLR: 0.010000\n",
            "Training Epoch: 13 [37120/50000]\tLoss: 1.3091\tLR: 0.010000\n",
            "Training Epoch: 13 [37248/50000]\tLoss: 1.0769\tLR: 0.010000\n",
            "Training Epoch: 13 [37376/50000]\tLoss: 0.9996\tLR: 0.010000\n",
            "Training Epoch: 13 [37504/50000]\tLoss: 1.0111\tLR: 0.010000\n",
            "Training Epoch: 13 [37632/50000]\tLoss: 1.1460\tLR: 0.010000\n",
            "Training Epoch: 13 [37760/50000]\tLoss: 1.1300\tLR: 0.010000\n",
            "Training Epoch: 13 [37888/50000]\tLoss: 1.1657\tLR: 0.010000\n",
            "Training Epoch: 13 [38016/50000]\tLoss: 1.1752\tLR: 0.010000\n",
            "Training Epoch: 13 [38144/50000]\tLoss: 1.1361\tLR: 0.010000\n",
            "Training Epoch: 13 [38272/50000]\tLoss: 1.2171\tLR: 0.010000\n",
            "Training Epoch: 13 [38400/50000]\tLoss: 1.2400\tLR: 0.010000\n",
            "Training Epoch: 13 [38528/50000]\tLoss: 1.1210\tLR: 0.010000\n",
            "Training Epoch: 13 [38656/50000]\tLoss: 1.0068\tLR: 0.010000\n",
            "Training Epoch: 13 [38784/50000]\tLoss: 1.1296\tLR: 0.010000\n",
            "Training Epoch: 13 [38912/50000]\tLoss: 1.0300\tLR: 0.010000\n",
            "Training Epoch: 13 [39040/50000]\tLoss: 1.2589\tLR: 0.010000\n",
            "Training Epoch: 13 [39168/50000]\tLoss: 1.3027\tLR: 0.010000\n",
            "Training Epoch: 13 [39296/50000]\tLoss: 1.3767\tLR: 0.010000\n",
            "Training Epoch: 13 [39424/50000]\tLoss: 1.2175\tLR: 0.010000\n",
            "Training Epoch: 13 [39552/50000]\tLoss: 1.0730\tLR: 0.010000\n",
            "Training Epoch: 13 [39680/50000]\tLoss: 0.9987\tLR: 0.010000\n",
            "Training Epoch: 13 [39808/50000]\tLoss: 1.1591\tLR: 0.010000\n",
            "Training Epoch: 13 [39936/50000]\tLoss: 1.2985\tLR: 0.010000\n",
            "Training Epoch: 13 [40064/50000]\tLoss: 1.1711\tLR: 0.010000\n",
            "Training Epoch: 13 [40192/50000]\tLoss: 1.1943\tLR: 0.010000\n",
            "Training Epoch: 13 [40320/50000]\tLoss: 1.3811\tLR: 0.010000\n",
            "Training Epoch: 13 [40448/50000]\tLoss: 1.2304\tLR: 0.010000\n",
            "Training Epoch: 13 [40576/50000]\tLoss: 1.1353\tLR: 0.010000\n",
            "Training Epoch: 13 [40704/50000]\tLoss: 1.0469\tLR: 0.010000\n",
            "Training Epoch: 13 [40832/50000]\tLoss: 1.2282\tLR: 0.010000\n",
            "Training Epoch: 13 [40960/50000]\tLoss: 1.3977\tLR: 0.010000\n",
            "Training Epoch: 13 [41088/50000]\tLoss: 1.2003\tLR: 0.010000\n",
            "Training Epoch: 13 [41216/50000]\tLoss: 1.1939\tLR: 0.010000\n",
            "Training Epoch: 13 [41344/50000]\tLoss: 1.1940\tLR: 0.010000\n",
            "Training Epoch: 13 [41472/50000]\tLoss: 1.0677\tLR: 0.010000\n",
            "Training Epoch: 13 [41600/50000]\tLoss: 1.1430\tLR: 0.010000\n",
            "Training Epoch: 13 [41728/50000]\tLoss: 1.1878\tLR: 0.010000\n",
            "Training Epoch: 13 [41856/50000]\tLoss: 1.3387\tLR: 0.010000\n",
            "Training Epoch: 13 [41984/50000]\tLoss: 1.0362\tLR: 0.010000\n",
            "Training Epoch: 13 [42112/50000]\tLoss: 1.1476\tLR: 0.010000\n",
            "Training Epoch: 13 [42240/50000]\tLoss: 1.3544\tLR: 0.010000\n",
            "Training Epoch: 13 [42368/50000]\tLoss: 1.1272\tLR: 0.010000\n",
            "Training Epoch: 13 [42496/50000]\tLoss: 1.3454\tLR: 0.010000\n",
            "Training Epoch: 13 [42624/50000]\tLoss: 1.2480\tLR: 0.010000\n",
            "Training Epoch: 13 [42752/50000]\tLoss: 1.2579\tLR: 0.010000\n",
            "Training Epoch: 13 [42880/50000]\tLoss: 0.9205\tLR: 0.010000\n",
            "Training Epoch: 13 [43008/50000]\tLoss: 1.3448\tLR: 0.010000\n",
            "Training Epoch: 13 [43136/50000]\tLoss: 1.2609\tLR: 0.010000\n",
            "Training Epoch: 13 [43264/50000]\tLoss: 1.3660\tLR: 0.010000\n",
            "Training Epoch: 13 [43392/50000]\tLoss: 1.2416\tLR: 0.010000\n",
            "Training Epoch: 13 [43520/50000]\tLoss: 1.3345\tLR: 0.010000\n",
            "Training Epoch: 13 [43648/50000]\tLoss: 1.1309\tLR: 0.010000\n",
            "Training Epoch: 13 [43776/50000]\tLoss: 1.1014\tLR: 0.010000\n",
            "Training Epoch: 13 [43904/50000]\tLoss: 1.1639\tLR: 0.010000\n",
            "Training Epoch: 13 [44032/50000]\tLoss: 1.0445\tLR: 0.010000\n",
            "Training Epoch: 13 [44160/50000]\tLoss: 1.1105\tLR: 0.010000\n",
            "Training Epoch: 13 [44288/50000]\tLoss: 0.8643\tLR: 0.010000\n",
            "Training Epoch: 13 [44416/50000]\tLoss: 1.3416\tLR: 0.010000\n",
            "Training Epoch: 13 [44544/50000]\tLoss: 1.1918\tLR: 0.010000\n",
            "Training Epoch: 13 [44672/50000]\tLoss: 1.1894\tLR: 0.010000\n",
            "Training Epoch: 13 [44800/50000]\tLoss: 1.1896\tLR: 0.010000\n",
            "Training Epoch: 13 [44928/50000]\tLoss: 1.1443\tLR: 0.010000\n",
            "Training Epoch: 13 [45056/50000]\tLoss: 1.1771\tLR: 0.010000\n",
            "Training Epoch: 13 [45184/50000]\tLoss: 1.1165\tLR: 0.010000\n",
            "Training Epoch: 13 [45312/50000]\tLoss: 1.3144\tLR: 0.010000\n",
            "Training Epoch: 13 [45440/50000]\tLoss: 1.2068\tLR: 0.010000\n",
            "Training Epoch: 13 [45568/50000]\tLoss: 1.1898\tLR: 0.010000\n",
            "Training Epoch: 13 [45696/50000]\tLoss: 1.1060\tLR: 0.010000\n",
            "Training Epoch: 13 [45824/50000]\tLoss: 1.0507\tLR: 0.010000\n",
            "Training Epoch: 13 [45952/50000]\tLoss: 1.1897\tLR: 0.010000\n",
            "Training Epoch: 13 [46080/50000]\tLoss: 1.1129\tLR: 0.010000\n",
            "Training Epoch: 13 [46208/50000]\tLoss: 0.9441\tLR: 0.010000\n",
            "Training Epoch: 13 [46336/50000]\tLoss: 1.2800\tLR: 0.010000\n",
            "Training Epoch: 13 [46464/50000]\tLoss: 1.0853\tLR: 0.010000\n",
            "Training Epoch: 13 [46592/50000]\tLoss: 1.2200\tLR: 0.010000\n",
            "Training Epoch: 13 [46720/50000]\tLoss: 1.2307\tLR: 0.010000\n",
            "Training Epoch: 13 [46848/50000]\tLoss: 1.1751\tLR: 0.010000\n",
            "Training Epoch: 13 [46976/50000]\tLoss: 1.0760\tLR: 0.010000\n",
            "Training Epoch: 13 [47104/50000]\tLoss: 1.2966\tLR: 0.010000\n",
            "Training Epoch: 13 [47232/50000]\tLoss: 0.9994\tLR: 0.010000\n",
            "Training Epoch: 13 [47360/50000]\tLoss: 1.1776\tLR: 0.010000\n",
            "Training Epoch: 13 [47488/50000]\tLoss: 1.1565\tLR: 0.010000\n",
            "Training Epoch: 13 [47616/50000]\tLoss: 0.9634\tLR: 0.010000\n",
            "Training Epoch: 13 [47744/50000]\tLoss: 1.1449\tLR: 0.010000\n",
            "Training Epoch: 13 [47872/50000]\tLoss: 1.0675\tLR: 0.010000\n",
            "Training Epoch: 13 [48000/50000]\tLoss: 1.2525\tLR: 0.010000\n",
            "Training Epoch: 13 [48128/50000]\tLoss: 1.3216\tLR: 0.010000\n",
            "Training Epoch: 13 [48256/50000]\tLoss: 1.5059\tLR: 0.010000\n",
            "Training Epoch: 13 [48384/50000]\tLoss: 1.0989\tLR: 0.010000\n",
            "Training Epoch: 13 [48512/50000]\tLoss: 1.1240\tLR: 0.010000\n",
            "Training Epoch: 13 [48640/50000]\tLoss: 1.0104\tLR: 0.010000\n",
            "Training Epoch: 13 [48768/50000]\tLoss: 1.1067\tLR: 0.010000\n",
            "Training Epoch: 13 [48896/50000]\tLoss: 1.0853\tLR: 0.010000\n",
            "Training Epoch: 13 [49024/50000]\tLoss: 1.1088\tLR: 0.010000\n",
            "Training Epoch: 13 [49152/50000]\tLoss: 1.1768\tLR: 0.010000\n",
            "Training Epoch: 13 [49280/50000]\tLoss: 1.0148\tLR: 0.010000\n",
            "Training Epoch: 13 [49408/50000]\tLoss: 1.3078\tLR: 0.010000\n",
            "Training Epoch: 13 [49536/50000]\tLoss: 1.2827\tLR: 0.010000\n",
            "Training Epoch: 13 [49664/50000]\tLoss: 1.2989\tLR: 0.010000\n",
            "Training Epoch: 13 [49792/50000]\tLoss: 1.0964\tLR: 0.010000\n",
            "Training Epoch: 13 [49920/50000]\tLoss: 1.0318\tLR: 0.010000\n",
            "Training Epoch: 13 [50000/50000]\tLoss: 1.2856\tLR: 0.010000\n",
            "epoch 13 training time consumed: 206.07s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 109205 GiB | 109205 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 109126 GiB | 109125 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     79 GiB |     79 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 109205 GiB | 109205 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 109126 GiB | 109125 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     79 GiB |     79 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 109194 GiB | 109193 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 109115 GiB | 109114 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     78 GiB |     78 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  79303 GiB |  79302 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  79221 GiB |  79221 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     81 GiB |     81 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    4336 K  |    4335 K  |\n",
            "|       from large pool |      92    |     184    |    2444 K  |    2444 K  |\n",
            "|       from small pool |     519    |     646    |    1891 K  |    1890 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    4336 K  |    4335 K  |\n",
            "|       from large pool |      92    |     184    |    2444 K  |    2444 K  |\n",
            "|       from small pool |     519    |     646    |    1891 K  |    1890 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |     122    |    1830 K  |    1830 K  |\n",
            "|       from large pool |      42    |     111    |    1443 K  |    1443 K  |\n",
            "|       from small pool |       8    |      21    |     387 K  |     387 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0115, Accuracy: 0.6072, Time consumed:11.26s\n",
            "\n",
            "Training Epoch: 14 [128/50000]\tLoss: 0.6999\tLR: 0.010000\n",
            "Training Epoch: 14 [256/50000]\tLoss: 0.9016\tLR: 0.010000\n",
            "Training Epoch: 14 [384/50000]\tLoss: 1.1132\tLR: 0.010000\n",
            "Training Epoch: 14 [512/50000]\tLoss: 0.8299\tLR: 0.010000\n",
            "Training Epoch: 14 [640/50000]\tLoss: 1.1318\tLR: 0.010000\n",
            "Training Epoch: 14 [768/50000]\tLoss: 1.1429\tLR: 0.010000\n",
            "Training Epoch: 14 [896/50000]\tLoss: 0.9329\tLR: 0.010000\n",
            "Training Epoch: 14 [1024/50000]\tLoss: 1.0473\tLR: 0.010000\n",
            "Training Epoch: 14 [1152/50000]\tLoss: 1.0548\tLR: 0.010000\n",
            "Training Epoch: 14 [1280/50000]\tLoss: 0.9587\tLR: 0.010000\n",
            "Training Epoch: 14 [1408/50000]\tLoss: 0.9883\tLR: 0.010000\n",
            "Training Epoch: 14 [1536/50000]\tLoss: 0.9685\tLR: 0.010000\n",
            "Training Epoch: 14 [1664/50000]\tLoss: 0.9978\tLR: 0.010000\n",
            "Training Epoch: 14 [1792/50000]\tLoss: 0.9205\tLR: 0.010000\n",
            "Training Epoch: 14 [1920/50000]\tLoss: 0.9700\tLR: 0.010000\n",
            "Training Epoch: 14 [2048/50000]\tLoss: 1.1148\tLR: 0.010000\n",
            "Training Epoch: 14 [2176/50000]\tLoss: 0.9241\tLR: 0.010000\n",
            "Training Epoch: 14 [2304/50000]\tLoss: 1.0453\tLR: 0.010000\n",
            "Training Epoch: 14 [2432/50000]\tLoss: 0.9732\tLR: 0.010000\n",
            "Training Epoch: 14 [2560/50000]\tLoss: 1.1520\tLR: 0.010000\n",
            "Training Epoch: 14 [2688/50000]\tLoss: 0.9843\tLR: 0.010000\n",
            "Training Epoch: 14 [2816/50000]\tLoss: 1.0267\tLR: 0.010000\n",
            "Training Epoch: 14 [2944/50000]\tLoss: 0.8854\tLR: 0.010000\n",
            "Training Epoch: 14 [3072/50000]\tLoss: 1.0065\tLR: 0.010000\n",
            "Training Epoch: 14 [3200/50000]\tLoss: 0.9511\tLR: 0.010000\n",
            "Training Epoch: 14 [3328/50000]\tLoss: 0.9184\tLR: 0.010000\n",
            "Training Epoch: 14 [3456/50000]\tLoss: 0.9142\tLR: 0.010000\n",
            "Training Epoch: 14 [3584/50000]\tLoss: 1.1136\tLR: 0.010000\n",
            "Training Epoch: 14 [3712/50000]\tLoss: 1.1051\tLR: 0.010000\n",
            "Training Epoch: 14 [3840/50000]\tLoss: 0.9552\tLR: 0.010000\n",
            "Training Epoch: 14 [3968/50000]\tLoss: 1.1396\tLR: 0.010000\n",
            "Training Epoch: 14 [4096/50000]\tLoss: 0.9662\tLR: 0.010000\n",
            "Training Epoch: 14 [4224/50000]\tLoss: 0.9182\tLR: 0.010000\n",
            "Training Epoch: 14 [4352/50000]\tLoss: 0.8647\tLR: 0.010000\n",
            "Training Epoch: 14 [4480/50000]\tLoss: 1.1661\tLR: 0.010000\n",
            "Training Epoch: 14 [4608/50000]\tLoss: 1.0846\tLR: 0.010000\n",
            "Training Epoch: 14 [4736/50000]\tLoss: 0.9788\tLR: 0.010000\n",
            "Training Epoch: 14 [4864/50000]\tLoss: 0.9151\tLR: 0.010000\n",
            "Training Epoch: 14 [4992/50000]\tLoss: 1.1056\tLR: 0.010000\n",
            "Training Epoch: 14 [5120/50000]\tLoss: 0.9388\tLR: 0.010000\n",
            "Training Epoch: 14 [5248/50000]\tLoss: 0.8698\tLR: 0.010000\n",
            "Training Epoch: 14 [5376/50000]\tLoss: 1.0156\tLR: 0.010000\n",
            "Training Epoch: 14 [5504/50000]\tLoss: 0.8893\tLR: 0.010000\n",
            "Training Epoch: 14 [5632/50000]\tLoss: 1.0202\tLR: 0.010000\n",
            "Training Epoch: 14 [5760/50000]\tLoss: 1.0777\tLR: 0.010000\n",
            "Training Epoch: 14 [5888/50000]\tLoss: 0.8560\tLR: 0.010000\n",
            "Training Epoch: 14 [6016/50000]\tLoss: 0.8572\tLR: 0.010000\n",
            "Training Epoch: 14 [6144/50000]\tLoss: 0.8925\tLR: 0.010000\n",
            "Training Epoch: 14 [6272/50000]\tLoss: 1.0740\tLR: 0.010000\n",
            "Training Epoch: 14 [6400/50000]\tLoss: 0.9672\tLR: 0.010000\n",
            "Training Epoch: 14 [6528/50000]\tLoss: 0.9462\tLR: 0.010000\n",
            "Training Epoch: 14 [6656/50000]\tLoss: 1.1059\tLR: 0.010000\n",
            "Training Epoch: 14 [6784/50000]\tLoss: 1.0393\tLR: 0.010000\n",
            "Training Epoch: 14 [6912/50000]\tLoss: 1.1837\tLR: 0.010000\n",
            "Training Epoch: 14 [7040/50000]\tLoss: 0.8857\tLR: 0.010000\n",
            "Training Epoch: 14 [7168/50000]\tLoss: 1.0623\tLR: 0.010000\n",
            "Training Epoch: 14 [7296/50000]\tLoss: 0.8491\tLR: 0.010000\n",
            "Training Epoch: 14 [7424/50000]\tLoss: 1.1837\tLR: 0.010000\n",
            "Training Epoch: 14 [7552/50000]\tLoss: 0.9384\tLR: 0.010000\n",
            "Training Epoch: 14 [7680/50000]\tLoss: 1.3223\tLR: 0.010000\n",
            "Training Epoch: 14 [7808/50000]\tLoss: 1.0132\tLR: 0.010000\n",
            "Training Epoch: 14 [7936/50000]\tLoss: 1.1246\tLR: 0.010000\n",
            "Training Epoch: 14 [8064/50000]\tLoss: 0.8744\tLR: 0.010000\n",
            "Training Epoch: 14 [8192/50000]\tLoss: 1.2268\tLR: 0.010000\n",
            "Training Epoch: 14 [8320/50000]\tLoss: 0.9868\tLR: 0.010000\n",
            "Training Epoch: 14 [8448/50000]\tLoss: 0.9434\tLR: 0.010000\n",
            "Training Epoch: 14 [8576/50000]\tLoss: 1.0422\tLR: 0.010000\n",
            "Training Epoch: 14 [8704/50000]\tLoss: 0.9624\tLR: 0.010000\n",
            "Training Epoch: 14 [8832/50000]\tLoss: 1.1318\tLR: 0.010000\n",
            "Training Epoch: 14 [8960/50000]\tLoss: 0.9640\tLR: 0.010000\n",
            "Training Epoch: 14 [9088/50000]\tLoss: 0.8908\tLR: 0.010000\n",
            "Training Epoch: 14 [9216/50000]\tLoss: 0.9794\tLR: 0.010000\n",
            "Training Epoch: 14 [9344/50000]\tLoss: 1.1419\tLR: 0.010000\n",
            "Training Epoch: 14 [9472/50000]\tLoss: 0.8671\tLR: 0.010000\n",
            "Training Epoch: 14 [9600/50000]\tLoss: 1.1386\tLR: 0.010000\n",
            "Training Epoch: 14 [9728/50000]\tLoss: 1.0202\tLR: 0.010000\n",
            "Training Epoch: 14 [9856/50000]\tLoss: 1.1185\tLR: 0.010000\n",
            "Training Epoch: 14 [9984/50000]\tLoss: 1.0646\tLR: 0.010000\n",
            "Training Epoch: 14 [10112/50000]\tLoss: 1.0345\tLR: 0.010000\n",
            "Training Epoch: 14 [10240/50000]\tLoss: 1.3524\tLR: 0.010000\n",
            "Training Epoch: 14 [10368/50000]\tLoss: 0.9207\tLR: 0.010000\n",
            "Training Epoch: 14 [10496/50000]\tLoss: 1.0570\tLR: 0.010000\n",
            "Training Epoch: 14 [10624/50000]\tLoss: 0.7862\tLR: 0.010000\n",
            "Training Epoch: 14 [10752/50000]\tLoss: 1.0529\tLR: 0.010000\n",
            "Training Epoch: 14 [10880/50000]\tLoss: 1.1453\tLR: 0.010000\n",
            "Training Epoch: 14 [11008/50000]\tLoss: 0.9160\tLR: 0.010000\n",
            "Training Epoch: 14 [11136/50000]\tLoss: 0.8570\tLR: 0.010000\n",
            "Training Epoch: 14 [11264/50000]\tLoss: 1.0797\tLR: 0.010000\n",
            "Training Epoch: 14 [11392/50000]\tLoss: 1.0659\tLR: 0.010000\n",
            "Training Epoch: 14 [11520/50000]\tLoss: 1.1813\tLR: 0.010000\n",
            "Training Epoch: 14 [11648/50000]\tLoss: 1.2809\tLR: 0.010000\n",
            "Training Epoch: 14 [11776/50000]\tLoss: 0.8783\tLR: 0.010000\n",
            "Training Epoch: 14 [11904/50000]\tLoss: 1.0950\tLR: 0.010000\n",
            "Training Epoch: 14 [12032/50000]\tLoss: 1.0807\tLR: 0.010000\n",
            "Training Epoch: 14 [12160/50000]\tLoss: 1.0071\tLR: 0.010000\n",
            "Training Epoch: 14 [12288/50000]\tLoss: 0.9491\tLR: 0.010000\n",
            "Training Epoch: 14 [12416/50000]\tLoss: 1.1438\tLR: 0.010000\n",
            "Training Epoch: 14 [12544/50000]\tLoss: 0.9433\tLR: 0.010000\n",
            "Training Epoch: 14 [12672/50000]\tLoss: 0.9026\tLR: 0.010000\n",
            "Training Epoch: 14 [12800/50000]\tLoss: 1.0494\tLR: 0.010000\n",
            "Training Epoch: 14 [12928/50000]\tLoss: 0.9911\tLR: 0.010000\n",
            "Training Epoch: 14 [13056/50000]\tLoss: 1.1446\tLR: 0.010000\n",
            "Training Epoch: 14 [13184/50000]\tLoss: 1.1284\tLR: 0.010000\n",
            "Training Epoch: 14 [13312/50000]\tLoss: 0.9525\tLR: 0.010000\n",
            "Training Epoch: 14 [13440/50000]\tLoss: 0.9784\tLR: 0.010000\n",
            "Training Epoch: 14 [13568/50000]\tLoss: 0.9626\tLR: 0.010000\n",
            "Training Epoch: 14 [13696/50000]\tLoss: 1.1663\tLR: 0.010000\n",
            "Training Epoch: 14 [13824/50000]\tLoss: 1.0226\tLR: 0.010000\n",
            "Training Epoch: 14 [13952/50000]\tLoss: 0.8899\tLR: 0.010000\n",
            "Training Epoch: 14 [14080/50000]\tLoss: 1.1606\tLR: 0.010000\n",
            "Training Epoch: 14 [14208/50000]\tLoss: 0.9069\tLR: 0.010000\n",
            "Training Epoch: 14 [14336/50000]\tLoss: 1.2076\tLR: 0.010000\n",
            "Training Epoch: 14 [14464/50000]\tLoss: 0.9899\tLR: 0.010000\n",
            "Training Epoch: 14 [14592/50000]\tLoss: 0.9676\tLR: 0.010000\n",
            "Training Epoch: 14 [14720/50000]\tLoss: 0.9858\tLR: 0.010000\n",
            "Training Epoch: 14 [14848/50000]\tLoss: 1.1736\tLR: 0.010000\n",
            "Training Epoch: 14 [14976/50000]\tLoss: 1.1582\tLR: 0.010000\n",
            "Training Epoch: 14 [15104/50000]\tLoss: 1.1034\tLR: 0.010000\n",
            "Training Epoch: 14 [15232/50000]\tLoss: 0.9978\tLR: 0.010000\n",
            "Training Epoch: 14 [15360/50000]\tLoss: 1.1523\tLR: 0.010000\n",
            "Training Epoch: 14 [15488/50000]\tLoss: 1.0295\tLR: 0.010000\n",
            "Training Epoch: 14 [15616/50000]\tLoss: 1.0940\tLR: 0.010000\n",
            "Training Epoch: 14 [15744/50000]\tLoss: 1.0602\tLR: 0.010000\n",
            "Training Epoch: 14 [15872/50000]\tLoss: 0.9877\tLR: 0.010000\n",
            "Training Epoch: 14 [16000/50000]\tLoss: 1.1024\tLR: 0.010000\n",
            "Training Epoch: 14 [16128/50000]\tLoss: 1.2514\tLR: 0.010000\n",
            "Training Epoch: 14 [16256/50000]\tLoss: 0.9269\tLR: 0.010000\n",
            "Training Epoch: 14 [16384/50000]\tLoss: 0.9789\tLR: 0.010000\n",
            "Training Epoch: 14 [16512/50000]\tLoss: 1.0472\tLR: 0.010000\n",
            "Training Epoch: 14 [16640/50000]\tLoss: 0.9202\tLR: 0.010000\n",
            "Training Epoch: 14 [16768/50000]\tLoss: 1.2633\tLR: 0.010000\n",
            "Training Epoch: 14 [16896/50000]\tLoss: 1.0235\tLR: 0.010000\n",
            "Training Epoch: 14 [17024/50000]\tLoss: 1.0303\tLR: 0.010000\n",
            "Training Epoch: 14 [17152/50000]\tLoss: 1.0737\tLR: 0.010000\n",
            "Training Epoch: 14 [17280/50000]\tLoss: 1.1467\tLR: 0.010000\n",
            "Training Epoch: 14 [17408/50000]\tLoss: 1.1663\tLR: 0.010000\n",
            "Training Epoch: 14 [17536/50000]\tLoss: 1.0393\tLR: 0.010000\n",
            "Training Epoch: 14 [17664/50000]\tLoss: 1.1340\tLR: 0.010000\n",
            "Training Epoch: 14 [17792/50000]\tLoss: 1.0610\tLR: 0.010000\n",
            "Training Epoch: 14 [17920/50000]\tLoss: 1.0980\tLR: 0.010000\n",
            "Training Epoch: 14 [18048/50000]\tLoss: 0.9962\tLR: 0.010000\n",
            "Training Epoch: 14 [18176/50000]\tLoss: 1.0768\tLR: 0.010000\n",
            "Training Epoch: 14 [18304/50000]\tLoss: 1.1285\tLR: 0.010000\n",
            "Training Epoch: 14 [18432/50000]\tLoss: 1.0894\tLR: 0.010000\n",
            "Training Epoch: 14 [18560/50000]\tLoss: 0.9363\tLR: 0.010000\n",
            "Training Epoch: 14 [18688/50000]\tLoss: 0.9181\tLR: 0.010000\n",
            "Training Epoch: 14 [18816/50000]\tLoss: 1.0372\tLR: 0.010000\n",
            "Training Epoch: 14 [18944/50000]\tLoss: 1.0790\tLR: 0.010000\n",
            "Training Epoch: 14 [19072/50000]\tLoss: 0.9138\tLR: 0.010000\n",
            "Training Epoch: 14 [19200/50000]\tLoss: 1.0838\tLR: 0.010000\n",
            "Training Epoch: 14 [19328/50000]\tLoss: 0.9004\tLR: 0.010000\n",
            "Training Epoch: 14 [19456/50000]\tLoss: 1.1197\tLR: 0.010000\n",
            "Training Epoch: 14 [19584/50000]\tLoss: 1.1018\tLR: 0.010000\n",
            "Training Epoch: 14 [19712/50000]\tLoss: 1.1323\tLR: 0.010000\n",
            "Training Epoch: 14 [19840/50000]\tLoss: 1.2287\tLR: 0.010000\n",
            "Training Epoch: 14 [19968/50000]\tLoss: 0.9511\tLR: 0.010000\n",
            "Training Epoch: 14 [20096/50000]\tLoss: 1.0236\tLR: 0.010000\n",
            "Training Epoch: 14 [20224/50000]\tLoss: 1.1183\tLR: 0.010000\n",
            "Training Epoch: 14 [20352/50000]\tLoss: 1.3034\tLR: 0.010000\n",
            "Training Epoch: 14 [20480/50000]\tLoss: 1.0172\tLR: 0.010000\n",
            "Training Epoch: 14 [20608/50000]\tLoss: 1.0606\tLR: 0.010000\n",
            "Training Epoch: 14 [20736/50000]\tLoss: 0.9777\tLR: 0.010000\n",
            "Training Epoch: 14 [20864/50000]\tLoss: 1.1593\tLR: 0.010000\n",
            "Training Epoch: 14 [20992/50000]\tLoss: 0.8659\tLR: 0.010000\n",
            "Training Epoch: 14 [21120/50000]\tLoss: 1.1085\tLR: 0.010000\n",
            "Training Epoch: 14 [21248/50000]\tLoss: 1.0591\tLR: 0.010000\n",
            "Training Epoch: 14 [21376/50000]\tLoss: 0.9978\tLR: 0.010000\n",
            "Training Epoch: 14 [21504/50000]\tLoss: 0.9621\tLR: 0.010000\n",
            "Training Epoch: 14 [21632/50000]\tLoss: 1.1361\tLR: 0.010000\n",
            "Training Epoch: 14 [21760/50000]\tLoss: 0.8326\tLR: 0.010000\n",
            "Training Epoch: 14 [21888/50000]\tLoss: 1.2410\tLR: 0.010000\n",
            "Training Epoch: 14 [22016/50000]\tLoss: 0.9703\tLR: 0.010000\n",
            "Training Epoch: 14 [22144/50000]\tLoss: 1.0267\tLR: 0.010000\n",
            "Training Epoch: 14 [22272/50000]\tLoss: 1.1588\tLR: 0.010000\n",
            "Training Epoch: 14 [22400/50000]\tLoss: 1.0637\tLR: 0.010000\n",
            "Training Epoch: 14 [22528/50000]\tLoss: 1.2076\tLR: 0.010000\n",
            "Training Epoch: 14 [22656/50000]\tLoss: 1.0298\tLR: 0.010000\n",
            "Training Epoch: 14 [22784/50000]\tLoss: 1.0356\tLR: 0.010000\n",
            "Training Epoch: 14 [22912/50000]\tLoss: 1.0505\tLR: 0.010000\n",
            "Training Epoch: 14 [23040/50000]\tLoss: 1.0971\tLR: 0.010000\n",
            "Training Epoch: 14 [23168/50000]\tLoss: 1.0398\tLR: 0.010000\n",
            "Training Epoch: 14 [23296/50000]\tLoss: 1.2715\tLR: 0.010000\n",
            "Training Epoch: 14 [23424/50000]\tLoss: 0.8839\tLR: 0.010000\n",
            "Training Epoch: 14 [23552/50000]\tLoss: 1.1984\tLR: 0.010000\n",
            "Training Epoch: 14 [23680/50000]\tLoss: 0.8062\tLR: 0.010000\n",
            "Training Epoch: 14 [23808/50000]\tLoss: 0.9475\tLR: 0.010000\n",
            "Training Epoch: 14 [23936/50000]\tLoss: 0.8796\tLR: 0.010000\n",
            "Training Epoch: 14 [24064/50000]\tLoss: 0.9577\tLR: 0.010000\n",
            "Training Epoch: 14 [24192/50000]\tLoss: 0.9099\tLR: 0.010000\n",
            "Training Epoch: 14 [24320/50000]\tLoss: 1.0464\tLR: 0.010000\n",
            "Training Epoch: 14 [24448/50000]\tLoss: 0.9838\tLR: 0.010000\n",
            "Training Epoch: 14 [24576/50000]\tLoss: 1.2461\tLR: 0.010000\n",
            "Training Epoch: 14 [24704/50000]\tLoss: 1.1707\tLR: 0.010000\n",
            "Training Epoch: 14 [24832/50000]\tLoss: 0.9912\tLR: 0.010000\n",
            "Training Epoch: 14 [24960/50000]\tLoss: 1.0068\tLR: 0.010000\n",
            "Training Epoch: 14 [25088/50000]\tLoss: 1.0580\tLR: 0.010000\n",
            "Training Epoch: 14 [25216/50000]\tLoss: 0.8387\tLR: 0.010000\n",
            "Training Epoch: 14 [25344/50000]\tLoss: 1.2690\tLR: 0.010000\n",
            "Training Epoch: 14 [25472/50000]\tLoss: 1.1629\tLR: 0.010000\n",
            "Training Epoch: 14 [25600/50000]\tLoss: 0.9772\tLR: 0.010000\n",
            "Training Epoch: 14 [25728/50000]\tLoss: 0.8116\tLR: 0.010000\n",
            "Training Epoch: 14 [25856/50000]\tLoss: 1.0481\tLR: 0.010000\n",
            "Training Epoch: 14 [25984/50000]\tLoss: 1.2416\tLR: 0.010000\n",
            "Training Epoch: 14 [26112/50000]\tLoss: 1.1754\tLR: 0.010000\n",
            "Training Epoch: 14 [26240/50000]\tLoss: 1.2302\tLR: 0.010000\n",
            "Training Epoch: 14 [26368/50000]\tLoss: 1.0735\tLR: 0.010000\n",
            "Training Epoch: 14 [26496/50000]\tLoss: 0.8872\tLR: 0.010000\n",
            "Training Epoch: 14 [26624/50000]\tLoss: 1.2170\tLR: 0.010000\n",
            "Training Epoch: 14 [26752/50000]\tLoss: 1.1412\tLR: 0.010000\n",
            "Training Epoch: 14 [26880/50000]\tLoss: 1.0308\tLR: 0.010000\n",
            "Training Epoch: 14 [27008/50000]\tLoss: 1.0390\tLR: 0.010000\n",
            "Training Epoch: 14 [27136/50000]\tLoss: 0.9393\tLR: 0.010000\n",
            "Training Epoch: 14 [27264/50000]\tLoss: 0.9265\tLR: 0.010000\n",
            "Training Epoch: 14 [27392/50000]\tLoss: 0.9534\tLR: 0.010000\n",
            "Training Epoch: 14 [27520/50000]\tLoss: 1.0721\tLR: 0.010000\n",
            "Training Epoch: 14 [27648/50000]\tLoss: 0.9704\tLR: 0.010000\n",
            "Training Epoch: 14 [27776/50000]\tLoss: 1.1264\tLR: 0.010000\n",
            "Training Epoch: 14 [27904/50000]\tLoss: 0.9776\tLR: 0.010000\n",
            "Training Epoch: 14 [28032/50000]\tLoss: 0.9880\tLR: 0.010000\n",
            "Training Epoch: 14 [28160/50000]\tLoss: 0.9174\tLR: 0.010000\n",
            "Training Epoch: 14 [28288/50000]\tLoss: 0.9005\tLR: 0.010000\n",
            "Training Epoch: 14 [28416/50000]\tLoss: 1.2714\tLR: 0.010000\n",
            "Training Epoch: 14 [28544/50000]\tLoss: 1.1394\tLR: 0.010000\n",
            "Training Epoch: 14 [28672/50000]\tLoss: 0.8936\tLR: 0.010000\n",
            "Training Epoch: 14 [28800/50000]\tLoss: 1.1279\tLR: 0.010000\n",
            "Training Epoch: 14 [28928/50000]\tLoss: 0.9261\tLR: 0.010000\n",
            "Training Epoch: 14 [29056/50000]\tLoss: 1.1110\tLR: 0.010000\n",
            "Training Epoch: 14 [29184/50000]\tLoss: 1.1167\tLR: 0.010000\n",
            "Training Epoch: 14 [29312/50000]\tLoss: 0.8407\tLR: 0.010000\n",
            "Training Epoch: 14 [29440/50000]\tLoss: 0.9133\tLR: 0.010000\n",
            "Training Epoch: 14 [29568/50000]\tLoss: 0.9705\tLR: 0.010000\n",
            "Training Epoch: 14 [29696/50000]\tLoss: 1.0372\tLR: 0.010000\n",
            "Training Epoch: 14 [29824/50000]\tLoss: 0.9915\tLR: 0.010000\n",
            "Training Epoch: 14 [29952/50000]\tLoss: 0.7602\tLR: 0.010000\n",
            "Training Epoch: 14 [30080/50000]\tLoss: 1.1452\tLR: 0.010000\n",
            "Training Epoch: 14 [30208/50000]\tLoss: 1.0762\tLR: 0.010000\n",
            "Training Epoch: 14 [30336/50000]\tLoss: 0.9982\tLR: 0.010000\n",
            "Training Epoch: 14 [30464/50000]\tLoss: 1.1208\tLR: 0.010000\n",
            "Training Epoch: 14 [30592/50000]\tLoss: 0.7516\tLR: 0.010000\n",
            "Training Epoch: 14 [30720/50000]\tLoss: 1.1329\tLR: 0.010000\n",
            "Training Epoch: 14 [30848/50000]\tLoss: 1.0973\tLR: 0.010000\n",
            "Training Epoch: 14 [30976/50000]\tLoss: 1.1408\tLR: 0.010000\n",
            "Training Epoch: 14 [31104/50000]\tLoss: 1.1693\tLR: 0.010000\n",
            "Training Epoch: 14 [31232/50000]\tLoss: 1.0222\tLR: 0.010000\n",
            "Training Epoch: 14 [31360/50000]\tLoss: 0.9025\tLR: 0.010000\n",
            "Training Epoch: 14 [31488/50000]\tLoss: 1.1584\tLR: 0.010000\n",
            "Training Epoch: 14 [31616/50000]\tLoss: 1.0705\tLR: 0.010000\n",
            "Training Epoch: 14 [31744/50000]\tLoss: 0.9837\tLR: 0.010000\n",
            "Training Epoch: 14 [31872/50000]\tLoss: 1.0729\tLR: 0.010000\n",
            "Training Epoch: 14 [32000/50000]\tLoss: 1.1075\tLR: 0.010000\n",
            "Training Epoch: 14 [32128/50000]\tLoss: 1.0559\tLR: 0.010000\n",
            "Training Epoch: 14 [32256/50000]\tLoss: 1.4317\tLR: 0.010000\n",
            "Training Epoch: 14 [32384/50000]\tLoss: 0.9051\tLR: 0.010000\n",
            "Training Epoch: 14 [32512/50000]\tLoss: 1.0415\tLR: 0.010000\n",
            "Training Epoch: 14 [32640/50000]\tLoss: 1.1052\tLR: 0.010000\n",
            "Training Epoch: 14 [32768/50000]\tLoss: 0.9059\tLR: 0.010000\n",
            "Training Epoch: 14 [32896/50000]\tLoss: 1.1597\tLR: 0.010000\n",
            "Training Epoch: 14 [33024/50000]\tLoss: 1.2955\tLR: 0.010000\n",
            "Training Epoch: 14 [33152/50000]\tLoss: 0.7646\tLR: 0.010000\n",
            "Training Epoch: 14 [33280/50000]\tLoss: 1.0593\tLR: 0.010000\n",
            "Training Epoch: 14 [33408/50000]\tLoss: 0.9892\tLR: 0.010000\n",
            "Training Epoch: 14 [33536/50000]\tLoss: 1.2734\tLR: 0.010000\n",
            "Training Epoch: 14 [33664/50000]\tLoss: 1.1644\tLR: 0.010000\n",
            "Training Epoch: 14 [33792/50000]\tLoss: 1.0896\tLR: 0.010000\n",
            "Training Epoch: 14 [33920/50000]\tLoss: 0.9722\tLR: 0.010000\n",
            "Training Epoch: 14 [34048/50000]\tLoss: 1.2141\tLR: 0.010000\n",
            "Training Epoch: 14 [34176/50000]\tLoss: 1.0737\tLR: 0.010000\n",
            "Training Epoch: 14 [34304/50000]\tLoss: 1.0407\tLR: 0.010000\n",
            "Training Epoch: 14 [34432/50000]\tLoss: 1.1263\tLR: 0.010000\n",
            "Training Epoch: 14 [34560/50000]\tLoss: 0.9540\tLR: 0.010000\n",
            "Training Epoch: 14 [34688/50000]\tLoss: 1.1694\tLR: 0.010000\n",
            "Training Epoch: 14 [34816/50000]\tLoss: 1.1244\tLR: 0.010000\n",
            "Training Epoch: 14 [34944/50000]\tLoss: 1.0291\tLR: 0.010000\n",
            "Training Epoch: 14 [35072/50000]\tLoss: 1.1520\tLR: 0.010000\n",
            "Training Epoch: 14 [35200/50000]\tLoss: 0.9486\tLR: 0.010000\n",
            "Training Epoch: 14 [35328/50000]\tLoss: 0.9455\tLR: 0.010000\n",
            "Training Epoch: 14 [35456/50000]\tLoss: 1.1946\tLR: 0.010000\n",
            "Training Epoch: 14 [35584/50000]\tLoss: 0.9705\tLR: 0.010000\n",
            "Training Epoch: 14 [35712/50000]\tLoss: 1.1168\tLR: 0.010000\n",
            "Training Epoch: 14 [35840/50000]\tLoss: 1.0370\tLR: 0.010000\n",
            "Training Epoch: 14 [35968/50000]\tLoss: 1.1805\tLR: 0.010000\n",
            "Training Epoch: 14 [36096/50000]\tLoss: 1.1038\tLR: 0.010000\n",
            "Training Epoch: 14 [36224/50000]\tLoss: 1.0220\tLR: 0.010000\n",
            "Training Epoch: 14 [36352/50000]\tLoss: 1.1916\tLR: 0.010000\n",
            "Training Epoch: 14 [36480/50000]\tLoss: 0.9224\tLR: 0.010000\n",
            "Training Epoch: 14 [36608/50000]\tLoss: 1.2516\tLR: 0.010000\n",
            "Training Epoch: 14 [36736/50000]\tLoss: 1.0196\tLR: 0.010000\n",
            "Training Epoch: 14 [36864/50000]\tLoss: 1.1612\tLR: 0.010000\n",
            "Training Epoch: 14 [36992/50000]\tLoss: 1.1187\tLR: 0.010000\n",
            "Training Epoch: 14 [37120/50000]\tLoss: 1.0609\tLR: 0.010000\n",
            "Training Epoch: 14 [37248/50000]\tLoss: 1.2449\tLR: 0.010000\n",
            "Training Epoch: 14 [37376/50000]\tLoss: 1.0992\tLR: 0.010000\n",
            "Training Epoch: 14 [37504/50000]\tLoss: 1.4483\tLR: 0.010000\n",
            "Training Epoch: 14 [37632/50000]\tLoss: 1.2751\tLR: 0.010000\n",
            "Training Epoch: 14 [37760/50000]\tLoss: 0.8567\tLR: 0.010000\n",
            "Training Epoch: 14 [37888/50000]\tLoss: 0.9393\tLR: 0.010000\n",
            "Training Epoch: 14 [38016/50000]\tLoss: 1.2069\tLR: 0.010000\n",
            "Training Epoch: 14 [38144/50000]\tLoss: 1.0728\tLR: 0.010000\n",
            "Training Epoch: 14 [38272/50000]\tLoss: 1.1177\tLR: 0.010000\n",
            "Training Epoch: 14 [38400/50000]\tLoss: 1.0133\tLR: 0.010000\n",
            "Training Epoch: 14 [38528/50000]\tLoss: 1.0474\tLR: 0.010000\n",
            "Training Epoch: 14 [38656/50000]\tLoss: 1.2505\tLR: 0.010000\n",
            "Training Epoch: 14 [38784/50000]\tLoss: 0.9110\tLR: 0.010000\n",
            "Training Epoch: 14 [38912/50000]\tLoss: 0.9547\tLR: 0.010000\n",
            "Training Epoch: 14 [39040/50000]\tLoss: 1.3187\tLR: 0.010000\n",
            "Training Epoch: 14 [39168/50000]\tLoss: 0.9199\tLR: 0.010000\n",
            "Training Epoch: 14 [39296/50000]\tLoss: 1.1998\tLR: 0.010000\n",
            "Training Epoch: 14 [39424/50000]\tLoss: 1.0574\tLR: 0.010000\n",
            "Training Epoch: 14 [39552/50000]\tLoss: 1.2137\tLR: 0.010000\n",
            "Training Epoch: 14 [39680/50000]\tLoss: 1.2277\tLR: 0.010000\n",
            "Training Epoch: 14 [39808/50000]\tLoss: 1.0995\tLR: 0.010000\n",
            "Training Epoch: 14 [39936/50000]\tLoss: 1.0554\tLR: 0.010000\n",
            "Training Epoch: 14 [40064/50000]\tLoss: 1.0306\tLR: 0.010000\n",
            "Training Epoch: 14 [40192/50000]\tLoss: 1.2197\tLR: 0.010000\n",
            "Training Epoch: 14 [40320/50000]\tLoss: 1.0130\tLR: 0.010000\n",
            "Training Epoch: 14 [40448/50000]\tLoss: 0.9124\tLR: 0.010000\n",
            "Training Epoch: 14 [40576/50000]\tLoss: 1.0609\tLR: 0.010000\n",
            "Training Epoch: 14 [40704/50000]\tLoss: 1.1609\tLR: 0.010000\n",
            "Training Epoch: 14 [40832/50000]\tLoss: 0.9745\tLR: 0.010000\n",
            "Training Epoch: 14 [40960/50000]\tLoss: 0.9005\tLR: 0.010000\n",
            "Training Epoch: 14 [41088/50000]\tLoss: 1.1258\tLR: 0.010000\n",
            "Training Epoch: 14 [41216/50000]\tLoss: 1.2010\tLR: 0.010000\n",
            "Training Epoch: 14 [41344/50000]\tLoss: 1.3137\tLR: 0.010000\n",
            "Training Epoch: 14 [41472/50000]\tLoss: 1.0858\tLR: 0.010000\n",
            "Training Epoch: 14 [41600/50000]\tLoss: 1.2176\tLR: 0.010000\n",
            "Training Epoch: 14 [41728/50000]\tLoss: 1.0988\tLR: 0.010000\n",
            "Training Epoch: 14 [41856/50000]\tLoss: 1.0306\tLR: 0.010000\n",
            "Training Epoch: 14 [41984/50000]\tLoss: 0.9704\tLR: 0.010000\n",
            "Training Epoch: 14 [42112/50000]\tLoss: 1.1335\tLR: 0.010000\n",
            "Training Epoch: 14 [42240/50000]\tLoss: 1.1406\tLR: 0.010000\n",
            "Training Epoch: 14 [42368/50000]\tLoss: 1.0481\tLR: 0.010000\n",
            "Training Epoch: 14 [42496/50000]\tLoss: 1.0498\tLR: 0.010000\n",
            "Training Epoch: 14 [42624/50000]\tLoss: 1.0608\tLR: 0.010000\n",
            "Training Epoch: 14 [42752/50000]\tLoss: 1.0899\tLR: 0.010000\n",
            "Training Epoch: 14 [42880/50000]\tLoss: 1.0663\tLR: 0.010000\n",
            "Training Epoch: 14 [43008/50000]\tLoss: 1.0992\tLR: 0.010000\n",
            "Training Epoch: 14 [43136/50000]\tLoss: 1.1692\tLR: 0.010000\n",
            "Training Epoch: 14 [43264/50000]\tLoss: 0.8057\tLR: 0.010000\n",
            "Training Epoch: 14 [43392/50000]\tLoss: 1.1030\tLR: 0.010000\n",
            "Training Epoch: 14 [43520/50000]\tLoss: 1.1517\tLR: 0.010000\n",
            "Training Epoch: 14 [43648/50000]\tLoss: 1.1412\tLR: 0.010000\n",
            "Training Epoch: 14 [43776/50000]\tLoss: 1.1903\tLR: 0.010000\n",
            "Training Epoch: 14 [43904/50000]\tLoss: 1.2633\tLR: 0.010000\n",
            "Training Epoch: 14 [44032/50000]\tLoss: 1.2497\tLR: 0.010000\n",
            "Training Epoch: 14 [44160/50000]\tLoss: 1.1291\tLR: 0.010000\n",
            "Training Epoch: 14 [44288/50000]\tLoss: 1.1831\tLR: 0.010000\n",
            "Training Epoch: 14 [44416/50000]\tLoss: 1.1159\tLR: 0.010000\n",
            "Training Epoch: 14 [44544/50000]\tLoss: 1.0321\tLR: 0.010000\n",
            "Training Epoch: 14 [44672/50000]\tLoss: 1.1470\tLR: 0.010000\n",
            "Training Epoch: 14 [44800/50000]\tLoss: 1.1875\tLR: 0.010000\n",
            "Training Epoch: 14 [44928/50000]\tLoss: 1.2050\tLR: 0.010000\n",
            "Training Epoch: 14 [45056/50000]\tLoss: 1.0580\tLR: 0.010000\n",
            "Training Epoch: 14 [45184/50000]\tLoss: 1.0849\tLR: 0.010000\n",
            "Training Epoch: 14 [45312/50000]\tLoss: 1.0473\tLR: 0.010000\n",
            "Training Epoch: 14 [45440/50000]\tLoss: 0.8789\tLR: 0.010000\n",
            "Training Epoch: 14 [45568/50000]\tLoss: 0.9897\tLR: 0.010000\n",
            "Training Epoch: 14 [45696/50000]\tLoss: 1.0093\tLR: 0.010000\n",
            "Training Epoch: 14 [45824/50000]\tLoss: 1.1736\tLR: 0.010000\n",
            "Training Epoch: 14 [45952/50000]\tLoss: 0.9506\tLR: 0.010000\n",
            "Training Epoch: 14 [46080/50000]\tLoss: 1.0640\tLR: 0.010000\n",
            "Training Epoch: 14 [46208/50000]\tLoss: 1.2560\tLR: 0.010000\n",
            "Training Epoch: 14 [46336/50000]\tLoss: 1.0755\tLR: 0.010000\n",
            "Training Epoch: 14 [46464/50000]\tLoss: 1.3970\tLR: 0.010000\n",
            "Training Epoch: 14 [46592/50000]\tLoss: 1.0275\tLR: 0.010000\n",
            "Training Epoch: 14 [46720/50000]\tLoss: 0.9855\tLR: 0.010000\n",
            "Training Epoch: 14 [46848/50000]\tLoss: 1.2461\tLR: 0.010000\n",
            "Training Epoch: 14 [46976/50000]\tLoss: 1.0969\tLR: 0.010000\n",
            "Training Epoch: 14 [47104/50000]\tLoss: 0.9815\tLR: 0.010000\n",
            "Training Epoch: 14 [47232/50000]\tLoss: 1.1895\tLR: 0.010000\n",
            "Training Epoch: 14 [47360/50000]\tLoss: 1.2028\tLR: 0.010000\n",
            "Training Epoch: 14 [47488/50000]\tLoss: 1.1858\tLR: 0.010000\n",
            "Training Epoch: 14 [47616/50000]\tLoss: 0.9377\tLR: 0.010000\n",
            "Training Epoch: 14 [47744/50000]\tLoss: 1.0459\tLR: 0.010000\n",
            "Training Epoch: 14 [47872/50000]\tLoss: 1.0439\tLR: 0.010000\n",
            "Training Epoch: 14 [48000/50000]\tLoss: 1.1114\tLR: 0.010000\n",
            "Training Epoch: 14 [48128/50000]\tLoss: 1.2022\tLR: 0.010000\n",
            "Training Epoch: 14 [48256/50000]\tLoss: 1.0890\tLR: 0.010000\n",
            "Training Epoch: 14 [48384/50000]\tLoss: 0.9524\tLR: 0.010000\n",
            "Training Epoch: 14 [48512/50000]\tLoss: 0.7987\tLR: 0.010000\n",
            "Training Epoch: 14 [48640/50000]\tLoss: 1.0245\tLR: 0.010000\n",
            "Training Epoch: 14 [48768/50000]\tLoss: 1.0497\tLR: 0.010000\n",
            "Training Epoch: 14 [48896/50000]\tLoss: 1.0709\tLR: 0.010000\n",
            "Training Epoch: 14 [49024/50000]\tLoss: 1.3834\tLR: 0.010000\n",
            "Training Epoch: 14 [49152/50000]\tLoss: 1.1112\tLR: 0.010000\n",
            "Training Epoch: 14 [49280/50000]\tLoss: 1.1797\tLR: 0.010000\n",
            "Training Epoch: 14 [49408/50000]\tLoss: 1.1382\tLR: 0.010000\n",
            "Training Epoch: 14 [49536/50000]\tLoss: 1.1320\tLR: 0.010000\n",
            "Training Epoch: 14 [49664/50000]\tLoss: 1.2317\tLR: 0.010000\n",
            "Training Epoch: 14 [49792/50000]\tLoss: 0.9612\tLR: 0.010000\n",
            "Training Epoch: 14 [49920/50000]\tLoss: 1.1401\tLR: 0.010000\n",
            "Training Epoch: 14 [50000/50000]\tLoss: 1.1360\tLR: 0.010000\n",
            "epoch 14 training time consumed: 207.04s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 117604 GiB | 117603 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 117518 GiB | 117518 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     85 GiB |     85 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 117604 GiB | 117603 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 117518 GiB | 117518 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     85 GiB |     85 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 117592 GiB | 117591 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 117507 GiB | 117506 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     84 GiB |     84 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  85403 GiB |  85402 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  85315 GiB |  85315 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     87 GiB |     87 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    4669 K  |    4668 K  |\n",
            "|       from large pool |      92    |     184    |    2632 K  |    2632 K  |\n",
            "|       from small pool |     519    |     646    |    2036 K  |    2036 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    4669 K  |    4668 K  |\n",
            "|       from large pool |      92    |     184    |    2632 K  |    2632 K  |\n",
            "|       from small pool |     519    |     646    |    2036 K  |    2036 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |     122    |    1970 K  |    1970 K  |\n",
            "|       from large pool |      42    |     111    |    1554 K  |    1554 K  |\n",
            "|       from small pool |       9    |      21    |     416 K  |     416 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0110, Accuracy: 0.6181, Time consumed:11.24s\n",
            "\n",
            "Training Epoch: 15 [128/50000]\tLoss: 0.8656\tLR: 0.010000\n",
            "Training Epoch: 15 [256/50000]\tLoss: 0.8432\tLR: 0.010000\n",
            "Training Epoch: 15 [384/50000]\tLoss: 1.1059\tLR: 0.010000\n",
            "Training Epoch: 15 [512/50000]\tLoss: 1.0873\tLR: 0.010000\n",
            "Training Epoch: 15 [640/50000]\tLoss: 0.8909\tLR: 0.010000\n",
            "Training Epoch: 15 [768/50000]\tLoss: 0.9197\tLR: 0.010000\n",
            "Training Epoch: 15 [896/50000]\tLoss: 0.9383\tLR: 0.010000\n",
            "Training Epoch: 15 [1024/50000]\tLoss: 1.1936\tLR: 0.010000\n",
            "Training Epoch: 15 [1152/50000]\tLoss: 1.0191\tLR: 0.010000\n",
            "Training Epoch: 15 [1280/50000]\tLoss: 1.0522\tLR: 0.010000\n",
            "Training Epoch: 15 [1408/50000]\tLoss: 0.7290\tLR: 0.010000\n",
            "Training Epoch: 15 [1536/50000]\tLoss: 1.0261\tLR: 0.010000\n",
            "Training Epoch: 15 [1664/50000]\tLoss: 0.9568\tLR: 0.010000\n",
            "Training Epoch: 15 [1792/50000]\tLoss: 0.8791\tLR: 0.010000\n",
            "Training Epoch: 15 [1920/50000]\tLoss: 1.0565\tLR: 0.010000\n",
            "Training Epoch: 15 [2048/50000]\tLoss: 0.8693\tLR: 0.010000\n",
            "Training Epoch: 15 [2176/50000]\tLoss: 0.8733\tLR: 0.010000\n",
            "Training Epoch: 15 [2304/50000]\tLoss: 0.7630\tLR: 0.010000\n",
            "Training Epoch: 15 [2432/50000]\tLoss: 1.0303\tLR: 0.010000\n",
            "Training Epoch: 15 [2560/50000]\tLoss: 0.8941\tLR: 0.010000\n",
            "Training Epoch: 15 [2688/50000]\tLoss: 0.9427\tLR: 0.010000\n",
            "Training Epoch: 15 [2816/50000]\tLoss: 0.9399\tLR: 0.010000\n",
            "Training Epoch: 15 [2944/50000]\tLoss: 0.9645\tLR: 0.010000\n",
            "Training Epoch: 15 [3072/50000]\tLoss: 1.1097\tLR: 0.010000\n",
            "Training Epoch: 15 [3200/50000]\tLoss: 0.8505\tLR: 0.010000\n",
            "Training Epoch: 15 [3328/50000]\tLoss: 1.1063\tLR: 0.010000\n",
            "Training Epoch: 15 [3456/50000]\tLoss: 0.9244\tLR: 0.010000\n",
            "Training Epoch: 15 [3584/50000]\tLoss: 1.0665\tLR: 0.010000\n",
            "Training Epoch: 15 [3712/50000]\tLoss: 0.8927\tLR: 0.010000\n",
            "Training Epoch: 15 [3840/50000]\tLoss: 0.7945\tLR: 0.010000\n",
            "Training Epoch: 15 [3968/50000]\tLoss: 0.8030\tLR: 0.010000\n",
            "Training Epoch: 15 [4096/50000]\tLoss: 0.9118\tLR: 0.010000\n",
            "Training Epoch: 15 [4224/50000]\tLoss: 1.0366\tLR: 0.010000\n",
            "Training Epoch: 15 [4352/50000]\tLoss: 0.7871\tLR: 0.010000\n",
            "Training Epoch: 15 [4480/50000]\tLoss: 0.7458\tLR: 0.010000\n",
            "Training Epoch: 15 [4608/50000]\tLoss: 1.0839\tLR: 0.010000\n",
            "Training Epoch: 15 [4736/50000]\tLoss: 0.9850\tLR: 0.010000\n",
            "Training Epoch: 15 [4864/50000]\tLoss: 1.0467\tLR: 0.010000\n",
            "Training Epoch: 15 [4992/50000]\tLoss: 0.9847\tLR: 0.010000\n",
            "Training Epoch: 15 [5120/50000]\tLoss: 0.8443\tLR: 0.010000\n",
            "Training Epoch: 15 [5248/50000]\tLoss: 1.0989\tLR: 0.010000\n",
            "Training Epoch: 15 [5376/50000]\tLoss: 0.8720\tLR: 0.010000\n",
            "Training Epoch: 15 [5504/50000]\tLoss: 1.0273\tLR: 0.010000\n",
            "Training Epoch: 15 [5632/50000]\tLoss: 0.8283\tLR: 0.010000\n",
            "Training Epoch: 15 [5760/50000]\tLoss: 0.9948\tLR: 0.010000\n",
            "Training Epoch: 15 [5888/50000]\tLoss: 1.0634\tLR: 0.010000\n",
            "Training Epoch: 15 [6016/50000]\tLoss: 0.7809\tLR: 0.010000\n",
            "Training Epoch: 15 [6144/50000]\tLoss: 0.8790\tLR: 0.010000\n",
            "Training Epoch: 15 [6272/50000]\tLoss: 0.9616\tLR: 0.010000\n",
            "Training Epoch: 15 [6400/50000]\tLoss: 0.9948\tLR: 0.010000\n",
            "Training Epoch: 15 [6528/50000]\tLoss: 1.0174\tLR: 0.010000\n",
            "Training Epoch: 15 [6656/50000]\tLoss: 0.9374\tLR: 0.010000\n",
            "Training Epoch: 15 [6784/50000]\tLoss: 1.1139\tLR: 0.010000\n",
            "Training Epoch: 15 [6912/50000]\tLoss: 1.0764\tLR: 0.010000\n",
            "Training Epoch: 15 [7040/50000]\tLoss: 1.0772\tLR: 0.010000\n",
            "Training Epoch: 15 [7168/50000]\tLoss: 0.8519\tLR: 0.010000\n",
            "Training Epoch: 15 [7296/50000]\tLoss: 0.9359\tLR: 0.010000\n",
            "Training Epoch: 15 [7424/50000]\tLoss: 0.8018\tLR: 0.010000\n",
            "Training Epoch: 15 [7552/50000]\tLoss: 1.0128\tLR: 0.010000\n",
            "Training Epoch: 15 [7680/50000]\tLoss: 0.8990\tLR: 0.010000\n",
            "Training Epoch: 15 [7808/50000]\tLoss: 1.2719\tLR: 0.010000\n",
            "Training Epoch: 15 [7936/50000]\tLoss: 1.0500\tLR: 0.010000\n",
            "Training Epoch: 15 [8064/50000]\tLoss: 1.0997\tLR: 0.010000\n",
            "Training Epoch: 15 [8192/50000]\tLoss: 0.7703\tLR: 0.010000\n",
            "Training Epoch: 15 [8320/50000]\tLoss: 0.7681\tLR: 0.010000\n",
            "Training Epoch: 15 [8448/50000]\tLoss: 0.8772\tLR: 0.010000\n",
            "Training Epoch: 15 [8576/50000]\tLoss: 0.8377\tLR: 0.010000\n",
            "Training Epoch: 15 [8704/50000]\tLoss: 0.8797\tLR: 0.010000\n",
            "Training Epoch: 15 [8832/50000]\tLoss: 0.8425\tLR: 0.010000\n",
            "Training Epoch: 15 [8960/50000]\tLoss: 1.0508\tLR: 0.010000\n",
            "Training Epoch: 15 [9088/50000]\tLoss: 0.9174\tLR: 0.010000\n",
            "Training Epoch: 15 [9216/50000]\tLoss: 0.9739\tLR: 0.010000\n",
            "Training Epoch: 15 [9344/50000]\tLoss: 0.9612\tLR: 0.010000\n",
            "Training Epoch: 15 [9472/50000]\tLoss: 1.1001\tLR: 0.010000\n",
            "Training Epoch: 15 [9600/50000]\tLoss: 1.1167\tLR: 0.010000\n",
            "Training Epoch: 15 [9728/50000]\tLoss: 0.9129\tLR: 0.010000\n",
            "Training Epoch: 15 [9856/50000]\tLoss: 0.9426\tLR: 0.010000\n",
            "Training Epoch: 15 [9984/50000]\tLoss: 0.9199\tLR: 0.010000\n",
            "Training Epoch: 15 [10112/50000]\tLoss: 0.9530\tLR: 0.010000\n",
            "Training Epoch: 15 [10240/50000]\tLoss: 0.8961\tLR: 0.010000\n",
            "Training Epoch: 15 [10368/50000]\tLoss: 0.7682\tLR: 0.010000\n",
            "Training Epoch: 15 [10496/50000]\tLoss: 0.7584\tLR: 0.010000\n",
            "Training Epoch: 15 [10624/50000]\tLoss: 0.8874\tLR: 0.010000\n",
            "Training Epoch: 15 [10752/50000]\tLoss: 0.9944\tLR: 0.010000\n",
            "Training Epoch: 15 [10880/50000]\tLoss: 0.9114\tLR: 0.010000\n",
            "Training Epoch: 15 [11008/50000]\tLoss: 0.7414\tLR: 0.010000\n",
            "Training Epoch: 15 [11136/50000]\tLoss: 1.0353\tLR: 0.010000\n",
            "Training Epoch: 15 [11264/50000]\tLoss: 1.0591\tLR: 0.010000\n",
            "Training Epoch: 15 [11392/50000]\tLoss: 1.1425\tLR: 0.010000\n",
            "Training Epoch: 15 [11520/50000]\tLoss: 0.7279\tLR: 0.010000\n",
            "Training Epoch: 15 [11648/50000]\tLoss: 0.7600\tLR: 0.010000\n",
            "Training Epoch: 15 [11776/50000]\tLoss: 0.8322\tLR: 0.010000\n",
            "Training Epoch: 15 [11904/50000]\tLoss: 0.8601\tLR: 0.010000\n",
            "Training Epoch: 15 [12032/50000]\tLoss: 1.1390\tLR: 0.010000\n",
            "Training Epoch: 15 [12160/50000]\tLoss: 0.9333\tLR: 0.010000\n",
            "Training Epoch: 15 [12288/50000]\tLoss: 0.6961\tLR: 0.010000\n",
            "Training Epoch: 15 [12416/50000]\tLoss: 0.7267\tLR: 0.010000\n",
            "Training Epoch: 15 [12544/50000]\tLoss: 0.7754\tLR: 0.010000\n",
            "Training Epoch: 15 [12672/50000]\tLoss: 1.0569\tLR: 0.010000\n",
            "Training Epoch: 15 [12800/50000]\tLoss: 1.0909\tLR: 0.010000\n",
            "Training Epoch: 15 [12928/50000]\tLoss: 0.9470\tLR: 0.010000\n",
            "Training Epoch: 15 [13056/50000]\tLoss: 1.2397\tLR: 0.010000\n",
            "Training Epoch: 15 [13184/50000]\tLoss: 1.0356\tLR: 0.010000\n",
            "Training Epoch: 15 [13312/50000]\tLoss: 1.0360\tLR: 0.010000\n",
            "Training Epoch: 15 [13440/50000]\tLoss: 0.8460\tLR: 0.010000\n",
            "Training Epoch: 15 [13568/50000]\tLoss: 0.9784\tLR: 0.010000\n",
            "Training Epoch: 15 [13696/50000]\tLoss: 0.9731\tLR: 0.010000\n",
            "Training Epoch: 15 [13824/50000]\tLoss: 0.9329\tLR: 0.010000\n",
            "Training Epoch: 15 [13952/50000]\tLoss: 1.1883\tLR: 0.010000\n",
            "Training Epoch: 15 [14080/50000]\tLoss: 1.1642\tLR: 0.010000\n",
            "Training Epoch: 15 [14208/50000]\tLoss: 0.8440\tLR: 0.010000\n",
            "Training Epoch: 15 [14336/50000]\tLoss: 0.9568\tLR: 0.010000\n",
            "Training Epoch: 15 [14464/50000]\tLoss: 0.8603\tLR: 0.010000\n",
            "Training Epoch: 15 [14592/50000]\tLoss: 0.9653\tLR: 0.010000\n",
            "Training Epoch: 15 [14720/50000]\tLoss: 1.1112\tLR: 0.010000\n",
            "Training Epoch: 15 [14848/50000]\tLoss: 0.9142\tLR: 0.010000\n",
            "Training Epoch: 15 [14976/50000]\tLoss: 0.8843\tLR: 0.010000\n",
            "Training Epoch: 15 [15104/50000]\tLoss: 0.8942\tLR: 0.010000\n",
            "Training Epoch: 15 [15232/50000]\tLoss: 0.9983\tLR: 0.010000\n",
            "Training Epoch: 15 [15360/50000]\tLoss: 0.9529\tLR: 0.010000\n",
            "Training Epoch: 15 [15488/50000]\tLoss: 1.1234\tLR: 0.010000\n",
            "Training Epoch: 15 [15616/50000]\tLoss: 1.0059\tLR: 0.010000\n",
            "Training Epoch: 15 [15744/50000]\tLoss: 0.8027\tLR: 0.010000\n",
            "Training Epoch: 15 [15872/50000]\tLoss: 1.0611\tLR: 0.010000\n",
            "Training Epoch: 15 [16000/50000]\tLoss: 1.3013\tLR: 0.010000\n",
            "Training Epoch: 15 [16128/50000]\tLoss: 0.9664\tLR: 0.010000\n",
            "Training Epoch: 15 [16256/50000]\tLoss: 1.0792\tLR: 0.010000\n",
            "Training Epoch: 15 [16384/50000]\tLoss: 0.8424\tLR: 0.010000\n",
            "Training Epoch: 15 [16512/50000]\tLoss: 1.1445\tLR: 0.010000\n",
            "Training Epoch: 15 [16640/50000]\tLoss: 1.0293\tLR: 0.010000\n",
            "Training Epoch: 15 [16768/50000]\tLoss: 1.0729\tLR: 0.010000\n",
            "Training Epoch: 15 [16896/50000]\tLoss: 0.9952\tLR: 0.010000\n",
            "Training Epoch: 15 [17024/50000]\tLoss: 1.2653\tLR: 0.010000\n",
            "Training Epoch: 15 [17152/50000]\tLoss: 1.1197\tLR: 0.010000\n",
            "Training Epoch: 15 [17280/50000]\tLoss: 0.8137\tLR: 0.010000\n",
            "Training Epoch: 15 [17408/50000]\tLoss: 0.9281\tLR: 0.010000\n",
            "Training Epoch: 15 [17536/50000]\tLoss: 1.0837\tLR: 0.010000\n",
            "Training Epoch: 15 [17664/50000]\tLoss: 0.9844\tLR: 0.010000\n",
            "Training Epoch: 15 [17792/50000]\tLoss: 0.9651\tLR: 0.010000\n",
            "Training Epoch: 15 [17920/50000]\tLoss: 0.9662\tLR: 0.010000\n",
            "Training Epoch: 15 [18048/50000]\tLoss: 0.9658\tLR: 0.010000\n",
            "Training Epoch: 15 [18176/50000]\tLoss: 1.1578\tLR: 0.010000\n",
            "Training Epoch: 15 [18304/50000]\tLoss: 0.9779\tLR: 0.010000\n",
            "Training Epoch: 15 [18432/50000]\tLoss: 0.8572\tLR: 0.010000\n",
            "Training Epoch: 15 [18560/50000]\tLoss: 1.2254\tLR: 0.010000\n",
            "Training Epoch: 15 [18688/50000]\tLoss: 0.9388\tLR: 0.010000\n",
            "Training Epoch: 15 [18816/50000]\tLoss: 1.1587\tLR: 0.010000\n",
            "Training Epoch: 15 [18944/50000]\tLoss: 0.9836\tLR: 0.010000\n",
            "Training Epoch: 15 [19072/50000]\tLoss: 1.1972\tLR: 0.010000\n",
            "Training Epoch: 15 [19200/50000]\tLoss: 1.0548\tLR: 0.010000\n",
            "Training Epoch: 15 [19328/50000]\tLoss: 1.0395\tLR: 0.010000\n",
            "Training Epoch: 15 [19456/50000]\tLoss: 0.8971\tLR: 0.010000\n",
            "Training Epoch: 15 [19584/50000]\tLoss: 0.9892\tLR: 0.010000\n",
            "Training Epoch: 15 [19712/50000]\tLoss: 0.8877\tLR: 0.010000\n",
            "Training Epoch: 15 [19840/50000]\tLoss: 0.9915\tLR: 0.010000\n",
            "Training Epoch: 15 [19968/50000]\tLoss: 0.9226\tLR: 0.010000\n",
            "Training Epoch: 15 [20096/50000]\tLoss: 1.0572\tLR: 0.010000\n",
            "Training Epoch: 15 [20224/50000]\tLoss: 0.8574\tLR: 0.010000\n",
            "Training Epoch: 15 [20352/50000]\tLoss: 0.8493\tLR: 0.010000\n",
            "Training Epoch: 15 [20480/50000]\tLoss: 0.9446\tLR: 0.010000\n",
            "Training Epoch: 15 [20608/50000]\tLoss: 0.7553\tLR: 0.010000\n",
            "Training Epoch: 15 [20736/50000]\tLoss: 1.1239\tLR: 0.010000\n",
            "Training Epoch: 15 [20864/50000]\tLoss: 0.8040\tLR: 0.010000\n",
            "Training Epoch: 15 [20992/50000]\tLoss: 1.0545\tLR: 0.010000\n",
            "Training Epoch: 15 [21120/50000]\tLoss: 1.1180\tLR: 0.010000\n",
            "Training Epoch: 15 [21248/50000]\tLoss: 0.8886\tLR: 0.010000\n",
            "Training Epoch: 15 [21376/50000]\tLoss: 1.0277\tLR: 0.010000\n",
            "Training Epoch: 15 [21504/50000]\tLoss: 1.1140\tLR: 0.010000\n",
            "Training Epoch: 15 [21632/50000]\tLoss: 0.9808\tLR: 0.010000\n",
            "Training Epoch: 15 [21760/50000]\tLoss: 1.0461\tLR: 0.010000\n",
            "Training Epoch: 15 [21888/50000]\tLoss: 1.0012\tLR: 0.010000\n",
            "Training Epoch: 15 [22016/50000]\tLoss: 0.9128\tLR: 0.010000\n",
            "Training Epoch: 15 [22144/50000]\tLoss: 1.3272\tLR: 0.010000\n",
            "Training Epoch: 15 [22272/50000]\tLoss: 0.8609\tLR: 0.010000\n",
            "Training Epoch: 15 [22400/50000]\tLoss: 1.0228\tLR: 0.010000\n",
            "Training Epoch: 15 [22528/50000]\tLoss: 1.1232\tLR: 0.010000\n",
            "Training Epoch: 15 [22656/50000]\tLoss: 0.9321\tLR: 0.010000\n",
            "Training Epoch: 15 [22784/50000]\tLoss: 1.1103\tLR: 0.010000\n",
            "Training Epoch: 15 [22912/50000]\tLoss: 1.0144\tLR: 0.010000\n",
            "Training Epoch: 15 [23040/50000]\tLoss: 1.1391\tLR: 0.010000\n",
            "Training Epoch: 15 [23168/50000]\tLoss: 0.9581\tLR: 0.010000\n",
            "Training Epoch: 15 [23296/50000]\tLoss: 1.0133\tLR: 0.010000\n",
            "Training Epoch: 15 [23424/50000]\tLoss: 0.9596\tLR: 0.010000\n",
            "Training Epoch: 15 [23552/50000]\tLoss: 1.0410\tLR: 0.010000\n",
            "Training Epoch: 15 [23680/50000]\tLoss: 0.9493\tLR: 0.010000\n",
            "Training Epoch: 15 [23808/50000]\tLoss: 1.1114\tLR: 0.010000\n",
            "Training Epoch: 15 [23936/50000]\tLoss: 1.0304\tLR: 0.010000\n",
            "Training Epoch: 15 [24064/50000]\tLoss: 0.9183\tLR: 0.010000\n",
            "Training Epoch: 15 [24192/50000]\tLoss: 0.8482\tLR: 0.010000\n",
            "Training Epoch: 15 [24320/50000]\tLoss: 1.0996\tLR: 0.010000\n",
            "Training Epoch: 15 [24448/50000]\tLoss: 1.0827\tLR: 0.010000\n",
            "Training Epoch: 15 [24576/50000]\tLoss: 1.0833\tLR: 0.010000\n",
            "Training Epoch: 15 [24704/50000]\tLoss: 1.0290\tLR: 0.010000\n",
            "Training Epoch: 15 [24832/50000]\tLoss: 1.0295\tLR: 0.010000\n",
            "Training Epoch: 15 [24960/50000]\tLoss: 0.8985\tLR: 0.010000\n",
            "Training Epoch: 15 [25088/50000]\tLoss: 1.0613\tLR: 0.010000\n",
            "Training Epoch: 15 [25216/50000]\tLoss: 0.9138\tLR: 0.010000\n",
            "Training Epoch: 15 [25344/50000]\tLoss: 1.0381\tLR: 0.010000\n",
            "Training Epoch: 15 [25472/50000]\tLoss: 0.9404\tLR: 0.010000\n",
            "Training Epoch: 15 [25600/50000]\tLoss: 1.0075\tLR: 0.010000\n",
            "Training Epoch: 15 [25728/50000]\tLoss: 0.8901\tLR: 0.010000\n",
            "Training Epoch: 15 [25856/50000]\tLoss: 0.8535\tLR: 0.010000\n",
            "Training Epoch: 15 [25984/50000]\tLoss: 1.1199\tLR: 0.010000\n",
            "Training Epoch: 15 [26112/50000]\tLoss: 1.5047\tLR: 0.010000\n",
            "Training Epoch: 15 [26240/50000]\tLoss: 1.1050\tLR: 0.010000\n",
            "Training Epoch: 15 [26368/50000]\tLoss: 0.9407\tLR: 0.010000\n",
            "Training Epoch: 15 [26496/50000]\tLoss: 1.0633\tLR: 0.010000\n",
            "Training Epoch: 15 [26624/50000]\tLoss: 1.3139\tLR: 0.010000\n",
            "Training Epoch: 15 [26752/50000]\tLoss: 0.8323\tLR: 0.010000\n",
            "Training Epoch: 15 [26880/50000]\tLoss: 1.0651\tLR: 0.010000\n",
            "Training Epoch: 15 [27008/50000]\tLoss: 1.0520\tLR: 0.010000\n",
            "Training Epoch: 15 [27136/50000]\tLoss: 1.1411\tLR: 0.010000\n",
            "Training Epoch: 15 [27264/50000]\tLoss: 1.1668\tLR: 0.010000\n",
            "Training Epoch: 15 [27392/50000]\tLoss: 1.0413\tLR: 0.010000\n",
            "Training Epoch: 15 [27520/50000]\tLoss: 1.2697\tLR: 0.010000\n",
            "Training Epoch: 15 [27648/50000]\tLoss: 1.0221\tLR: 0.010000\n",
            "Training Epoch: 15 [27776/50000]\tLoss: 1.0678\tLR: 0.010000\n",
            "Training Epoch: 15 [27904/50000]\tLoss: 1.1486\tLR: 0.010000\n",
            "Training Epoch: 15 [28032/50000]\tLoss: 0.9931\tLR: 0.010000\n",
            "Training Epoch: 15 [28160/50000]\tLoss: 0.9855\tLR: 0.010000\n",
            "Training Epoch: 15 [28288/50000]\tLoss: 1.1474\tLR: 0.010000\n",
            "Training Epoch: 15 [28416/50000]\tLoss: 1.0154\tLR: 0.010000\n",
            "Training Epoch: 15 [28544/50000]\tLoss: 1.1804\tLR: 0.010000\n",
            "Training Epoch: 15 [28672/50000]\tLoss: 0.9217\tLR: 0.010000\n",
            "Training Epoch: 15 [28800/50000]\tLoss: 0.9920\tLR: 0.010000\n",
            "Training Epoch: 15 [28928/50000]\tLoss: 0.9188\tLR: 0.010000\n",
            "Training Epoch: 15 [29056/50000]\tLoss: 1.0332\tLR: 0.010000\n",
            "Training Epoch: 15 [29184/50000]\tLoss: 0.8563\tLR: 0.010000\n",
            "Training Epoch: 15 [29312/50000]\tLoss: 0.9317\tLR: 0.010000\n",
            "Training Epoch: 15 [29440/50000]\tLoss: 0.9195\tLR: 0.010000\n",
            "Training Epoch: 15 [29568/50000]\tLoss: 1.0590\tLR: 0.010000\n",
            "Training Epoch: 15 [29696/50000]\tLoss: 0.9811\tLR: 0.010000\n",
            "Training Epoch: 15 [29824/50000]\tLoss: 0.7225\tLR: 0.010000\n",
            "Training Epoch: 15 [29952/50000]\tLoss: 0.8255\tLR: 0.010000\n",
            "Training Epoch: 15 [30080/50000]\tLoss: 0.8495\tLR: 0.010000\n",
            "Training Epoch: 15 [30208/50000]\tLoss: 0.9281\tLR: 0.010000\n",
            "Training Epoch: 15 [30336/50000]\tLoss: 0.9543\tLR: 0.010000\n",
            "Training Epoch: 15 [30464/50000]\tLoss: 1.0767\tLR: 0.010000\n",
            "Training Epoch: 15 [30592/50000]\tLoss: 0.9336\tLR: 0.010000\n",
            "Training Epoch: 15 [30720/50000]\tLoss: 1.0922\tLR: 0.010000\n",
            "Training Epoch: 15 [30848/50000]\tLoss: 0.9027\tLR: 0.010000\n",
            "Training Epoch: 15 [30976/50000]\tLoss: 1.0986\tLR: 0.010000\n",
            "Training Epoch: 15 [31104/50000]\tLoss: 1.0355\tLR: 0.010000\n",
            "Training Epoch: 15 [31232/50000]\tLoss: 0.7248\tLR: 0.010000\n",
            "Training Epoch: 15 [31360/50000]\tLoss: 1.0454\tLR: 0.010000\n",
            "Training Epoch: 15 [31488/50000]\tLoss: 1.0282\tLR: 0.010000\n",
            "Training Epoch: 15 [31616/50000]\tLoss: 1.0375\tLR: 0.010000\n",
            "Training Epoch: 15 [31744/50000]\tLoss: 0.8874\tLR: 0.010000\n",
            "Training Epoch: 15 [31872/50000]\tLoss: 0.9297\tLR: 0.010000\n",
            "Training Epoch: 15 [32000/50000]\tLoss: 0.9726\tLR: 0.010000\n",
            "Training Epoch: 15 [32128/50000]\tLoss: 1.0689\tLR: 0.010000\n",
            "Training Epoch: 15 [32256/50000]\tLoss: 0.8191\tLR: 0.010000\n",
            "Training Epoch: 15 [32384/50000]\tLoss: 1.0760\tLR: 0.010000\n",
            "Training Epoch: 15 [32512/50000]\tLoss: 0.8085\tLR: 0.010000\n",
            "Training Epoch: 15 [32640/50000]\tLoss: 0.9975\tLR: 0.010000\n",
            "Training Epoch: 15 [32768/50000]\tLoss: 1.0373\tLR: 0.010000\n",
            "Training Epoch: 15 [32896/50000]\tLoss: 1.0744\tLR: 0.010000\n",
            "Training Epoch: 15 [33024/50000]\tLoss: 0.8445\tLR: 0.010000\n",
            "Training Epoch: 15 [33152/50000]\tLoss: 1.1702\tLR: 0.010000\n",
            "Training Epoch: 15 [33280/50000]\tLoss: 1.1663\tLR: 0.010000\n",
            "Training Epoch: 15 [33408/50000]\tLoss: 0.8954\tLR: 0.010000\n",
            "Training Epoch: 15 [33536/50000]\tLoss: 0.9274\tLR: 0.010000\n",
            "Training Epoch: 15 [33664/50000]\tLoss: 0.8754\tLR: 0.010000\n",
            "Training Epoch: 15 [33792/50000]\tLoss: 0.9979\tLR: 0.010000\n",
            "Training Epoch: 15 [33920/50000]\tLoss: 1.0768\tLR: 0.010000\n",
            "Training Epoch: 15 [34048/50000]\tLoss: 1.0767\tLR: 0.010000\n",
            "Training Epoch: 15 [34176/50000]\tLoss: 0.9519\tLR: 0.010000\n",
            "Training Epoch: 15 [34304/50000]\tLoss: 1.1149\tLR: 0.010000\n",
            "Training Epoch: 15 [34432/50000]\tLoss: 1.0283\tLR: 0.010000\n",
            "Training Epoch: 15 [34560/50000]\tLoss: 0.9819\tLR: 0.010000\n",
            "Training Epoch: 15 [34688/50000]\tLoss: 0.8811\tLR: 0.010000\n",
            "Training Epoch: 15 [34816/50000]\tLoss: 0.9832\tLR: 0.010000\n",
            "Training Epoch: 15 [34944/50000]\tLoss: 0.9901\tLR: 0.010000\n",
            "Training Epoch: 15 [35072/50000]\tLoss: 0.9391\tLR: 0.010000\n",
            "Training Epoch: 15 [35200/50000]\tLoss: 0.9220\tLR: 0.010000\n",
            "Training Epoch: 15 [35328/50000]\tLoss: 1.1869\tLR: 0.010000\n",
            "Training Epoch: 15 [35456/50000]\tLoss: 0.9768\tLR: 0.010000\n",
            "Training Epoch: 15 [35584/50000]\tLoss: 0.9682\tLR: 0.010000\n",
            "Training Epoch: 15 [35712/50000]\tLoss: 0.8921\tLR: 0.010000\n",
            "Training Epoch: 15 [35840/50000]\tLoss: 1.0031\tLR: 0.010000\n",
            "Training Epoch: 15 [35968/50000]\tLoss: 1.3573\tLR: 0.010000\n",
            "Training Epoch: 15 [36096/50000]\tLoss: 1.0581\tLR: 0.010000\n",
            "Training Epoch: 15 [36224/50000]\tLoss: 1.1698\tLR: 0.010000\n",
            "Training Epoch: 15 [36352/50000]\tLoss: 0.8666\tLR: 0.010000\n",
            "Training Epoch: 15 [36480/50000]\tLoss: 0.9959\tLR: 0.010000\n",
            "Training Epoch: 15 [36608/50000]\tLoss: 1.1970\tLR: 0.010000\n",
            "Training Epoch: 15 [36736/50000]\tLoss: 0.8734\tLR: 0.010000\n",
            "Training Epoch: 15 [36864/50000]\tLoss: 1.0873\tLR: 0.010000\n",
            "Training Epoch: 15 [36992/50000]\tLoss: 1.1483\tLR: 0.010000\n",
            "Training Epoch: 15 [37120/50000]\tLoss: 1.0345\tLR: 0.010000\n",
            "Training Epoch: 15 [37248/50000]\tLoss: 0.8641\tLR: 0.010000\n",
            "Training Epoch: 15 [37376/50000]\tLoss: 1.1122\tLR: 0.010000\n",
            "Training Epoch: 15 [37504/50000]\tLoss: 1.1459\tLR: 0.010000\n",
            "Training Epoch: 15 [37632/50000]\tLoss: 1.0369\tLR: 0.010000\n",
            "Training Epoch: 15 [37760/50000]\tLoss: 1.0850\tLR: 0.010000\n",
            "Training Epoch: 15 [37888/50000]\tLoss: 0.9135\tLR: 0.010000\n",
            "Training Epoch: 15 [38016/50000]\tLoss: 0.9878\tLR: 0.010000\n",
            "Training Epoch: 15 [38144/50000]\tLoss: 0.8813\tLR: 0.010000\n",
            "Training Epoch: 15 [38272/50000]\tLoss: 0.9531\tLR: 0.010000\n",
            "Training Epoch: 15 [38400/50000]\tLoss: 0.9963\tLR: 0.010000\n",
            "Training Epoch: 15 [38528/50000]\tLoss: 1.0798\tLR: 0.010000\n",
            "Training Epoch: 15 [38656/50000]\tLoss: 0.9441\tLR: 0.010000\n",
            "Training Epoch: 15 [38784/50000]\tLoss: 1.0203\tLR: 0.010000\n",
            "Training Epoch: 15 [38912/50000]\tLoss: 1.0232\tLR: 0.010000\n",
            "Training Epoch: 15 [39040/50000]\tLoss: 1.1017\tLR: 0.010000\n",
            "Training Epoch: 15 [39168/50000]\tLoss: 1.0761\tLR: 0.010000\n",
            "Training Epoch: 15 [39296/50000]\tLoss: 1.0163\tLR: 0.010000\n",
            "Training Epoch: 15 [39424/50000]\tLoss: 0.9939\tLR: 0.010000\n",
            "Training Epoch: 15 [39552/50000]\tLoss: 0.9313\tLR: 0.010000\n",
            "Training Epoch: 15 [39680/50000]\tLoss: 1.1649\tLR: 0.010000\n",
            "Training Epoch: 15 [39808/50000]\tLoss: 0.8372\tLR: 0.010000\n",
            "Training Epoch: 15 [39936/50000]\tLoss: 0.6528\tLR: 0.010000\n",
            "Training Epoch: 15 [40064/50000]\tLoss: 0.9544\tLR: 0.010000\n",
            "Training Epoch: 15 [40192/50000]\tLoss: 1.0609\tLR: 0.010000\n",
            "Training Epoch: 15 [40320/50000]\tLoss: 0.9386\tLR: 0.010000\n",
            "Training Epoch: 15 [40448/50000]\tLoss: 0.9742\tLR: 0.010000\n",
            "Training Epoch: 15 [40576/50000]\tLoss: 0.9583\tLR: 0.010000\n",
            "Training Epoch: 15 [40704/50000]\tLoss: 1.0834\tLR: 0.010000\n",
            "Training Epoch: 15 [40832/50000]\tLoss: 0.8128\tLR: 0.010000\n",
            "Training Epoch: 15 [40960/50000]\tLoss: 0.9562\tLR: 0.010000\n",
            "Training Epoch: 15 [41088/50000]\tLoss: 1.1891\tLR: 0.010000\n",
            "Training Epoch: 15 [41216/50000]\tLoss: 1.0624\tLR: 0.010000\n",
            "Training Epoch: 15 [41344/50000]\tLoss: 0.9815\tLR: 0.010000\n",
            "Training Epoch: 15 [41472/50000]\tLoss: 0.8732\tLR: 0.010000\n",
            "Training Epoch: 15 [41600/50000]\tLoss: 1.1715\tLR: 0.010000\n",
            "Training Epoch: 15 [41728/50000]\tLoss: 1.2244\tLR: 0.010000\n",
            "Training Epoch: 15 [41856/50000]\tLoss: 1.0998\tLR: 0.010000\n",
            "Training Epoch: 15 [41984/50000]\tLoss: 1.0806\tLR: 0.010000\n",
            "Training Epoch: 15 [42112/50000]\tLoss: 1.1703\tLR: 0.010000\n",
            "Training Epoch: 15 [42240/50000]\tLoss: 1.1147\tLR: 0.010000\n",
            "Training Epoch: 15 [42368/50000]\tLoss: 1.2123\tLR: 0.010000\n",
            "Training Epoch: 15 [42496/50000]\tLoss: 1.0839\tLR: 0.010000\n",
            "Training Epoch: 15 [42624/50000]\tLoss: 0.9765\tLR: 0.010000\n",
            "Training Epoch: 15 [42752/50000]\tLoss: 0.9138\tLR: 0.010000\n",
            "Training Epoch: 15 [42880/50000]\tLoss: 1.0098\tLR: 0.010000\n",
            "Training Epoch: 15 [43008/50000]\tLoss: 0.9403\tLR: 0.010000\n",
            "Training Epoch: 15 [43136/50000]\tLoss: 1.1275\tLR: 0.010000\n",
            "Training Epoch: 15 [43264/50000]\tLoss: 0.9118\tLR: 0.010000\n",
            "Training Epoch: 15 [43392/50000]\tLoss: 0.8182\tLR: 0.010000\n",
            "Training Epoch: 15 [43520/50000]\tLoss: 1.0632\tLR: 0.010000\n",
            "Training Epoch: 15 [43648/50000]\tLoss: 1.1670\tLR: 0.010000\n",
            "Training Epoch: 15 [43776/50000]\tLoss: 0.8506\tLR: 0.010000\n",
            "Training Epoch: 15 [43904/50000]\tLoss: 0.9520\tLR: 0.010000\n",
            "Training Epoch: 15 [44032/50000]\tLoss: 1.1320\tLR: 0.010000\n",
            "Training Epoch: 15 [44160/50000]\tLoss: 0.9269\tLR: 0.010000\n",
            "Training Epoch: 15 [44288/50000]\tLoss: 0.8081\tLR: 0.010000\n",
            "Training Epoch: 15 [44416/50000]\tLoss: 0.9511\tLR: 0.010000\n",
            "Training Epoch: 15 [44544/50000]\tLoss: 0.8518\tLR: 0.010000\n",
            "Training Epoch: 15 [44672/50000]\tLoss: 0.8466\tLR: 0.010000\n",
            "Training Epoch: 15 [44800/50000]\tLoss: 1.0571\tLR: 0.010000\n",
            "Training Epoch: 15 [44928/50000]\tLoss: 0.9968\tLR: 0.010000\n",
            "Training Epoch: 15 [45056/50000]\tLoss: 0.9141\tLR: 0.010000\n",
            "Training Epoch: 15 [45184/50000]\tLoss: 1.4777\tLR: 0.010000\n",
            "Training Epoch: 15 [45312/50000]\tLoss: 1.0611\tLR: 0.010000\n",
            "Training Epoch: 15 [45440/50000]\tLoss: 0.8731\tLR: 0.010000\n",
            "Training Epoch: 15 [45568/50000]\tLoss: 0.8938\tLR: 0.010000\n",
            "Training Epoch: 15 [45696/50000]\tLoss: 1.2679\tLR: 0.010000\n",
            "Training Epoch: 15 [45824/50000]\tLoss: 0.9410\tLR: 0.010000\n",
            "Training Epoch: 15 [45952/50000]\tLoss: 0.9989\tLR: 0.010000\n",
            "Training Epoch: 15 [46080/50000]\tLoss: 0.9657\tLR: 0.010000\n",
            "Training Epoch: 15 [46208/50000]\tLoss: 0.9971\tLR: 0.010000\n",
            "Training Epoch: 15 [46336/50000]\tLoss: 0.9644\tLR: 0.010000\n",
            "Training Epoch: 15 [46464/50000]\tLoss: 1.2380\tLR: 0.010000\n",
            "Training Epoch: 15 [46592/50000]\tLoss: 1.1366\tLR: 0.010000\n",
            "Training Epoch: 15 [46720/50000]\tLoss: 1.0395\tLR: 0.010000\n",
            "Training Epoch: 15 [46848/50000]\tLoss: 0.9510\tLR: 0.010000\n",
            "Training Epoch: 15 [46976/50000]\tLoss: 1.1982\tLR: 0.010000\n",
            "Training Epoch: 15 [47104/50000]\tLoss: 1.0019\tLR: 0.010000\n",
            "Training Epoch: 15 [47232/50000]\tLoss: 1.1266\tLR: 0.010000\n",
            "Training Epoch: 15 [47360/50000]\tLoss: 0.9670\tLR: 0.010000\n",
            "Training Epoch: 15 [47488/50000]\tLoss: 1.1372\tLR: 0.010000\n",
            "Training Epoch: 15 [47616/50000]\tLoss: 1.0823\tLR: 0.010000\n",
            "Training Epoch: 15 [47744/50000]\tLoss: 1.2444\tLR: 0.010000\n",
            "Training Epoch: 15 [47872/50000]\tLoss: 0.9542\tLR: 0.010000\n",
            "Training Epoch: 15 [48000/50000]\tLoss: 1.0779\tLR: 0.010000\n",
            "Training Epoch: 15 [48128/50000]\tLoss: 1.1186\tLR: 0.010000\n",
            "Training Epoch: 15 [48256/50000]\tLoss: 0.9756\tLR: 0.010000\n",
            "Training Epoch: 15 [48384/50000]\tLoss: 0.9590\tLR: 0.010000\n",
            "Training Epoch: 15 [48512/50000]\tLoss: 1.2444\tLR: 0.010000\n",
            "Training Epoch: 15 [48640/50000]\tLoss: 0.9677\tLR: 0.010000\n",
            "Training Epoch: 15 [48768/50000]\tLoss: 1.2989\tLR: 0.010000\n",
            "Training Epoch: 15 [48896/50000]\tLoss: 0.9811\tLR: 0.010000\n",
            "Training Epoch: 15 [49024/50000]\tLoss: 0.9136\tLR: 0.010000\n",
            "Training Epoch: 15 [49152/50000]\tLoss: 0.8939\tLR: 0.010000\n",
            "Training Epoch: 15 [49280/50000]\tLoss: 0.9375\tLR: 0.010000\n",
            "Training Epoch: 15 [49408/50000]\tLoss: 1.0797\tLR: 0.010000\n",
            "Training Epoch: 15 [49536/50000]\tLoss: 1.1601\tLR: 0.010000\n",
            "Training Epoch: 15 [49664/50000]\tLoss: 1.1190\tLR: 0.010000\n",
            "Training Epoch: 15 [49792/50000]\tLoss: 1.2724\tLR: 0.010000\n",
            "Training Epoch: 15 [49920/50000]\tLoss: 1.2239\tLR: 0.010000\n",
            "Training Epoch: 15 [50000/50000]\tLoss: 1.0555\tLR: 0.010000\n",
            "epoch 15 training time consumed: 205.73s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 126003 GiB | 126002 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 125911 GiB | 125911 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     91 GiB |     91 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 126003 GiB | 126002 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 125911 GiB | 125911 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     91 GiB |     91 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 125990 GiB | 125989 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 125899 GiB | 125898 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     90 GiB |     90 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  91503 GiB |  91502 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  91409 GiB |  91408 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |     93 GiB |     93 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    5002 K  |    5002 K  |\n",
            "|       from large pool |      92    |     184    |    2820 K  |    2820 K  |\n",
            "|       from small pool |     519    |     646    |    2182 K  |    2181 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    5002 K  |    5002 K  |\n",
            "|       from large pool |      92    |     184    |    2820 K  |    2820 K  |\n",
            "|       from small pool |     519    |     646    |    2182 K  |    2181 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |     122    |    2111 K  |    2111 K  |\n",
            "|       from large pool |      42    |     111    |    1665 K  |    1665 K  |\n",
            "|       from small pool |       8    |      21    |     446 K  |     446 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0113, Accuracy: 0.6188, Time consumed:11.22s\n",
            "\n",
            "Training Epoch: 16 [128/50000]\tLoss: 1.2170\tLR: 0.010000\n",
            "Training Epoch: 16 [256/50000]\tLoss: 1.0153\tLR: 0.010000\n",
            "Training Epoch: 16 [384/50000]\tLoss: 0.9352\tLR: 0.010000\n",
            "Training Epoch: 16 [512/50000]\tLoss: 1.0733\tLR: 0.010000\n",
            "Training Epoch: 16 [640/50000]\tLoss: 0.7899\tLR: 0.010000\n",
            "Training Epoch: 16 [768/50000]\tLoss: 0.8115\tLR: 0.010000\n",
            "Training Epoch: 16 [896/50000]\tLoss: 0.8124\tLR: 0.010000\n",
            "Training Epoch: 16 [1024/50000]\tLoss: 1.0394\tLR: 0.010000\n",
            "Training Epoch: 16 [1152/50000]\tLoss: 1.0608\tLR: 0.010000\n",
            "Training Epoch: 16 [1280/50000]\tLoss: 0.6982\tLR: 0.010000\n",
            "Training Epoch: 16 [1408/50000]\tLoss: 0.8893\tLR: 0.010000\n",
            "Training Epoch: 16 [1536/50000]\tLoss: 0.9085\tLR: 0.010000\n",
            "Training Epoch: 16 [1664/50000]\tLoss: 1.0436\tLR: 0.010000\n",
            "Training Epoch: 16 [1792/50000]\tLoss: 0.8588\tLR: 0.010000\n",
            "Training Epoch: 16 [1920/50000]\tLoss: 0.8322\tLR: 0.010000\n",
            "Training Epoch: 16 [2048/50000]\tLoss: 1.0217\tLR: 0.010000\n",
            "Training Epoch: 16 [2176/50000]\tLoss: 1.0069\tLR: 0.010000\n",
            "Training Epoch: 16 [2304/50000]\tLoss: 0.8826\tLR: 0.010000\n",
            "Training Epoch: 16 [2432/50000]\tLoss: 1.0351\tLR: 0.010000\n",
            "Training Epoch: 16 [2560/50000]\tLoss: 0.9793\tLR: 0.010000\n",
            "Training Epoch: 16 [2688/50000]\tLoss: 0.8940\tLR: 0.010000\n",
            "Training Epoch: 16 [2816/50000]\tLoss: 0.8153\tLR: 0.010000\n",
            "Training Epoch: 16 [2944/50000]\tLoss: 0.9299\tLR: 0.010000\n",
            "Training Epoch: 16 [3072/50000]\tLoss: 1.1793\tLR: 0.010000\n",
            "Training Epoch: 16 [3200/50000]\tLoss: 0.9148\tLR: 0.010000\n",
            "Training Epoch: 16 [3328/50000]\tLoss: 0.7777\tLR: 0.010000\n",
            "Training Epoch: 16 [3456/50000]\tLoss: 0.7821\tLR: 0.010000\n",
            "Training Epoch: 16 [3584/50000]\tLoss: 0.9157\tLR: 0.010000\n",
            "Training Epoch: 16 [3712/50000]\tLoss: 0.7332\tLR: 0.010000\n",
            "Training Epoch: 16 [3840/50000]\tLoss: 0.8960\tLR: 0.010000\n",
            "Training Epoch: 16 [3968/50000]\tLoss: 1.0147\tLR: 0.010000\n",
            "Training Epoch: 16 [4096/50000]\tLoss: 0.8784\tLR: 0.010000\n",
            "Training Epoch: 16 [4224/50000]\tLoss: 1.0041\tLR: 0.010000\n",
            "Training Epoch: 16 [4352/50000]\tLoss: 1.0351\tLR: 0.010000\n",
            "Training Epoch: 16 [4480/50000]\tLoss: 0.8303\tLR: 0.010000\n",
            "Training Epoch: 16 [4608/50000]\tLoss: 0.7055\tLR: 0.010000\n",
            "Training Epoch: 16 [4736/50000]\tLoss: 0.8544\tLR: 0.010000\n",
            "Training Epoch: 16 [4864/50000]\tLoss: 0.7012\tLR: 0.010000\n",
            "Training Epoch: 16 [4992/50000]\tLoss: 1.1575\tLR: 0.010000\n",
            "Training Epoch: 16 [5120/50000]\tLoss: 0.7654\tLR: 0.010000\n",
            "Training Epoch: 16 [5248/50000]\tLoss: 1.0689\tLR: 0.010000\n",
            "Training Epoch: 16 [5376/50000]\tLoss: 0.8990\tLR: 0.010000\n",
            "Training Epoch: 16 [5504/50000]\tLoss: 0.8684\tLR: 0.010000\n",
            "Training Epoch: 16 [5632/50000]\tLoss: 0.8878\tLR: 0.010000\n",
            "Training Epoch: 16 [5760/50000]\tLoss: 0.8324\tLR: 0.010000\n",
            "Training Epoch: 16 [5888/50000]\tLoss: 0.7678\tLR: 0.010000\n",
            "Training Epoch: 16 [6016/50000]\tLoss: 1.0477\tLR: 0.010000\n",
            "Training Epoch: 16 [6144/50000]\tLoss: 0.9397\tLR: 0.010000\n",
            "Training Epoch: 16 [6272/50000]\tLoss: 0.8494\tLR: 0.010000\n",
            "Training Epoch: 16 [6400/50000]\tLoss: 0.9295\tLR: 0.010000\n",
            "Training Epoch: 16 [6528/50000]\tLoss: 0.9331\tLR: 0.010000\n",
            "Training Epoch: 16 [6656/50000]\tLoss: 0.9954\tLR: 0.010000\n",
            "Training Epoch: 16 [6784/50000]\tLoss: 0.8526\tLR: 0.010000\n",
            "Training Epoch: 16 [6912/50000]\tLoss: 0.8990\tLR: 0.010000\n",
            "Training Epoch: 16 [7040/50000]\tLoss: 0.9382\tLR: 0.010000\n",
            "Training Epoch: 16 [7168/50000]\tLoss: 0.7427\tLR: 0.010000\n",
            "Training Epoch: 16 [7296/50000]\tLoss: 0.8778\tLR: 0.010000\n",
            "Training Epoch: 16 [7424/50000]\tLoss: 0.9336\tLR: 0.010000\n",
            "Training Epoch: 16 [7552/50000]\tLoss: 0.8428\tLR: 0.010000\n",
            "Training Epoch: 16 [7680/50000]\tLoss: 0.8749\tLR: 0.010000\n",
            "Training Epoch: 16 [7808/50000]\tLoss: 0.7613\tLR: 0.010000\n",
            "Training Epoch: 16 [7936/50000]\tLoss: 0.8101\tLR: 0.010000\n",
            "Training Epoch: 16 [8064/50000]\tLoss: 0.9524\tLR: 0.010000\n",
            "Training Epoch: 16 [8192/50000]\tLoss: 0.6801\tLR: 0.010000\n",
            "Training Epoch: 16 [8320/50000]\tLoss: 0.9167\tLR: 0.010000\n",
            "Training Epoch: 16 [8448/50000]\tLoss: 0.8471\tLR: 0.010000\n",
            "Training Epoch: 16 [8576/50000]\tLoss: 0.7807\tLR: 0.010000\n",
            "Training Epoch: 16 [8704/50000]\tLoss: 1.0005\tLR: 0.010000\n",
            "Training Epoch: 16 [8832/50000]\tLoss: 1.0161\tLR: 0.010000\n",
            "Training Epoch: 16 [8960/50000]\tLoss: 0.9161\tLR: 0.010000\n",
            "Training Epoch: 16 [9088/50000]\tLoss: 0.8076\tLR: 0.010000\n",
            "Training Epoch: 16 [9216/50000]\tLoss: 0.6800\tLR: 0.010000\n",
            "Training Epoch: 16 [9344/50000]\tLoss: 0.8243\tLR: 0.010000\n",
            "Training Epoch: 16 [9472/50000]\tLoss: 0.8762\tLR: 0.010000\n",
            "Training Epoch: 16 [9600/50000]\tLoss: 0.8240\tLR: 0.010000\n",
            "Training Epoch: 16 [9728/50000]\tLoss: 0.9123\tLR: 0.010000\n",
            "Training Epoch: 16 [9856/50000]\tLoss: 0.6977\tLR: 0.010000\n",
            "Training Epoch: 16 [9984/50000]\tLoss: 0.7263\tLR: 0.010000\n",
            "Training Epoch: 16 [10112/50000]\tLoss: 0.9782\tLR: 0.010000\n",
            "Training Epoch: 16 [10240/50000]\tLoss: 0.8471\tLR: 0.010000\n",
            "Training Epoch: 16 [10368/50000]\tLoss: 0.8504\tLR: 0.010000\n",
            "Training Epoch: 16 [10496/50000]\tLoss: 0.8355\tLR: 0.010000\n",
            "Training Epoch: 16 [10624/50000]\tLoss: 0.8677\tLR: 0.010000\n",
            "Training Epoch: 16 [10752/50000]\tLoss: 1.0450\tLR: 0.010000\n",
            "Training Epoch: 16 [10880/50000]\tLoss: 0.8009\tLR: 0.010000\n",
            "Training Epoch: 16 [11008/50000]\tLoss: 0.8147\tLR: 0.010000\n",
            "Training Epoch: 16 [11136/50000]\tLoss: 1.0578\tLR: 0.010000\n",
            "Training Epoch: 16 [11264/50000]\tLoss: 0.9329\tLR: 0.010000\n",
            "Training Epoch: 16 [11392/50000]\tLoss: 0.9885\tLR: 0.010000\n",
            "Training Epoch: 16 [11520/50000]\tLoss: 1.0329\tLR: 0.010000\n",
            "Training Epoch: 16 [11648/50000]\tLoss: 0.8825\tLR: 0.010000\n",
            "Training Epoch: 16 [11776/50000]\tLoss: 0.8037\tLR: 0.010000\n",
            "Training Epoch: 16 [11904/50000]\tLoss: 0.9316\tLR: 0.010000\n",
            "Training Epoch: 16 [12032/50000]\tLoss: 0.7022\tLR: 0.010000\n",
            "Training Epoch: 16 [12160/50000]\tLoss: 1.1734\tLR: 0.010000\n",
            "Training Epoch: 16 [12288/50000]\tLoss: 0.8291\tLR: 0.010000\n",
            "Training Epoch: 16 [12416/50000]\tLoss: 0.9309\tLR: 0.010000\n",
            "Training Epoch: 16 [12544/50000]\tLoss: 0.8296\tLR: 0.010000\n",
            "Training Epoch: 16 [12672/50000]\tLoss: 0.9365\tLR: 0.010000\n",
            "Training Epoch: 16 [12800/50000]\tLoss: 0.8582\tLR: 0.010000\n",
            "Training Epoch: 16 [12928/50000]\tLoss: 0.7956\tLR: 0.010000\n",
            "Training Epoch: 16 [13056/50000]\tLoss: 1.0088\tLR: 0.010000\n",
            "Training Epoch: 16 [13184/50000]\tLoss: 0.7092\tLR: 0.010000\n",
            "Training Epoch: 16 [13312/50000]\tLoss: 1.0751\tLR: 0.010000\n",
            "Training Epoch: 16 [13440/50000]\tLoss: 0.7181\tLR: 0.010000\n",
            "Training Epoch: 16 [13568/50000]\tLoss: 1.1689\tLR: 0.010000\n",
            "Training Epoch: 16 [13696/50000]\tLoss: 0.8105\tLR: 0.010000\n",
            "Training Epoch: 16 [13824/50000]\tLoss: 0.9658\tLR: 0.010000\n",
            "Training Epoch: 16 [13952/50000]\tLoss: 0.9737\tLR: 0.010000\n",
            "Training Epoch: 16 [14080/50000]\tLoss: 1.0285\tLR: 0.010000\n",
            "Training Epoch: 16 [14208/50000]\tLoss: 0.9139\tLR: 0.010000\n",
            "Training Epoch: 16 [14336/50000]\tLoss: 0.8808\tLR: 0.010000\n",
            "Training Epoch: 16 [14464/50000]\tLoss: 0.9140\tLR: 0.010000\n",
            "Training Epoch: 16 [14592/50000]\tLoss: 0.8775\tLR: 0.010000\n",
            "Training Epoch: 16 [14720/50000]\tLoss: 0.9192\tLR: 0.010000\n",
            "Training Epoch: 16 [14848/50000]\tLoss: 0.9823\tLR: 0.010000\n",
            "Training Epoch: 16 [14976/50000]\tLoss: 0.7592\tLR: 0.010000\n",
            "Training Epoch: 16 [15104/50000]\tLoss: 0.9500\tLR: 0.010000\n",
            "Training Epoch: 16 [15232/50000]\tLoss: 0.9312\tLR: 0.010000\n",
            "Training Epoch: 16 [15360/50000]\tLoss: 0.7831\tLR: 0.010000\n",
            "Training Epoch: 16 [15488/50000]\tLoss: 1.0121\tLR: 0.010000\n",
            "Training Epoch: 16 [15616/50000]\tLoss: 0.8804\tLR: 0.010000\n",
            "Training Epoch: 16 [15744/50000]\tLoss: 0.9103\tLR: 0.010000\n",
            "Training Epoch: 16 [15872/50000]\tLoss: 0.9475\tLR: 0.010000\n",
            "Training Epoch: 16 [16000/50000]\tLoss: 0.8322\tLR: 0.010000\n",
            "Training Epoch: 16 [16128/50000]\tLoss: 1.0978\tLR: 0.010000\n",
            "Training Epoch: 16 [16256/50000]\tLoss: 0.8285\tLR: 0.010000\n",
            "Training Epoch: 16 [16384/50000]\tLoss: 0.8841\tLR: 0.010000\n",
            "Training Epoch: 16 [16512/50000]\tLoss: 0.8665\tLR: 0.010000\n",
            "Training Epoch: 16 [16640/50000]\tLoss: 1.0122\tLR: 0.010000\n",
            "Training Epoch: 16 [16768/50000]\tLoss: 1.0453\tLR: 0.010000\n",
            "Training Epoch: 16 [16896/50000]\tLoss: 0.9750\tLR: 0.010000\n",
            "Training Epoch: 16 [17024/50000]\tLoss: 0.9607\tLR: 0.010000\n",
            "Training Epoch: 16 [17152/50000]\tLoss: 0.9902\tLR: 0.010000\n",
            "Training Epoch: 16 [17280/50000]\tLoss: 0.9473\tLR: 0.010000\n",
            "Training Epoch: 16 [17408/50000]\tLoss: 0.7682\tLR: 0.010000\n",
            "Training Epoch: 16 [17536/50000]\tLoss: 0.9997\tLR: 0.010000\n",
            "Training Epoch: 16 [17664/50000]\tLoss: 1.1871\tLR: 0.010000\n",
            "Training Epoch: 16 [17792/50000]\tLoss: 0.9094\tLR: 0.010000\n",
            "Training Epoch: 16 [17920/50000]\tLoss: 1.0336\tLR: 0.010000\n",
            "Training Epoch: 16 [18048/50000]\tLoss: 0.8418\tLR: 0.010000\n",
            "Training Epoch: 16 [18176/50000]\tLoss: 1.0024\tLR: 0.010000\n",
            "Training Epoch: 16 [18304/50000]\tLoss: 0.9549\tLR: 0.010000\n",
            "Training Epoch: 16 [18432/50000]\tLoss: 0.9167\tLR: 0.010000\n",
            "Training Epoch: 16 [18560/50000]\tLoss: 0.8801\tLR: 0.010000\n",
            "Training Epoch: 16 [18688/50000]\tLoss: 1.1387\tLR: 0.010000\n",
            "Training Epoch: 16 [18816/50000]\tLoss: 0.9575\tLR: 0.010000\n",
            "Training Epoch: 16 [18944/50000]\tLoss: 0.7505\tLR: 0.010000\n",
            "Training Epoch: 16 [19072/50000]\tLoss: 0.8637\tLR: 0.010000\n",
            "Training Epoch: 16 [19200/50000]\tLoss: 0.9550\tLR: 0.010000\n",
            "Training Epoch: 16 [19328/50000]\tLoss: 0.9210\tLR: 0.010000\n",
            "Training Epoch: 16 [19456/50000]\tLoss: 0.9584\tLR: 0.010000\n",
            "Training Epoch: 16 [19584/50000]\tLoss: 0.8259\tLR: 0.010000\n",
            "Training Epoch: 16 [19712/50000]\tLoss: 0.7402\tLR: 0.010000\n",
            "Training Epoch: 16 [19840/50000]\tLoss: 0.9616\tLR: 0.010000\n",
            "Training Epoch: 16 [19968/50000]\tLoss: 0.8424\tLR: 0.010000\n",
            "Training Epoch: 16 [20096/50000]\tLoss: 0.9311\tLR: 0.010000\n",
            "Training Epoch: 16 [20224/50000]\tLoss: 1.0982\tLR: 0.010000\n",
            "Training Epoch: 16 [20352/50000]\tLoss: 0.6123\tLR: 0.010000\n",
            "Training Epoch: 16 [20480/50000]\tLoss: 0.9918\tLR: 0.010000\n",
            "Training Epoch: 16 [20608/50000]\tLoss: 1.0924\tLR: 0.010000\n",
            "Training Epoch: 16 [20736/50000]\tLoss: 0.8550\tLR: 0.010000\n",
            "Training Epoch: 16 [20864/50000]\tLoss: 0.9022\tLR: 0.010000\n",
            "Training Epoch: 16 [20992/50000]\tLoss: 0.9817\tLR: 0.010000\n",
            "Training Epoch: 16 [21120/50000]\tLoss: 0.8284\tLR: 0.010000\n",
            "Training Epoch: 16 [21248/50000]\tLoss: 0.8431\tLR: 0.010000\n",
            "Training Epoch: 16 [21376/50000]\tLoss: 0.9669\tLR: 0.010000\n",
            "Training Epoch: 16 [21504/50000]\tLoss: 0.9885\tLR: 0.010000\n",
            "Training Epoch: 16 [21632/50000]\tLoss: 0.9955\tLR: 0.010000\n",
            "Training Epoch: 16 [21760/50000]\tLoss: 1.1143\tLR: 0.010000\n",
            "Training Epoch: 16 [21888/50000]\tLoss: 0.8848\tLR: 0.010000\n",
            "Training Epoch: 16 [22016/50000]\tLoss: 1.0448\tLR: 0.010000\n",
            "Training Epoch: 16 [22144/50000]\tLoss: 0.9053\tLR: 0.010000\n",
            "Training Epoch: 16 [22272/50000]\tLoss: 0.9188\tLR: 0.010000\n",
            "Training Epoch: 16 [22400/50000]\tLoss: 0.8811\tLR: 0.010000\n",
            "Training Epoch: 16 [22528/50000]\tLoss: 0.8037\tLR: 0.010000\n",
            "Training Epoch: 16 [22656/50000]\tLoss: 0.9235\tLR: 0.010000\n",
            "Training Epoch: 16 [22784/50000]\tLoss: 1.0386\tLR: 0.010000\n",
            "Training Epoch: 16 [22912/50000]\tLoss: 0.9268\tLR: 0.010000\n",
            "Training Epoch: 16 [23040/50000]\tLoss: 0.9167\tLR: 0.010000\n",
            "Training Epoch: 16 [23168/50000]\tLoss: 0.9545\tLR: 0.010000\n",
            "Training Epoch: 16 [23296/50000]\tLoss: 1.1205\tLR: 0.010000\n",
            "Training Epoch: 16 [23424/50000]\tLoss: 0.8602\tLR: 0.010000\n",
            "Training Epoch: 16 [23552/50000]\tLoss: 1.1551\tLR: 0.010000\n",
            "Training Epoch: 16 [23680/50000]\tLoss: 0.9835\tLR: 0.010000\n",
            "Training Epoch: 16 [23808/50000]\tLoss: 0.7905\tLR: 0.010000\n",
            "Training Epoch: 16 [23936/50000]\tLoss: 0.9524\tLR: 0.010000\n",
            "Training Epoch: 16 [24064/50000]\tLoss: 0.8811\tLR: 0.010000\n",
            "Training Epoch: 16 [24192/50000]\tLoss: 0.8452\tLR: 0.010000\n",
            "Training Epoch: 16 [24320/50000]\tLoss: 0.7923\tLR: 0.010000\n",
            "Training Epoch: 16 [24448/50000]\tLoss: 1.0303\tLR: 0.010000\n",
            "Training Epoch: 16 [24576/50000]\tLoss: 0.8831\tLR: 0.010000\n",
            "Training Epoch: 16 [24704/50000]\tLoss: 0.9791\tLR: 0.010000\n",
            "Training Epoch: 16 [24832/50000]\tLoss: 1.0111\tLR: 0.010000\n",
            "Training Epoch: 16 [24960/50000]\tLoss: 0.8849\tLR: 0.010000\n",
            "Training Epoch: 16 [25088/50000]\tLoss: 0.8187\tLR: 0.010000\n",
            "Training Epoch: 16 [25216/50000]\tLoss: 1.2863\tLR: 0.010000\n",
            "Training Epoch: 16 [25344/50000]\tLoss: 0.9733\tLR: 0.010000\n",
            "Training Epoch: 16 [25472/50000]\tLoss: 0.8597\tLR: 0.010000\n",
            "Training Epoch: 16 [25600/50000]\tLoss: 1.2094\tLR: 0.010000\n",
            "Training Epoch: 16 [25728/50000]\tLoss: 0.9551\tLR: 0.010000\n",
            "Training Epoch: 16 [25856/50000]\tLoss: 0.9882\tLR: 0.010000\n",
            "Training Epoch: 16 [25984/50000]\tLoss: 1.0729\tLR: 0.010000\n",
            "Training Epoch: 16 [26112/50000]\tLoss: 0.9989\tLR: 0.010000\n",
            "Training Epoch: 16 [26240/50000]\tLoss: 0.9187\tLR: 0.010000\n",
            "Training Epoch: 16 [26368/50000]\tLoss: 1.0062\tLR: 0.010000\n",
            "Training Epoch: 16 [26496/50000]\tLoss: 0.8974\tLR: 0.010000\n",
            "Training Epoch: 16 [26624/50000]\tLoss: 0.7791\tLR: 0.010000\n",
            "Training Epoch: 16 [26752/50000]\tLoss: 1.2026\tLR: 0.010000\n",
            "Training Epoch: 16 [26880/50000]\tLoss: 0.7599\tLR: 0.010000\n",
            "Training Epoch: 16 [27008/50000]\tLoss: 1.1373\tLR: 0.010000\n",
            "Training Epoch: 16 [27136/50000]\tLoss: 0.7674\tLR: 0.010000\n",
            "Training Epoch: 16 [27264/50000]\tLoss: 0.9663\tLR: 0.010000\n",
            "Training Epoch: 16 [27392/50000]\tLoss: 0.9394\tLR: 0.010000\n",
            "Training Epoch: 16 [27520/50000]\tLoss: 0.7305\tLR: 0.010000\n",
            "Training Epoch: 16 [27648/50000]\tLoss: 0.8212\tLR: 0.010000\n",
            "Training Epoch: 16 [27776/50000]\tLoss: 0.9603\tLR: 0.010000\n",
            "Training Epoch: 16 [27904/50000]\tLoss: 1.0185\tLR: 0.010000\n",
            "Training Epoch: 16 [28032/50000]\tLoss: 0.9822\tLR: 0.010000\n",
            "Training Epoch: 16 [28160/50000]\tLoss: 1.0770\tLR: 0.010000\n",
            "Training Epoch: 16 [28288/50000]\tLoss: 0.7395\tLR: 0.010000\n",
            "Training Epoch: 16 [28416/50000]\tLoss: 1.1445\tLR: 0.010000\n",
            "Training Epoch: 16 [28544/50000]\tLoss: 0.8918\tLR: 0.010000\n",
            "Training Epoch: 16 [28672/50000]\tLoss: 0.7713\tLR: 0.010000\n",
            "Training Epoch: 16 [28800/50000]\tLoss: 0.9660\tLR: 0.010000\n",
            "Training Epoch: 16 [28928/50000]\tLoss: 1.0505\tLR: 0.010000\n",
            "Training Epoch: 16 [29056/50000]\tLoss: 1.0536\tLR: 0.010000\n",
            "Training Epoch: 16 [29184/50000]\tLoss: 1.0706\tLR: 0.010000\n",
            "Training Epoch: 16 [29312/50000]\tLoss: 1.0839\tLR: 0.010000\n",
            "Training Epoch: 16 [29440/50000]\tLoss: 1.0540\tLR: 0.010000\n",
            "Training Epoch: 16 [29568/50000]\tLoss: 0.9496\tLR: 0.010000\n",
            "Training Epoch: 16 [29696/50000]\tLoss: 0.9373\tLR: 0.010000\n",
            "Training Epoch: 16 [29824/50000]\tLoss: 0.9631\tLR: 0.010000\n",
            "Training Epoch: 16 [29952/50000]\tLoss: 1.0514\tLR: 0.010000\n",
            "Training Epoch: 16 [30080/50000]\tLoss: 1.1723\tLR: 0.010000\n",
            "Training Epoch: 16 [30208/50000]\tLoss: 0.9380\tLR: 0.010000\n",
            "Training Epoch: 16 [30336/50000]\tLoss: 0.8100\tLR: 0.010000\n",
            "Training Epoch: 16 [30464/50000]\tLoss: 0.9075\tLR: 0.010000\n",
            "Training Epoch: 16 [30592/50000]\tLoss: 0.9266\tLR: 0.010000\n",
            "Training Epoch: 16 [30720/50000]\tLoss: 0.8911\tLR: 0.010000\n",
            "Training Epoch: 16 [30848/50000]\tLoss: 0.9666\tLR: 0.010000\n",
            "Training Epoch: 16 [30976/50000]\tLoss: 1.1959\tLR: 0.010000\n",
            "Training Epoch: 16 [31104/50000]\tLoss: 0.8884\tLR: 0.010000\n",
            "Training Epoch: 16 [31232/50000]\tLoss: 0.8955\tLR: 0.010000\n",
            "Training Epoch: 16 [31360/50000]\tLoss: 0.8311\tLR: 0.010000\n",
            "Training Epoch: 16 [31488/50000]\tLoss: 1.0203\tLR: 0.010000\n",
            "Training Epoch: 16 [31616/50000]\tLoss: 1.2610\tLR: 0.010000\n",
            "Training Epoch: 16 [31744/50000]\tLoss: 1.1783\tLR: 0.010000\n",
            "Training Epoch: 16 [31872/50000]\tLoss: 1.0075\tLR: 0.010000\n",
            "Training Epoch: 16 [32000/50000]\tLoss: 0.9889\tLR: 0.010000\n",
            "Training Epoch: 16 [32128/50000]\tLoss: 0.8133\tLR: 0.010000\n",
            "Training Epoch: 16 [32256/50000]\tLoss: 0.8568\tLR: 0.010000\n",
            "Training Epoch: 16 [32384/50000]\tLoss: 0.6703\tLR: 0.010000\n",
            "Training Epoch: 16 [32512/50000]\tLoss: 0.9481\tLR: 0.010000\n",
            "Training Epoch: 16 [32640/50000]\tLoss: 0.9869\tLR: 0.010000\n",
            "Training Epoch: 16 [32768/50000]\tLoss: 0.9485\tLR: 0.010000\n",
            "Training Epoch: 16 [32896/50000]\tLoss: 0.9414\tLR: 0.010000\n",
            "Training Epoch: 16 [33024/50000]\tLoss: 0.9971\tLR: 0.010000\n",
            "Training Epoch: 16 [33152/50000]\tLoss: 0.9276\tLR: 0.010000\n",
            "Training Epoch: 16 [33280/50000]\tLoss: 1.0636\tLR: 0.010000\n",
            "Training Epoch: 16 [33408/50000]\tLoss: 0.8477\tLR: 0.010000\n",
            "Training Epoch: 16 [33536/50000]\tLoss: 0.7492\tLR: 0.010000\n",
            "Training Epoch: 16 [33664/50000]\tLoss: 1.0539\tLR: 0.010000\n",
            "Training Epoch: 16 [33792/50000]\tLoss: 0.9886\tLR: 0.010000\n",
            "Training Epoch: 16 [33920/50000]\tLoss: 0.9713\tLR: 0.010000\n",
            "Training Epoch: 16 [34048/50000]\tLoss: 1.0990\tLR: 0.010000\n",
            "Training Epoch: 16 [34176/50000]\tLoss: 0.9696\tLR: 0.010000\n",
            "Training Epoch: 16 [34304/50000]\tLoss: 0.9713\tLR: 0.010000\n",
            "Training Epoch: 16 [34432/50000]\tLoss: 1.1795\tLR: 0.010000\n",
            "Training Epoch: 16 [34560/50000]\tLoss: 0.9241\tLR: 0.010000\n",
            "Training Epoch: 16 [34688/50000]\tLoss: 0.8959\tLR: 0.010000\n",
            "Training Epoch: 16 [34816/50000]\tLoss: 0.9923\tLR: 0.010000\n",
            "Training Epoch: 16 [34944/50000]\tLoss: 1.0610\tLR: 0.010000\n",
            "Training Epoch: 16 [35072/50000]\tLoss: 0.8230\tLR: 0.010000\n",
            "Training Epoch: 16 [35200/50000]\tLoss: 1.0260\tLR: 0.010000\n",
            "Training Epoch: 16 [35328/50000]\tLoss: 0.8905\tLR: 0.010000\n",
            "Training Epoch: 16 [35456/50000]\tLoss: 0.9120\tLR: 0.010000\n",
            "Training Epoch: 16 [35584/50000]\tLoss: 0.9575\tLR: 0.010000\n",
            "Training Epoch: 16 [35712/50000]\tLoss: 0.8045\tLR: 0.010000\n",
            "Training Epoch: 16 [35840/50000]\tLoss: 0.9619\tLR: 0.010000\n",
            "Training Epoch: 16 [35968/50000]\tLoss: 1.2302\tLR: 0.010000\n",
            "Training Epoch: 16 [36096/50000]\tLoss: 1.1666\tLR: 0.010000\n",
            "Training Epoch: 16 [36224/50000]\tLoss: 1.0779\tLR: 0.010000\n",
            "Training Epoch: 16 [36352/50000]\tLoss: 0.9848\tLR: 0.010000\n",
            "Training Epoch: 16 [36480/50000]\tLoss: 1.0997\tLR: 0.010000\n",
            "Training Epoch: 16 [36608/50000]\tLoss: 1.0261\tLR: 0.010000\n",
            "Training Epoch: 16 [36736/50000]\tLoss: 0.9154\tLR: 0.010000\n",
            "Training Epoch: 16 [36864/50000]\tLoss: 0.8268\tLR: 0.010000\n",
            "Training Epoch: 16 [36992/50000]\tLoss: 1.0538\tLR: 0.010000\n",
            "Training Epoch: 16 [37120/50000]\tLoss: 1.1069\tLR: 0.010000\n",
            "Training Epoch: 16 [37248/50000]\tLoss: 0.8854\tLR: 0.010000\n",
            "Training Epoch: 16 [37376/50000]\tLoss: 0.8199\tLR: 0.010000\n",
            "Training Epoch: 16 [37504/50000]\tLoss: 0.8038\tLR: 0.010000\n",
            "Training Epoch: 16 [37632/50000]\tLoss: 1.1314\tLR: 0.010000\n",
            "Training Epoch: 16 [37760/50000]\tLoss: 1.1194\tLR: 0.010000\n",
            "Training Epoch: 16 [37888/50000]\tLoss: 1.1176\tLR: 0.010000\n",
            "Training Epoch: 16 [38016/50000]\tLoss: 1.1179\tLR: 0.010000\n",
            "Training Epoch: 16 [38144/50000]\tLoss: 0.9600\tLR: 0.010000\n",
            "Training Epoch: 16 [38272/50000]\tLoss: 0.8383\tLR: 0.010000\n",
            "Training Epoch: 16 [38400/50000]\tLoss: 1.0010\tLR: 0.010000\n",
            "Training Epoch: 16 [38528/50000]\tLoss: 0.9316\tLR: 0.010000\n",
            "Training Epoch: 16 [38656/50000]\tLoss: 0.9573\tLR: 0.010000\n",
            "Training Epoch: 16 [38784/50000]\tLoss: 0.8950\tLR: 0.010000\n",
            "Training Epoch: 16 [38912/50000]\tLoss: 1.1569\tLR: 0.010000\n",
            "Training Epoch: 16 [39040/50000]\tLoss: 0.7665\tLR: 0.010000\n",
            "Training Epoch: 16 [39168/50000]\tLoss: 1.0355\tLR: 0.010000\n",
            "Training Epoch: 16 [39296/50000]\tLoss: 0.8727\tLR: 0.010000\n",
            "Training Epoch: 16 [39424/50000]\tLoss: 1.0802\tLR: 0.010000\n",
            "Training Epoch: 16 [39552/50000]\tLoss: 0.9901\tLR: 0.010000\n",
            "Training Epoch: 16 [39680/50000]\tLoss: 0.9754\tLR: 0.010000\n",
            "Training Epoch: 16 [39808/50000]\tLoss: 1.0523\tLR: 0.010000\n",
            "Training Epoch: 16 [39936/50000]\tLoss: 1.1267\tLR: 0.010000\n",
            "Training Epoch: 16 [40064/50000]\tLoss: 0.9869\tLR: 0.010000\n",
            "Training Epoch: 16 [40192/50000]\tLoss: 0.8669\tLR: 0.010000\n",
            "Training Epoch: 16 [40320/50000]\tLoss: 1.1138\tLR: 0.010000\n",
            "Training Epoch: 16 [40448/50000]\tLoss: 0.7191\tLR: 0.010000\n",
            "Training Epoch: 16 [40576/50000]\tLoss: 0.8988\tLR: 0.010000\n",
            "Training Epoch: 16 [40704/50000]\tLoss: 1.0614\tLR: 0.010000\n",
            "Training Epoch: 16 [40832/50000]\tLoss: 0.9357\tLR: 0.010000\n",
            "Training Epoch: 16 [40960/50000]\tLoss: 0.9905\tLR: 0.010000\n",
            "Training Epoch: 16 [41088/50000]\tLoss: 0.8757\tLR: 0.010000\n",
            "Training Epoch: 16 [41216/50000]\tLoss: 0.9755\tLR: 0.010000\n",
            "Training Epoch: 16 [41344/50000]\tLoss: 0.9036\tLR: 0.010000\n",
            "Training Epoch: 16 [41472/50000]\tLoss: 0.7719\tLR: 0.010000\n",
            "Training Epoch: 16 [41600/50000]\tLoss: 0.9512\tLR: 0.010000\n",
            "Training Epoch: 16 [41728/50000]\tLoss: 1.0749\tLR: 0.010000\n",
            "Training Epoch: 16 [41856/50000]\tLoss: 1.0627\tLR: 0.010000\n",
            "Training Epoch: 16 [41984/50000]\tLoss: 0.9286\tLR: 0.010000\n",
            "Training Epoch: 16 [42112/50000]\tLoss: 0.9049\tLR: 0.010000\n",
            "Training Epoch: 16 [42240/50000]\tLoss: 1.1187\tLR: 0.010000\n",
            "Training Epoch: 16 [42368/50000]\tLoss: 1.0440\tLR: 0.010000\n",
            "Training Epoch: 16 [42496/50000]\tLoss: 0.8948\tLR: 0.010000\n",
            "Training Epoch: 16 [42624/50000]\tLoss: 0.9684\tLR: 0.010000\n",
            "Training Epoch: 16 [42752/50000]\tLoss: 1.0444\tLR: 0.010000\n",
            "Training Epoch: 16 [42880/50000]\tLoss: 0.9160\tLR: 0.010000\n",
            "Training Epoch: 16 [43008/50000]\tLoss: 0.8506\tLR: 0.010000\n",
            "Training Epoch: 16 [43136/50000]\tLoss: 0.9103\tLR: 0.010000\n",
            "Training Epoch: 16 [43264/50000]\tLoss: 1.0557\tLR: 0.010000\n",
            "Training Epoch: 16 [43392/50000]\tLoss: 1.1019\tLR: 0.010000\n",
            "Training Epoch: 16 [43520/50000]\tLoss: 0.8973\tLR: 0.010000\n",
            "Training Epoch: 16 [43648/50000]\tLoss: 1.1259\tLR: 0.010000\n",
            "Training Epoch: 16 [43776/50000]\tLoss: 1.1035\tLR: 0.010000\n",
            "Training Epoch: 16 [43904/50000]\tLoss: 0.8342\tLR: 0.010000\n",
            "Training Epoch: 16 [44032/50000]\tLoss: 0.7586\tLR: 0.010000\n",
            "Training Epoch: 16 [44160/50000]\tLoss: 0.9170\tLR: 0.010000\n",
            "Training Epoch: 16 [44288/50000]\tLoss: 1.1144\tLR: 0.010000\n",
            "Training Epoch: 16 [44416/50000]\tLoss: 0.8767\tLR: 0.010000\n",
            "Training Epoch: 16 [44544/50000]\tLoss: 1.1267\tLR: 0.010000\n",
            "Training Epoch: 16 [44672/50000]\tLoss: 0.9992\tLR: 0.010000\n",
            "Training Epoch: 16 [44800/50000]\tLoss: 1.0067\tLR: 0.010000\n",
            "Training Epoch: 16 [44928/50000]\tLoss: 0.9155\tLR: 0.010000\n",
            "Training Epoch: 16 [45056/50000]\tLoss: 0.8934\tLR: 0.010000\n",
            "Training Epoch: 16 [45184/50000]\tLoss: 0.9939\tLR: 0.010000\n",
            "Training Epoch: 16 [45312/50000]\tLoss: 0.9444\tLR: 0.010000\n",
            "Training Epoch: 16 [45440/50000]\tLoss: 1.1429\tLR: 0.010000\n",
            "Training Epoch: 16 [45568/50000]\tLoss: 0.9890\tLR: 0.010000\n",
            "Training Epoch: 16 [45696/50000]\tLoss: 0.9213\tLR: 0.010000\n",
            "Training Epoch: 16 [45824/50000]\tLoss: 0.7924\tLR: 0.010000\n",
            "Training Epoch: 16 [45952/50000]\tLoss: 1.0111\tLR: 0.010000\n",
            "Training Epoch: 16 [46080/50000]\tLoss: 1.0219\tLR: 0.010000\n",
            "Training Epoch: 16 [46208/50000]\tLoss: 1.3003\tLR: 0.010000\n",
            "Training Epoch: 16 [46336/50000]\tLoss: 0.9979\tLR: 0.010000\n",
            "Training Epoch: 16 [46464/50000]\tLoss: 0.8335\tLR: 0.010000\n",
            "Training Epoch: 16 [46592/50000]\tLoss: 1.0247\tLR: 0.010000\n",
            "Training Epoch: 16 [46720/50000]\tLoss: 0.9617\tLR: 0.010000\n",
            "Training Epoch: 16 [46848/50000]\tLoss: 1.0596\tLR: 0.010000\n",
            "Training Epoch: 16 [46976/50000]\tLoss: 1.1112\tLR: 0.010000\n",
            "Training Epoch: 16 [47104/50000]\tLoss: 1.0945\tLR: 0.010000\n",
            "Training Epoch: 16 [47232/50000]\tLoss: 1.0628\tLR: 0.010000\n",
            "Training Epoch: 16 [47360/50000]\tLoss: 1.0580\tLR: 0.010000\n",
            "Training Epoch: 16 [47488/50000]\tLoss: 1.0756\tLR: 0.010000\n",
            "Training Epoch: 16 [47616/50000]\tLoss: 1.0320\tLR: 0.010000\n",
            "Training Epoch: 16 [47744/50000]\tLoss: 0.9564\tLR: 0.010000\n",
            "Training Epoch: 16 [47872/50000]\tLoss: 0.7650\tLR: 0.010000\n",
            "Training Epoch: 16 [48000/50000]\tLoss: 0.9961\tLR: 0.010000\n",
            "Training Epoch: 16 [48128/50000]\tLoss: 1.0920\tLR: 0.010000\n",
            "Training Epoch: 16 [48256/50000]\tLoss: 0.9062\tLR: 0.010000\n",
            "Training Epoch: 16 [48384/50000]\tLoss: 0.8502\tLR: 0.010000\n",
            "Training Epoch: 16 [48512/50000]\tLoss: 1.0872\tLR: 0.010000\n",
            "Training Epoch: 16 [48640/50000]\tLoss: 0.9384\tLR: 0.010000\n",
            "Training Epoch: 16 [48768/50000]\tLoss: 0.8396\tLR: 0.010000\n",
            "Training Epoch: 16 [48896/50000]\tLoss: 0.8436\tLR: 0.010000\n",
            "Training Epoch: 16 [49024/50000]\tLoss: 0.8460\tLR: 0.010000\n",
            "Training Epoch: 16 [49152/50000]\tLoss: 1.1001\tLR: 0.010000\n",
            "Training Epoch: 16 [49280/50000]\tLoss: 1.1218\tLR: 0.010000\n",
            "Training Epoch: 16 [49408/50000]\tLoss: 0.9558\tLR: 0.010000\n",
            "Training Epoch: 16 [49536/50000]\tLoss: 0.9650\tLR: 0.010000\n",
            "Training Epoch: 16 [49664/50000]\tLoss: 0.9170\tLR: 0.010000\n",
            "Training Epoch: 16 [49792/50000]\tLoss: 0.9268\tLR: 0.010000\n",
            "Training Epoch: 16 [49920/50000]\tLoss: 0.7349\tLR: 0.010000\n",
            "Training Epoch: 16 [50000/50000]\tLoss: 0.9209\tLR: 0.010000\n",
            "epoch 16 training time consumed: 205.92s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 134401 GiB | 134401 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 134304 GiB | 134304 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     97 GiB |     97 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 134401 GiB | 134401 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 134304 GiB | 134304 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |     97 GiB |     97 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 134388 GiB | 134387 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 134291 GiB | 134290 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |     96 GiB |     96 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB |  97603 GiB |  97602 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB |  97502 GiB |  97502 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |    100 GiB |    100 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    5336 K  |    5335 K  |\n",
            "|       from large pool |      92    |     184    |    3008 K  |    3008 K  |\n",
            "|       from small pool |     519    |     646    |    2327 K  |    2327 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    5336 K  |    5335 K  |\n",
            "|       from large pool |      92    |     184    |    3008 K  |    3008 K  |\n",
            "|       from small pool |     519    |     646    |    2327 K  |    2327 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |     122    |    2251 K  |    2251 K  |\n",
            "|       from large pool |      42    |     111    |    1776 K  |    1776 K  |\n",
            "|       from small pool |       9    |      21    |     475 K  |     475 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0109, Accuracy: 0.6262, Time consumed:11.28s\n",
            "\n",
            "Training Epoch: 17 [128/50000]\tLoss: 0.8306\tLR: 0.010000\n",
            "Training Epoch: 17 [256/50000]\tLoss: 0.7691\tLR: 0.010000\n",
            "Training Epoch: 17 [384/50000]\tLoss: 0.7617\tLR: 0.010000\n",
            "Training Epoch: 17 [512/50000]\tLoss: 0.9656\tLR: 0.010000\n",
            "Training Epoch: 17 [640/50000]\tLoss: 0.6787\tLR: 0.010000\n",
            "Training Epoch: 17 [768/50000]\tLoss: 0.9631\tLR: 0.010000\n",
            "Training Epoch: 17 [896/50000]\tLoss: 0.7555\tLR: 0.010000\n",
            "Training Epoch: 17 [1024/50000]\tLoss: 0.8441\tLR: 0.010000\n",
            "Training Epoch: 17 [1152/50000]\tLoss: 0.8156\tLR: 0.010000\n",
            "Training Epoch: 17 [1280/50000]\tLoss: 0.8259\tLR: 0.010000\n",
            "Training Epoch: 17 [1408/50000]\tLoss: 0.8077\tLR: 0.010000\n",
            "Training Epoch: 17 [1536/50000]\tLoss: 0.9772\tLR: 0.010000\n",
            "Training Epoch: 17 [1664/50000]\tLoss: 0.7753\tLR: 0.010000\n",
            "Training Epoch: 17 [1792/50000]\tLoss: 0.8517\tLR: 0.010000\n",
            "Training Epoch: 17 [1920/50000]\tLoss: 0.7957\tLR: 0.010000\n",
            "Training Epoch: 17 [2048/50000]\tLoss: 0.8205\tLR: 0.010000\n",
            "Training Epoch: 17 [2176/50000]\tLoss: 0.8668\tLR: 0.010000\n",
            "Training Epoch: 17 [2304/50000]\tLoss: 0.9225\tLR: 0.010000\n",
            "Training Epoch: 17 [2432/50000]\tLoss: 0.7924\tLR: 0.010000\n",
            "Training Epoch: 17 [2560/50000]\tLoss: 0.9168\tLR: 0.010000\n",
            "Training Epoch: 17 [2688/50000]\tLoss: 0.7411\tLR: 0.010000\n",
            "Training Epoch: 17 [2816/50000]\tLoss: 0.7773\tLR: 0.010000\n",
            "Training Epoch: 17 [2944/50000]\tLoss: 0.7753\tLR: 0.010000\n",
            "Training Epoch: 17 [3072/50000]\tLoss: 0.7670\tLR: 0.010000\n",
            "Training Epoch: 17 [3200/50000]\tLoss: 0.8697\tLR: 0.010000\n",
            "Training Epoch: 17 [3328/50000]\tLoss: 0.7401\tLR: 0.010000\n",
            "Training Epoch: 17 [3456/50000]\tLoss: 1.0689\tLR: 0.010000\n",
            "Training Epoch: 17 [3584/50000]\tLoss: 0.8266\tLR: 0.010000\n",
            "Training Epoch: 17 [3712/50000]\tLoss: 0.8747\tLR: 0.010000\n",
            "Training Epoch: 17 [3840/50000]\tLoss: 0.8814\tLR: 0.010000\n",
            "Training Epoch: 17 [3968/50000]\tLoss: 0.8993\tLR: 0.010000\n",
            "Training Epoch: 17 [4096/50000]\tLoss: 0.6815\tLR: 0.010000\n",
            "Training Epoch: 17 [4224/50000]\tLoss: 0.9643\tLR: 0.010000\n",
            "Training Epoch: 17 [4352/50000]\tLoss: 0.7804\tLR: 0.010000\n",
            "Training Epoch: 17 [4480/50000]\tLoss: 0.9470\tLR: 0.010000\n",
            "Training Epoch: 17 [4608/50000]\tLoss: 0.8227\tLR: 0.010000\n",
            "Training Epoch: 17 [4736/50000]\tLoss: 0.8517\tLR: 0.010000\n",
            "Training Epoch: 17 [4864/50000]\tLoss: 0.7649\tLR: 0.010000\n",
            "Training Epoch: 17 [4992/50000]\tLoss: 0.9568\tLR: 0.010000\n",
            "Training Epoch: 17 [5120/50000]\tLoss: 0.6271\tLR: 0.010000\n",
            "Training Epoch: 17 [5248/50000]\tLoss: 0.6967\tLR: 0.010000\n",
            "Training Epoch: 17 [5376/50000]\tLoss: 0.7366\tLR: 0.010000\n",
            "Training Epoch: 17 [5504/50000]\tLoss: 0.5741\tLR: 0.010000\n",
            "Training Epoch: 17 [5632/50000]\tLoss: 0.9954\tLR: 0.010000\n",
            "Training Epoch: 17 [5760/50000]\tLoss: 0.9699\tLR: 0.010000\n",
            "Training Epoch: 17 [5888/50000]\tLoss: 0.6901\tLR: 0.010000\n",
            "Training Epoch: 17 [6016/50000]\tLoss: 0.9092\tLR: 0.010000\n",
            "Training Epoch: 17 [6144/50000]\tLoss: 0.9777\tLR: 0.010000\n",
            "Training Epoch: 17 [6272/50000]\tLoss: 0.9475\tLR: 0.010000\n",
            "Training Epoch: 17 [6400/50000]\tLoss: 0.9888\tLR: 0.010000\n",
            "Training Epoch: 17 [6528/50000]\tLoss: 0.8108\tLR: 0.010000\n",
            "Training Epoch: 17 [6656/50000]\tLoss: 0.9646\tLR: 0.010000\n",
            "Training Epoch: 17 [6784/50000]\tLoss: 0.7742\tLR: 0.010000\n",
            "Training Epoch: 17 [6912/50000]\tLoss: 0.9356\tLR: 0.010000\n",
            "Training Epoch: 17 [7040/50000]\tLoss: 0.8056\tLR: 0.010000\n",
            "Training Epoch: 17 [7168/50000]\tLoss: 0.8988\tLR: 0.010000\n",
            "Training Epoch: 17 [7296/50000]\tLoss: 0.8400\tLR: 0.010000\n",
            "Training Epoch: 17 [7424/50000]\tLoss: 0.5964\tLR: 0.010000\n",
            "Training Epoch: 17 [7552/50000]\tLoss: 0.7215\tLR: 0.010000\n",
            "Training Epoch: 17 [7680/50000]\tLoss: 0.8917\tLR: 0.010000\n",
            "Training Epoch: 17 [7808/50000]\tLoss: 0.7746\tLR: 0.010000\n",
            "Training Epoch: 17 [7936/50000]\tLoss: 0.8536\tLR: 0.010000\n",
            "Training Epoch: 17 [8064/50000]\tLoss: 0.8230\tLR: 0.010000\n",
            "Training Epoch: 17 [8192/50000]\tLoss: 1.1455\tLR: 0.010000\n",
            "Training Epoch: 17 [8320/50000]\tLoss: 0.6950\tLR: 0.010000\n",
            "Training Epoch: 17 [8448/50000]\tLoss: 0.5427\tLR: 0.010000\n",
            "Training Epoch: 17 [8576/50000]\tLoss: 0.9125\tLR: 0.010000\n",
            "Training Epoch: 17 [8704/50000]\tLoss: 0.5537\tLR: 0.010000\n",
            "Training Epoch: 17 [8832/50000]\tLoss: 0.7616\tLR: 0.010000\n",
            "Training Epoch: 17 [8960/50000]\tLoss: 0.8487\tLR: 0.010000\n",
            "Training Epoch: 17 [9088/50000]\tLoss: 1.0774\tLR: 0.010000\n",
            "Training Epoch: 17 [9216/50000]\tLoss: 0.9627\tLR: 0.010000\n",
            "Training Epoch: 17 [9344/50000]\tLoss: 0.9834\tLR: 0.010000\n",
            "Training Epoch: 17 [9472/50000]\tLoss: 0.8853\tLR: 0.010000\n",
            "Training Epoch: 17 [9600/50000]\tLoss: 0.8120\tLR: 0.010000\n",
            "Training Epoch: 17 [9728/50000]\tLoss: 0.8331\tLR: 0.010000\n",
            "Training Epoch: 17 [9856/50000]\tLoss: 0.8480\tLR: 0.010000\n",
            "Training Epoch: 17 [9984/50000]\tLoss: 0.9410\tLR: 0.010000\n",
            "Training Epoch: 17 [10112/50000]\tLoss: 0.9403\tLR: 0.010000\n",
            "Training Epoch: 17 [10240/50000]\tLoss: 0.9154\tLR: 0.010000\n",
            "Training Epoch: 17 [10368/50000]\tLoss: 0.8148\tLR: 0.010000\n",
            "Training Epoch: 17 [10496/50000]\tLoss: 0.9188\tLR: 0.010000\n",
            "Training Epoch: 17 [10624/50000]\tLoss: 0.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [10752/50000]\tLoss: 1.0238\tLR: 0.010000\n",
            "Training Epoch: 17 [10880/50000]\tLoss: 0.9352\tLR: 0.010000\n",
            "Training Epoch: 17 [11008/50000]\tLoss: 0.8266\tLR: 0.010000\n",
            "Training Epoch: 17 [11136/50000]\tLoss: 0.8049\tLR: 0.010000\n",
            "Training Epoch: 17 [11264/50000]\tLoss: 0.7795\tLR: 0.010000\n",
            "Training Epoch: 17 [11392/50000]\tLoss: 0.7796\tLR: 0.010000\n",
            "Training Epoch: 17 [11520/50000]\tLoss: 1.0242\tLR: 0.010000\n",
            "Training Epoch: 17 [11648/50000]\tLoss: 0.9096\tLR: 0.010000\n",
            "Training Epoch: 17 [11776/50000]\tLoss: 0.7915\tLR: 0.010000\n",
            "Training Epoch: 17 [11904/50000]\tLoss: 0.8798\tLR: 0.010000\n",
            "Training Epoch: 17 [12032/50000]\tLoss: 0.8540\tLR: 0.010000\n",
            "Training Epoch: 17 [12160/50000]\tLoss: 0.8978\tLR: 0.010000\n",
            "Training Epoch: 17 [12288/50000]\tLoss: 0.8818\tLR: 0.010000\n",
            "Training Epoch: 17 [12416/50000]\tLoss: 0.8577\tLR: 0.010000\n",
            "Training Epoch: 17 [12544/50000]\tLoss: 0.7646\tLR: 0.010000\n",
            "Training Epoch: 17 [12672/50000]\tLoss: 0.7417\tLR: 0.010000\n",
            "Training Epoch: 17 [12800/50000]\tLoss: 0.9939\tLR: 0.010000\n",
            "Training Epoch: 17 [12928/50000]\tLoss: 0.8712\tLR: 0.010000\n",
            "Training Epoch: 17 [13056/50000]\tLoss: 0.9549\tLR: 0.010000\n",
            "Training Epoch: 17 [13184/50000]\tLoss: 1.0077\tLR: 0.010000\n",
            "Training Epoch: 17 [13312/50000]\tLoss: 0.8845\tLR: 0.010000\n",
            "Training Epoch: 17 [13440/50000]\tLoss: 0.8910\tLR: 0.010000\n",
            "Training Epoch: 17 [13568/50000]\tLoss: 0.8363\tLR: 0.010000\n",
            "Training Epoch: 17 [13696/50000]\tLoss: 0.9202\tLR: 0.010000\n",
            "Training Epoch: 17 [13824/50000]\tLoss: 0.8136\tLR: 0.010000\n",
            "Training Epoch: 17 [13952/50000]\tLoss: 1.1092\tLR: 0.010000\n",
            "Training Epoch: 17 [14080/50000]\tLoss: 0.7578\tLR: 0.010000\n",
            "Training Epoch: 17 [14208/50000]\tLoss: 0.8901\tLR: 0.010000\n",
            "Training Epoch: 17 [14336/50000]\tLoss: 0.7648\tLR: 0.010000\n",
            "Training Epoch: 17 [14464/50000]\tLoss: 0.8676\tLR: 0.010000\n",
            "Training Epoch: 17 [14592/50000]\tLoss: 0.9306\tLR: 0.010000\n",
            "Training Epoch: 17 [14720/50000]\tLoss: 0.7632\tLR: 0.010000\n",
            "Training Epoch: 17 [14848/50000]\tLoss: 1.1011\tLR: 0.010000\n",
            "Training Epoch: 17 [14976/50000]\tLoss: 0.7558\tLR: 0.010000\n",
            "Training Epoch: 17 [15104/50000]\tLoss: 0.8400\tLR: 0.010000\n",
            "Training Epoch: 17 [15232/50000]\tLoss: 1.0169\tLR: 0.010000\n",
            "Training Epoch: 17 [15360/50000]\tLoss: 0.8584\tLR: 0.010000\n",
            "Training Epoch: 17 [15488/50000]\tLoss: 0.8641\tLR: 0.010000\n",
            "Training Epoch: 17 [15616/50000]\tLoss: 1.0435\tLR: 0.010000\n",
            "Training Epoch: 17 [15744/50000]\tLoss: 1.0553\tLR: 0.010000\n",
            "Training Epoch: 17 [15872/50000]\tLoss: 0.9165\tLR: 0.010000\n",
            "Training Epoch: 17 [16000/50000]\tLoss: 0.9166\tLR: 0.010000\n",
            "Training Epoch: 17 [16128/50000]\tLoss: 0.9192\tLR: 0.010000\n",
            "Training Epoch: 17 [16256/50000]\tLoss: 0.9368\tLR: 0.010000\n",
            "Training Epoch: 17 [16384/50000]\tLoss: 0.9667\tLR: 0.010000\n",
            "Training Epoch: 17 [16512/50000]\tLoss: 0.8752\tLR: 0.010000\n",
            "Training Epoch: 17 [16640/50000]\tLoss: 0.8582\tLR: 0.010000\n",
            "Training Epoch: 17 [16768/50000]\tLoss: 0.8647\tLR: 0.010000\n",
            "Training Epoch: 17 [16896/50000]\tLoss: 0.8928\tLR: 0.010000\n",
            "Training Epoch: 17 [17024/50000]\tLoss: 1.0157\tLR: 0.010000\n",
            "Training Epoch: 17 [17152/50000]\tLoss: 1.1139\tLR: 0.010000\n",
            "Training Epoch: 17 [17280/50000]\tLoss: 1.0929\tLR: 0.010000\n",
            "Training Epoch: 17 [17408/50000]\tLoss: 0.9030\tLR: 0.010000\n",
            "Training Epoch: 17 [17536/50000]\tLoss: 0.7940\tLR: 0.010000\n",
            "Training Epoch: 17 [17664/50000]\tLoss: 0.9676\tLR: 0.010000\n",
            "Training Epoch: 17 [17792/50000]\tLoss: 0.9474\tLR: 0.010000\n",
            "Training Epoch: 17 [17920/50000]\tLoss: 0.9762\tLR: 0.010000\n",
            "Training Epoch: 17 [18048/50000]\tLoss: 0.9175\tLR: 0.010000\n",
            "Training Epoch: 17 [18176/50000]\tLoss: 0.7145\tLR: 0.010000\n",
            "Training Epoch: 17 [18304/50000]\tLoss: 0.9318\tLR: 0.010000\n",
            "Training Epoch: 17 [18432/50000]\tLoss: 0.8081\tLR: 0.010000\n",
            "Training Epoch: 17 [18560/50000]\tLoss: 0.9721\tLR: 0.010000\n",
            "Training Epoch: 17 [18688/50000]\tLoss: 0.9645\tLR: 0.010000\n",
            "Training Epoch: 17 [18816/50000]\tLoss: 1.0007\tLR: 0.010000\n",
            "Training Epoch: 17 [18944/50000]\tLoss: 0.9878\tLR: 0.010000\n",
            "Training Epoch: 17 [19072/50000]\tLoss: 0.8549\tLR: 0.010000\n",
            "Training Epoch: 17 [19200/50000]\tLoss: 0.8321\tLR: 0.010000\n",
            "Training Epoch: 17 [19328/50000]\tLoss: 0.8756\tLR: 0.010000\n",
            "Training Epoch: 17 [19456/50000]\tLoss: 0.9749\tLR: 0.010000\n",
            "Training Epoch: 17 [19584/50000]\tLoss: 0.7412\tLR: 0.010000\n",
            "Training Epoch: 17 [19712/50000]\tLoss: 0.9835\tLR: 0.010000\n",
            "Training Epoch: 17 [19840/50000]\tLoss: 0.7522\tLR: 0.010000\n",
            "Training Epoch: 17 [19968/50000]\tLoss: 0.7757\tLR: 0.010000\n",
            "Training Epoch: 17 [20096/50000]\tLoss: 0.9934\tLR: 0.010000\n",
            "Training Epoch: 17 [20224/50000]\tLoss: 0.9794\tLR: 0.010000\n",
            "Training Epoch: 17 [20352/50000]\tLoss: 0.7970\tLR: 0.010000\n",
            "Training Epoch: 17 [20480/50000]\tLoss: 1.0530\tLR: 0.010000\n",
            "Training Epoch: 17 [20608/50000]\tLoss: 0.9593\tLR: 0.010000\n",
            "Training Epoch: 17 [20736/50000]\tLoss: 0.8423\tLR: 0.010000\n",
            "Training Epoch: 17 [20864/50000]\tLoss: 0.8836\tLR: 0.010000\n",
            "Training Epoch: 17 [20992/50000]\tLoss: 1.0350\tLR: 0.010000\n",
            "Training Epoch: 17 [21120/50000]\tLoss: 0.9585\tLR: 0.010000\n",
            "Training Epoch: 17 [21248/50000]\tLoss: 0.8683\tLR: 0.010000\n",
            "Training Epoch: 17 [21376/50000]\tLoss: 0.7746\tLR: 0.010000\n",
            "Training Epoch: 17 [21504/50000]\tLoss: 0.8867\tLR: 0.010000\n",
            "Training Epoch: 17 [21632/50000]\tLoss: 0.8773\tLR: 0.010000\n",
            "Training Epoch: 17 [21760/50000]\tLoss: 0.8798\tLR: 0.010000\n",
            "Training Epoch: 17 [21888/50000]\tLoss: 0.9071\tLR: 0.010000\n",
            "Training Epoch: 17 [22016/50000]\tLoss: 0.9630\tLR: 0.010000\n",
            "Training Epoch: 17 [22144/50000]\tLoss: 0.9765\tLR: 0.010000\n",
            "Training Epoch: 17 [22272/50000]\tLoss: 0.7151\tLR: 0.010000\n",
            "Training Epoch: 17 [22400/50000]\tLoss: 1.0375\tLR: 0.010000\n",
            "Training Epoch: 17 [22528/50000]\tLoss: 0.8678\tLR: 0.010000\n",
            "Training Epoch: 17 [22656/50000]\tLoss: 0.9390\tLR: 0.010000\n",
            "Training Epoch: 17 [22784/50000]\tLoss: 0.8086\tLR: 0.010000\n",
            "Training Epoch: 17 [22912/50000]\tLoss: 0.7576\tLR: 0.010000\n",
            "Training Epoch: 17 [23040/50000]\tLoss: 0.9315\tLR: 0.010000\n",
            "Training Epoch: 17 [23168/50000]\tLoss: 0.8442\tLR: 0.010000\n",
            "Training Epoch: 17 [23296/50000]\tLoss: 0.9432\tLR: 0.010000\n",
            "Training Epoch: 17 [23424/50000]\tLoss: 0.7314\tLR: 0.010000\n",
            "Training Epoch: 17 [23552/50000]\tLoss: 0.8504\tLR: 0.010000\n",
            "Training Epoch: 17 [23680/50000]\tLoss: 0.9166\tLR: 0.010000\n",
            "Training Epoch: 17 [23808/50000]\tLoss: 0.9456\tLR: 0.010000\n",
            "Training Epoch: 17 [23936/50000]\tLoss: 1.1422\tLR: 0.010000\n",
            "Training Epoch: 17 [24064/50000]\tLoss: 0.8839\tLR: 0.010000\n",
            "Training Epoch: 17 [24192/50000]\tLoss: 0.9662\tLR: 0.010000\n",
            "Training Epoch: 17 [24320/50000]\tLoss: 0.6679\tLR: 0.010000\n",
            "Training Epoch: 17 [24448/50000]\tLoss: 1.1318\tLR: 0.010000\n",
            "Training Epoch: 17 [24576/50000]\tLoss: 0.8075\tLR: 0.010000\n",
            "Training Epoch: 17 [24704/50000]\tLoss: 0.8193\tLR: 0.010000\n",
            "Training Epoch: 17 [24832/50000]\tLoss: 0.8728\tLR: 0.010000\n",
            "Training Epoch: 17 [24960/50000]\tLoss: 0.7581\tLR: 0.010000\n",
            "Training Epoch: 17 [25088/50000]\tLoss: 1.0508\tLR: 0.010000\n",
            "Training Epoch: 17 [25216/50000]\tLoss: 0.8681\tLR: 0.010000\n",
            "Training Epoch: 17 [25344/50000]\tLoss: 0.7980\tLR: 0.010000\n",
            "Training Epoch: 17 [25472/50000]\tLoss: 0.9979\tLR: 0.010000\n",
            "Training Epoch: 17 [25600/50000]\tLoss: 0.8166\tLR: 0.010000\n",
            "Training Epoch: 17 [25728/50000]\tLoss: 0.9850\tLR: 0.010000\n",
            "Training Epoch: 17 [25856/50000]\tLoss: 1.0022\tLR: 0.010000\n",
            "Training Epoch: 17 [25984/50000]\tLoss: 0.9792\tLR: 0.010000\n",
            "Training Epoch: 17 [26112/50000]\tLoss: 1.0466\tLR: 0.010000\n",
            "Training Epoch: 17 [26240/50000]\tLoss: 0.7535\tLR: 0.010000\n",
            "Training Epoch: 17 [26368/50000]\tLoss: 0.9877\tLR: 0.010000\n",
            "Training Epoch: 17 [26496/50000]\tLoss: 1.0451\tLR: 0.010000\n",
            "Training Epoch: 17 [26624/50000]\tLoss: 0.9448\tLR: 0.010000\n",
            "Training Epoch: 17 [26752/50000]\tLoss: 1.0253\tLR: 0.010000\n",
            "Training Epoch: 17 [26880/50000]\tLoss: 1.0132\tLR: 0.010000\n",
            "Training Epoch: 17 [27008/50000]\tLoss: 1.0867\tLR: 0.010000\n",
            "Training Epoch: 17 [27136/50000]\tLoss: 0.7436\tLR: 0.010000\n",
            "Training Epoch: 17 [27264/50000]\tLoss: 1.0948\tLR: 0.010000\n",
            "Training Epoch: 17 [27392/50000]\tLoss: 0.7271\tLR: 0.010000\n",
            "Training Epoch: 17 [27520/50000]\tLoss: 1.1547\tLR: 0.010000\n",
            "Training Epoch: 17 [27648/50000]\tLoss: 0.7339\tLR: 0.010000\n",
            "Training Epoch: 17 [27776/50000]\tLoss: 0.8874\tLR: 0.010000\n",
            "Training Epoch: 17 [27904/50000]\tLoss: 1.0223\tLR: 0.010000\n",
            "Training Epoch: 17 [28032/50000]\tLoss: 0.7616\tLR: 0.010000\n",
            "Training Epoch: 17 [28160/50000]\tLoss: 0.9441\tLR: 0.010000\n",
            "Training Epoch: 17 [28288/50000]\tLoss: 1.0943\tLR: 0.010000\n",
            "Training Epoch: 17 [28416/50000]\tLoss: 0.7966\tLR: 0.010000\n",
            "Training Epoch: 17 [28544/50000]\tLoss: 0.9510\tLR: 0.010000\n",
            "Training Epoch: 17 [28672/50000]\tLoss: 0.7053\tLR: 0.010000\n",
            "Training Epoch: 17 [28800/50000]\tLoss: 0.8567\tLR: 0.010000\n",
            "Training Epoch: 17 [28928/50000]\tLoss: 0.9612\tLR: 0.010000\n",
            "Training Epoch: 17 [29056/50000]\tLoss: 0.9541\tLR: 0.010000\n",
            "Training Epoch: 17 [29184/50000]\tLoss: 0.8915\tLR: 0.010000\n",
            "Training Epoch: 17 [29312/50000]\tLoss: 1.0755\tLR: 0.010000\n",
            "Training Epoch: 17 [29440/50000]\tLoss: 1.1102\tLR: 0.010000\n",
            "Training Epoch: 17 [29568/50000]\tLoss: 0.9638\tLR: 0.010000\n",
            "Training Epoch: 17 [29696/50000]\tLoss: 1.1402\tLR: 0.010000\n",
            "Training Epoch: 17 [29824/50000]\tLoss: 1.0795\tLR: 0.010000\n",
            "Training Epoch: 17 [29952/50000]\tLoss: 0.9522\tLR: 0.010000\n",
            "Training Epoch: 17 [30080/50000]\tLoss: 0.8675\tLR: 0.010000\n",
            "Training Epoch: 17 [30208/50000]\tLoss: 0.8354\tLR: 0.010000\n",
            "Training Epoch: 17 [30336/50000]\tLoss: 0.9461\tLR: 0.010000\n",
            "Training Epoch: 17 [30464/50000]\tLoss: 0.8689\tLR: 0.010000\n",
            "Training Epoch: 17 [30592/50000]\tLoss: 0.8134\tLR: 0.010000\n",
            "Training Epoch: 17 [30720/50000]\tLoss: 0.9110\tLR: 0.010000\n",
            "Training Epoch: 17 [30848/50000]\tLoss: 0.9928\tLR: 0.010000\n",
            "Training Epoch: 17 [30976/50000]\tLoss: 0.7236\tLR: 0.010000\n",
            "Training Epoch: 17 [31104/50000]\tLoss: 1.0107\tLR: 0.010000\n",
            "Training Epoch: 17 [31232/50000]\tLoss: 1.0597\tLR: 0.010000\n",
            "Training Epoch: 17 [31360/50000]\tLoss: 0.7021\tLR: 0.010000\n",
            "Training Epoch: 17 [31488/50000]\tLoss: 0.9688\tLR: 0.010000\n",
            "Training Epoch: 17 [31616/50000]\tLoss: 0.9679\tLR: 0.010000\n",
            "Training Epoch: 17 [31744/50000]\tLoss: 0.7542\tLR: 0.010000\n",
            "Training Epoch: 17 [31872/50000]\tLoss: 0.9005\tLR: 0.010000\n",
            "Training Epoch: 17 [32000/50000]\tLoss: 0.9139\tLR: 0.010000\n",
            "Training Epoch: 17 [32128/50000]\tLoss: 0.8484\tLR: 0.010000\n",
            "Training Epoch: 17 [32256/50000]\tLoss: 0.9558\tLR: 0.010000\n",
            "Training Epoch: 17 [32384/50000]\tLoss: 0.9587\tLR: 0.010000\n",
            "Training Epoch: 17 [32512/50000]\tLoss: 0.8671\tLR: 0.010000\n",
            "Training Epoch: 17 [32640/50000]\tLoss: 0.8027\tLR: 0.010000\n",
            "Training Epoch: 17 [32768/50000]\tLoss: 0.7695\tLR: 0.010000\n",
            "Training Epoch: 17 [32896/50000]\tLoss: 0.8275\tLR: 0.010000\n",
            "Training Epoch: 17 [33024/50000]\tLoss: 0.9018\tLR: 0.010000\n",
            "Training Epoch: 17 [33152/50000]\tLoss: 0.7231\tLR: 0.010000\n",
            "Training Epoch: 17 [33280/50000]\tLoss: 0.7988\tLR: 0.010000\n",
            "Training Epoch: 17 [33408/50000]\tLoss: 1.0827\tLR: 0.010000\n",
            "Training Epoch: 17 [33536/50000]\tLoss: 0.8857\tLR: 0.010000\n",
            "Training Epoch: 17 [33664/50000]\tLoss: 0.7857\tLR: 0.010000\n",
            "Training Epoch: 17 [33792/50000]\tLoss: 0.9458\tLR: 0.010000\n",
            "Training Epoch: 17 [33920/50000]\tLoss: 0.8440\tLR: 0.010000\n",
            "Training Epoch: 17 [34048/50000]\tLoss: 0.8522\tLR: 0.010000\n",
            "Training Epoch: 17 [34176/50000]\tLoss: 0.8451\tLR: 0.010000\n",
            "Training Epoch: 17 [34304/50000]\tLoss: 0.9133\tLR: 0.010000\n",
            "Training Epoch: 17 [34432/50000]\tLoss: 0.8958\tLR: 0.010000\n",
            "Training Epoch: 17 [34560/50000]\tLoss: 0.9701\tLR: 0.010000\n",
            "Training Epoch: 17 [34688/50000]\tLoss: 1.0446\tLR: 0.010000\n",
            "Training Epoch: 17 [34816/50000]\tLoss: 1.1522\tLR: 0.010000\n",
            "Training Epoch: 17 [34944/50000]\tLoss: 1.1470\tLR: 0.010000\n",
            "Training Epoch: 17 [35072/50000]\tLoss: 0.8958\tLR: 0.010000\n",
            "Training Epoch: 17 [35200/50000]\tLoss: 0.9699\tLR: 0.010000\n",
            "Training Epoch: 17 [35328/50000]\tLoss: 0.9342\tLR: 0.010000\n",
            "Training Epoch: 17 [35456/50000]\tLoss: 0.9583\tLR: 0.010000\n",
            "Training Epoch: 17 [35584/50000]\tLoss: 0.8185\tLR: 0.010000\n",
            "Training Epoch: 17 [35712/50000]\tLoss: 0.7634\tLR: 0.010000\n",
            "Training Epoch: 17 [35840/50000]\tLoss: 0.8234\tLR: 0.010000\n",
            "Training Epoch: 17 [35968/50000]\tLoss: 0.8025\tLR: 0.010000\n",
            "Training Epoch: 17 [36096/50000]\tLoss: 1.0439\tLR: 0.010000\n",
            "Training Epoch: 17 [36224/50000]\tLoss: 0.7053\tLR: 0.010000\n",
            "Training Epoch: 17 [36352/50000]\tLoss: 0.8553\tLR: 0.010000\n",
            "Training Epoch: 17 [36480/50000]\tLoss: 1.0145\tLR: 0.010000\n",
            "Training Epoch: 17 [36608/50000]\tLoss: 0.7337\tLR: 0.010000\n",
            "Training Epoch: 17 [36736/50000]\tLoss: 0.9984\tLR: 0.010000\n",
            "Training Epoch: 17 [36864/50000]\tLoss: 1.0589\tLR: 0.010000\n",
            "Training Epoch: 17 [36992/50000]\tLoss: 0.8728\tLR: 0.010000\n",
            "Training Epoch: 17 [37120/50000]\tLoss: 0.9620\tLR: 0.010000\n",
            "Training Epoch: 17 [37248/50000]\tLoss: 0.8711\tLR: 0.010000\n",
            "Training Epoch: 17 [37376/50000]\tLoss: 0.7890\tLR: 0.010000\n",
            "Training Epoch: 17 [37504/50000]\tLoss: 0.6834\tLR: 0.010000\n",
            "Training Epoch: 17 [37632/50000]\tLoss: 0.9868\tLR: 0.010000\n",
            "Training Epoch: 17 [37760/50000]\tLoss: 1.0699\tLR: 0.010000\n",
            "Training Epoch: 17 [37888/50000]\tLoss: 1.1135\tLR: 0.010000\n",
            "Training Epoch: 17 [38016/50000]\tLoss: 0.9706\tLR: 0.010000\n",
            "Training Epoch: 17 [38144/50000]\tLoss: 0.9526\tLR: 0.010000\n",
            "Training Epoch: 17 [38272/50000]\tLoss: 1.0599\tLR: 0.010000\n",
            "Training Epoch: 17 [38400/50000]\tLoss: 0.9039\tLR: 0.010000\n",
            "Training Epoch: 17 [38528/50000]\tLoss: 0.9882\tLR: 0.010000\n",
            "Training Epoch: 17 [38656/50000]\tLoss: 0.8863\tLR: 0.010000\n",
            "Training Epoch: 17 [38784/50000]\tLoss: 0.8384\tLR: 0.010000\n",
            "Training Epoch: 17 [38912/50000]\tLoss: 0.8806\tLR: 0.010000\n",
            "Training Epoch: 17 [39040/50000]\tLoss: 0.9436\tLR: 0.010000\n",
            "Training Epoch: 17 [39168/50000]\tLoss: 1.2950\tLR: 0.010000\n",
            "Training Epoch: 17 [39296/50000]\tLoss: 0.8932\tLR: 0.010000\n",
            "Training Epoch: 17 [39424/50000]\tLoss: 0.7746\tLR: 0.010000\n",
            "Training Epoch: 17 [39552/50000]\tLoss: 0.7117\tLR: 0.010000\n",
            "Training Epoch: 17 [39680/50000]\tLoss: 0.7795\tLR: 0.010000\n",
            "Training Epoch: 17 [39808/50000]\tLoss: 0.8306\tLR: 0.010000\n",
            "Training Epoch: 17 [39936/50000]\tLoss: 0.7283\tLR: 0.010000\n",
            "Training Epoch: 17 [40064/50000]\tLoss: 1.0366\tLR: 0.010000\n",
            "Training Epoch: 17 [40192/50000]\tLoss: 0.9536\tLR: 0.010000\n",
            "Training Epoch: 17 [40320/50000]\tLoss: 0.9972\tLR: 0.010000\n",
            "Training Epoch: 17 [40448/50000]\tLoss: 1.2132\tLR: 0.010000\n",
            "Training Epoch: 17 [40576/50000]\tLoss: 0.9135\tLR: 0.010000\n",
            "Training Epoch: 17 [40704/50000]\tLoss: 0.8577\tLR: 0.010000\n",
            "Training Epoch: 17 [40832/50000]\tLoss: 0.9843\tLR: 0.010000\n",
            "Training Epoch: 17 [40960/50000]\tLoss: 1.0310\tLR: 0.010000\n",
            "Training Epoch: 17 [41088/50000]\tLoss: 0.8895\tLR: 0.010000\n",
            "Training Epoch: 17 [41216/50000]\tLoss: 1.0938\tLR: 0.010000\n",
            "Training Epoch: 17 [41344/50000]\tLoss: 0.9202\tLR: 0.010000\n",
            "Training Epoch: 17 [41472/50000]\tLoss: 0.9445\tLR: 0.010000\n",
            "Training Epoch: 17 [41600/50000]\tLoss: 0.9490\tLR: 0.010000\n",
            "Training Epoch: 17 [41728/50000]\tLoss: 0.9644\tLR: 0.010000\n",
            "Training Epoch: 17 [41856/50000]\tLoss: 0.9405\tLR: 0.010000\n",
            "Training Epoch: 17 [41984/50000]\tLoss: 0.9159\tLR: 0.010000\n",
            "Training Epoch: 17 [42112/50000]\tLoss: 0.7295\tLR: 0.010000\n",
            "Training Epoch: 17 [42240/50000]\tLoss: 0.9486\tLR: 0.010000\n",
            "Training Epoch: 17 [42368/50000]\tLoss: 1.0125\tLR: 0.010000\n",
            "Training Epoch: 17 [42496/50000]\tLoss: 0.9500\tLR: 0.010000\n",
            "Training Epoch: 17 [42624/50000]\tLoss: 0.9387\tLR: 0.010000\n",
            "Training Epoch: 17 [42752/50000]\tLoss: 0.9260\tLR: 0.010000\n",
            "Training Epoch: 17 [42880/50000]\tLoss: 0.8459\tLR: 0.010000\n",
            "Training Epoch: 17 [43008/50000]\tLoss: 1.1150\tLR: 0.010000\n",
            "Training Epoch: 17 [43136/50000]\tLoss: 0.9297\tLR: 0.010000\n",
            "Training Epoch: 17 [43264/50000]\tLoss: 0.6577\tLR: 0.010000\n",
            "Training Epoch: 17 [43392/50000]\tLoss: 1.0199\tLR: 0.010000\n",
            "Training Epoch: 17 [43520/50000]\tLoss: 1.0685\tLR: 0.010000\n",
            "Training Epoch: 17 [43648/50000]\tLoss: 0.7854\tLR: 0.010000\n",
            "Training Epoch: 17 [43776/50000]\tLoss: 0.9032\tLR: 0.010000\n",
            "Training Epoch: 17 [43904/50000]\tLoss: 0.9954\tLR: 0.010000\n",
            "Training Epoch: 17 [44032/50000]\tLoss: 0.7573\tLR: 0.010000\n",
            "Training Epoch: 17 [44160/50000]\tLoss: 0.7774\tLR: 0.010000\n",
            "Training Epoch: 17 [44288/50000]\tLoss: 1.0538\tLR: 0.010000\n",
            "Training Epoch: 17 [44416/50000]\tLoss: 0.8407\tLR: 0.010000\n",
            "Training Epoch: 17 [44544/50000]\tLoss: 1.1796\tLR: 0.010000\n",
            "Training Epoch: 17 [44672/50000]\tLoss: 0.8298\tLR: 0.010000\n",
            "Training Epoch: 17 [44800/50000]\tLoss: 0.8774\tLR: 0.010000\n",
            "Training Epoch: 17 [44928/50000]\tLoss: 0.9967\tLR: 0.010000\n",
            "Training Epoch: 17 [45056/50000]\tLoss: 0.9259\tLR: 0.010000\n",
            "Training Epoch: 17 [45184/50000]\tLoss: 1.1376\tLR: 0.010000\n",
            "Training Epoch: 17 [45312/50000]\tLoss: 1.1215\tLR: 0.010000\n",
            "Training Epoch: 17 [45440/50000]\tLoss: 1.0439\tLR: 0.010000\n",
            "Training Epoch: 17 [45568/50000]\tLoss: 0.8554\tLR: 0.010000\n",
            "Training Epoch: 17 [45696/50000]\tLoss: 1.0927\tLR: 0.010000\n",
            "Training Epoch: 17 [45824/50000]\tLoss: 0.8092\tLR: 0.010000\n",
            "Training Epoch: 17 [45952/50000]\tLoss: 0.9667\tLR: 0.010000\n",
            "Training Epoch: 17 [46080/50000]\tLoss: 0.7150\tLR: 0.010000\n",
            "Training Epoch: 17 [46208/50000]\tLoss: 0.8316\tLR: 0.010000\n",
            "Training Epoch: 17 [46336/50000]\tLoss: 0.8231\tLR: 0.010000\n",
            "Training Epoch: 17 [46464/50000]\tLoss: 0.8741\tLR: 0.010000\n",
            "Training Epoch: 17 [46592/50000]\tLoss: 0.9203\tLR: 0.010000\n",
            "Training Epoch: 17 [46720/50000]\tLoss: 0.9646\tLR: 0.010000\n",
            "Training Epoch: 17 [46848/50000]\tLoss: 1.0117\tLR: 0.010000\n",
            "Training Epoch: 17 [46976/50000]\tLoss: 1.0526\tLR: 0.010000\n",
            "Training Epoch: 17 [47104/50000]\tLoss: 0.8893\tLR: 0.010000\n",
            "Training Epoch: 17 [47232/50000]\tLoss: 0.9452\tLR: 0.010000\n",
            "Training Epoch: 17 [47360/50000]\tLoss: 0.9631\tLR: 0.010000\n",
            "Training Epoch: 17 [47488/50000]\tLoss: 0.7996\tLR: 0.010000\n",
            "Training Epoch: 17 [47616/50000]\tLoss: 0.8541\tLR: 0.010000\n",
            "Training Epoch: 17 [47744/50000]\tLoss: 0.8944\tLR: 0.010000\n",
            "Training Epoch: 17 [47872/50000]\tLoss: 0.8036\tLR: 0.010000\n",
            "Training Epoch: 17 [48000/50000]\tLoss: 0.8271\tLR: 0.010000\n",
            "Training Epoch: 17 [48128/50000]\tLoss: 0.7800\tLR: 0.010000\n",
            "Training Epoch: 17 [48256/50000]\tLoss: 0.8346\tLR: 0.010000\n",
            "Training Epoch: 17 [48384/50000]\tLoss: 0.9793\tLR: 0.010000\n",
            "Training Epoch: 17 [48512/50000]\tLoss: 1.0500\tLR: 0.010000\n",
            "Training Epoch: 17 [48640/50000]\tLoss: 0.9012\tLR: 0.010000\n",
            "Training Epoch: 17 [48768/50000]\tLoss: 0.9132\tLR: 0.010000\n",
            "Training Epoch: 17 [48896/50000]\tLoss: 0.8262\tLR: 0.010000\n",
            "Training Epoch: 17 [49024/50000]\tLoss: 0.8341\tLR: 0.010000\n",
            "Training Epoch: 17 [49152/50000]\tLoss: 0.9136\tLR: 0.010000\n",
            "Training Epoch: 17 [49280/50000]\tLoss: 0.9089\tLR: 0.010000\n",
            "Training Epoch: 17 [49408/50000]\tLoss: 0.9419\tLR: 0.010000\n",
            "Training Epoch: 17 [49536/50000]\tLoss: 0.8770\tLR: 0.010000\n",
            "Training Epoch: 17 [49664/50000]\tLoss: 0.7119\tLR: 0.010000\n",
            "Training Epoch: 17 [49792/50000]\tLoss: 1.1730\tLR: 0.010000\n",
            "Training Epoch: 17 [49920/50000]\tLoss: 0.9397\tLR: 0.010000\n",
            "Training Epoch: 17 [50000/50000]\tLoss: 0.9163\tLR: 0.010000\n",
            "epoch 17 training time consumed: 206.76s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 142800 GiB | 142800 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 142697 GiB | 142697 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    103 GiB |    103 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 142800 GiB | 142800 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 142697 GiB | 142697 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    103 GiB |    103 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 142786 GiB | 142785 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 142683 GiB | 142682 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |    103 GiB |    103 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB | 103703 GiB | 103702 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB | 103596 GiB | 103596 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |    106 GiB |    106 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    5669 K  |    5669 K  |\n",
            "|       from large pool |      92    |     184    |    3196 K  |    3196 K  |\n",
            "|       from small pool |     519    |     646    |    2472 K  |    2472 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    5669 K  |    5669 K  |\n",
            "|       from large pool |      92    |     184    |    3196 K  |    3196 K  |\n",
            "|       from small pool |     519    |     646    |    2472 K  |    2472 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |     122    |    2392 K  |    2392 K  |\n",
            "|       from large pool |      42    |     111    |    1887 K  |    1887 K  |\n",
            "|       from small pool |       7    |      21    |     505 K  |     505 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0107, Accuracy: 0.6278, Time consumed:11.44s\n",
            "\n",
            "Training Epoch: 18 [128/50000]\tLoss: 0.8873\tLR: 0.010000\n",
            "Training Epoch: 18 [256/50000]\tLoss: 0.8741\tLR: 0.010000\n",
            "Training Epoch: 18 [384/50000]\tLoss: 0.5660\tLR: 0.010000\n",
            "Training Epoch: 18 [512/50000]\tLoss: 0.7957\tLR: 0.010000\n",
            "Training Epoch: 18 [640/50000]\tLoss: 0.8830\tLR: 0.010000\n",
            "Training Epoch: 18 [768/50000]\tLoss: 0.8118\tLR: 0.010000\n",
            "Training Epoch: 18 [896/50000]\tLoss: 0.6460\tLR: 0.010000\n",
            "Training Epoch: 18 [1024/50000]\tLoss: 0.7141\tLR: 0.010000\n",
            "Training Epoch: 18 [1152/50000]\tLoss: 0.8510\tLR: 0.010000\n",
            "Training Epoch: 18 [1280/50000]\tLoss: 0.7336\tLR: 0.010000\n",
            "Training Epoch: 18 [1408/50000]\tLoss: 0.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [1536/50000]\tLoss: 0.9069\tLR: 0.010000\n",
            "Training Epoch: 18 [1664/50000]\tLoss: 0.5907\tLR: 0.010000\n",
            "Training Epoch: 18 [1792/50000]\tLoss: 0.8434\tLR: 0.010000\n",
            "Training Epoch: 18 [1920/50000]\tLoss: 0.6437\tLR: 0.010000\n",
            "Training Epoch: 18 [2048/50000]\tLoss: 0.6751\tLR: 0.010000\n",
            "Training Epoch: 18 [2176/50000]\tLoss: 0.8773\tLR: 0.010000\n",
            "Training Epoch: 18 [2304/50000]\tLoss: 0.7733\tLR: 0.010000\n",
            "Training Epoch: 18 [2432/50000]\tLoss: 0.7392\tLR: 0.010000\n",
            "Training Epoch: 18 [2560/50000]\tLoss: 0.7838\tLR: 0.010000\n",
            "Training Epoch: 18 [2688/50000]\tLoss: 0.7616\tLR: 0.010000\n",
            "Training Epoch: 18 [2816/50000]\tLoss: 0.8026\tLR: 0.010000\n",
            "Training Epoch: 18 [2944/50000]\tLoss: 0.8875\tLR: 0.010000\n",
            "Training Epoch: 18 [3072/50000]\tLoss: 0.5958\tLR: 0.010000\n",
            "Training Epoch: 18 [3200/50000]\tLoss: 0.7301\tLR: 0.010000\n",
            "Training Epoch: 18 [3328/50000]\tLoss: 0.8183\tLR: 0.010000\n",
            "Training Epoch: 18 [3456/50000]\tLoss: 0.7241\tLR: 0.010000\n",
            "Training Epoch: 18 [3584/50000]\tLoss: 0.8493\tLR: 0.010000\n",
            "Training Epoch: 18 [3712/50000]\tLoss: 1.0164\tLR: 0.010000\n",
            "Training Epoch: 18 [3840/50000]\tLoss: 0.7419\tLR: 0.010000\n",
            "Training Epoch: 18 [3968/50000]\tLoss: 1.0531\tLR: 0.010000\n",
            "Training Epoch: 18 [4096/50000]\tLoss: 0.9563\tLR: 0.010000\n",
            "Training Epoch: 18 [4224/50000]\tLoss: 0.8280\tLR: 0.010000\n",
            "Training Epoch: 18 [4352/50000]\tLoss: 0.7401\tLR: 0.010000\n",
            "Training Epoch: 18 [4480/50000]\tLoss: 0.8163\tLR: 0.010000\n",
            "Training Epoch: 18 [4608/50000]\tLoss: 0.6031\tLR: 0.010000\n",
            "Training Epoch: 18 [4736/50000]\tLoss: 0.8253\tLR: 0.010000\n",
            "Training Epoch: 18 [4864/50000]\tLoss: 0.9828\tLR: 0.010000\n",
            "Training Epoch: 18 [4992/50000]\tLoss: 0.8580\tLR: 0.010000\n",
            "Training Epoch: 18 [5120/50000]\tLoss: 0.7650\tLR: 0.010000\n",
            "Training Epoch: 18 [5248/50000]\tLoss: 0.8177\tLR: 0.010000\n",
            "Training Epoch: 18 [5376/50000]\tLoss: 0.6653\tLR: 0.010000\n",
            "Training Epoch: 18 [5504/50000]\tLoss: 0.8024\tLR: 0.010000\n",
            "Training Epoch: 18 [5632/50000]\tLoss: 0.9731\tLR: 0.010000\n",
            "Training Epoch: 18 [5760/50000]\tLoss: 0.8275\tLR: 0.010000\n",
            "Training Epoch: 18 [5888/50000]\tLoss: 0.8671\tLR: 0.010000\n",
            "Training Epoch: 18 [6016/50000]\tLoss: 0.7707\tLR: 0.010000\n",
            "Training Epoch: 18 [6144/50000]\tLoss: 1.0019\tLR: 0.010000\n",
            "Training Epoch: 18 [6272/50000]\tLoss: 0.8208\tLR: 0.010000\n",
            "Training Epoch: 18 [6400/50000]\tLoss: 0.7658\tLR: 0.010000\n",
            "Training Epoch: 18 [6528/50000]\tLoss: 0.9785\tLR: 0.010000\n",
            "Training Epoch: 18 [6656/50000]\tLoss: 0.8667\tLR: 0.010000\n",
            "Training Epoch: 18 [6784/50000]\tLoss: 0.8290\tLR: 0.010000\n",
            "Training Epoch: 18 [6912/50000]\tLoss: 0.6944\tLR: 0.010000\n",
            "Training Epoch: 18 [7040/50000]\tLoss: 0.8880\tLR: 0.010000\n",
            "Training Epoch: 18 [7168/50000]\tLoss: 0.7683\tLR: 0.010000\n",
            "Training Epoch: 18 [7296/50000]\tLoss: 0.7344\tLR: 0.010000\n",
            "Training Epoch: 18 [7424/50000]\tLoss: 0.7473\tLR: 0.010000\n",
            "Training Epoch: 18 [7552/50000]\tLoss: 0.6965\tLR: 0.010000\n",
            "Training Epoch: 18 [7680/50000]\tLoss: 0.9321\tLR: 0.010000\n",
            "Training Epoch: 18 [7808/50000]\tLoss: 0.6193\tLR: 0.010000\n",
            "Training Epoch: 18 [7936/50000]\tLoss: 0.8862\tLR: 0.010000\n",
            "Training Epoch: 18 [8064/50000]\tLoss: 0.8861\tLR: 0.010000\n",
            "Training Epoch: 18 [8192/50000]\tLoss: 0.8263\tLR: 0.010000\n",
            "Training Epoch: 18 [8320/50000]\tLoss: 0.9037\tLR: 0.010000\n",
            "Training Epoch: 18 [8448/50000]\tLoss: 0.7719\tLR: 0.010000\n",
            "Training Epoch: 18 [8576/50000]\tLoss: 0.6414\tLR: 0.010000\n",
            "Training Epoch: 18 [8704/50000]\tLoss: 0.8694\tLR: 0.010000\n",
            "Training Epoch: 18 [8832/50000]\tLoss: 0.7947\tLR: 0.010000\n",
            "Training Epoch: 18 [8960/50000]\tLoss: 0.7322\tLR: 0.010000\n",
            "Training Epoch: 18 [9088/50000]\tLoss: 0.7523\tLR: 0.010000\n",
            "Training Epoch: 18 [9216/50000]\tLoss: 0.7135\tLR: 0.010000\n",
            "Training Epoch: 18 [9344/50000]\tLoss: 0.8670\tLR: 0.010000\n",
            "Training Epoch: 18 [9472/50000]\tLoss: 0.7448\tLR: 0.010000\n",
            "Training Epoch: 18 [9600/50000]\tLoss: 0.7796\tLR: 0.010000\n",
            "Training Epoch: 18 [9728/50000]\tLoss: 0.9305\tLR: 0.010000\n",
            "Training Epoch: 18 [9856/50000]\tLoss: 0.7923\tLR: 0.010000\n",
            "Training Epoch: 18 [9984/50000]\tLoss: 0.9351\tLR: 0.010000\n",
            "Training Epoch: 18 [10112/50000]\tLoss: 0.7046\tLR: 0.010000\n",
            "Training Epoch: 18 [10240/50000]\tLoss: 0.6878\tLR: 0.010000\n",
            "Training Epoch: 18 [10368/50000]\tLoss: 0.7845\tLR: 0.010000\n",
            "Training Epoch: 18 [10496/50000]\tLoss: 0.9683\tLR: 0.010000\n",
            "Training Epoch: 18 [10624/50000]\tLoss: 0.7064\tLR: 0.010000\n",
            "Training Epoch: 18 [10752/50000]\tLoss: 0.7208\tLR: 0.010000\n",
            "Training Epoch: 18 [10880/50000]\tLoss: 0.9271\tLR: 0.010000\n",
            "Training Epoch: 18 [11008/50000]\tLoss: 0.8426\tLR: 0.010000\n",
            "Training Epoch: 18 [11136/50000]\tLoss: 0.7635\tLR: 0.010000\n",
            "Training Epoch: 18 [11264/50000]\tLoss: 0.6833\tLR: 0.010000\n",
            "Training Epoch: 18 [11392/50000]\tLoss: 0.9160\tLR: 0.010000\n",
            "Training Epoch: 18 [11520/50000]\tLoss: 0.7865\tLR: 0.010000\n",
            "Training Epoch: 18 [11648/50000]\tLoss: 0.6293\tLR: 0.010000\n",
            "Training Epoch: 18 [11776/50000]\tLoss: 0.6646\tLR: 0.010000\n",
            "Training Epoch: 18 [11904/50000]\tLoss: 0.8623\tLR: 0.010000\n",
            "Training Epoch: 18 [12032/50000]\tLoss: 0.8652\tLR: 0.010000\n",
            "Training Epoch: 18 [12160/50000]\tLoss: 0.8998\tLR: 0.010000\n",
            "Training Epoch: 18 [12288/50000]\tLoss: 0.6829\tLR: 0.010000\n",
            "Training Epoch: 18 [12416/50000]\tLoss: 0.6526\tLR: 0.010000\n",
            "Training Epoch: 18 [12544/50000]\tLoss: 0.7966\tLR: 0.010000\n",
            "Training Epoch: 18 [12672/50000]\tLoss: 0.8998\tLR: 0.010000\n",
            "Training Epoch: 18 [12800/50000]\tLoss: 0.7312\tLR: 0.010000\n",
            "Training Epoch: 18 [12928/50000]\tLoss: 0.6869\tLR: 0.010000\n",
            "Training Epoch: 18 [13056/50000]\tLoss: 0.8986\tLR: 0.010000\n",
            "Training Epoch: 18 [13184/50000]\tLoss: 0.7468\tLR: 0.010000\n",
            "Training Epoch: 18 [13312/50000]\tLoss: 0.7092\tLR: 0.010000\n",
            "Training Epoch: 18 [13440/50000]\tLoss: 0.8554\tLR: 0.010000\n",
            "Training Epoch: 18 [13568/50000]\tLoss: 0.8614\tLR: 0.010000\n",
            "Training Epoch: 18 [13696/50000]\tLoss: 0.9214\tLR: 0.010000\n",
            "Training Epoch: 18 [13824/50000]\tLoss: 0.8434\tLR: 0.010000\n",
            "Training Epoch: 18 [13952/50000]\tLoss: 0.8414\tLR: 0.010000\n",
            "Training Epoch: 18 [14080/50000]\tLoss: 0.8977\tLR: 0.010000\n",
            "Training Epoch: 18 [14208/50000]\tLoss: 0.8713\tLR: 0.010000\n",
            "Training Epoch: 18 [14336/50000]\tLoss: 1.0573\tLR: 0.010000\n",
            "Training Epoch: 18 [14464/50000]\tLoss: 1.2674\tLR: 0.010000\n",
            "Training Epoch: 18 [14592/50000]\tLoss: 1.0606\tLR: 0.010000\n",
            "Training Epoch: 18 [14720/50000]\tLoss: 0.8066\tLR: 0.010000\n",
            "Training Epoch: 18 [14848/50000]\tLoss: 0.9509\tLR: 0.010000\n",
            "Training Epoch: 18 [14976/50000]\tLoss: 0.7335\tLR: 0.010000\n",
            "Training Epoch: 18 [15104/50000]\tLoss: 0.8194\tLR: 0.010000\n",
            "Training Epoch: 18 [15232/50000]\tLoss: 0.8792\tLR: 0.010000\n",
            "Training Epoch: 18 [15360/50000]\tLoss: 0.7402\tLR: 0.010000\n",
            "Training Epoch: 18 [15488/50000]\tLoss: 0.8169\tLR: 0.010000\n",
            "Training Epoch: 18 [15616/50000]\tLoss: 0.7453\tLR: 0.010000\n",
            "Training Epoch: 18 [15744/50000]\tLoss: 0.7034\tLR: 0.010000\n",
            "Training Epoch: 18 [15872/50000]\tLoss: 0.8466\tLR: 0.010000\n",
            "Training Epoch: 18 [16000/50000]\tLoss: 0.8202\tLR: 0.010000\n",
            "Training Epoch: 18 [16128/50000]\tLoss: 0.7344\tLR: 0.010000\n",
            "Training Epoch: 18 [16256/50000]\tLoss: 0.7040\tLR: 0.010000\n",
            "Training Epoch: 18 [16384/50000]\tLoss: 0.5982\tLR: 0.010000\n",
            "Training Epoch: 18 [16512/50000]\tLoss: 0.7911\tLR: 0.010000\n",
            "Training Epoch: 18 [16640/50000]\tLoss: 0.8186\tLR: 0.010000\n",
            "Training Epoch: 18 [16768/50000]\tLoss: 0.8338\tLR: 0.010000\n",
            "Training Epoch: 18 [16896/50000]\tLoss: 0.8281\tLR: 0.010000\n",
            "Training Epoch: 18 [17024/50000]\tLoss: 0.9879\tLR: 0.010000\n",
            "Training Epoch: 18 [17152/50000]\tLoss: 0.7139\tLR: 0.010000\n",
            "Training Epoch: 18 [17280/50000]\tLoss: 0.8887\tLR: 0.010000\n",
            "Training Epoch: 18 [17408/50000]\tLoss: 1.0433\tLR: 0.010000\n",
            "Training Epoch: 18 [17536/50000]\tLoss: 0.9297\tLR: 0.010000\n",
            "Training Epoch: 18 [17664/50000]\tLoss: 0.6146\tLR: 0.010000\n",
            "Training Epoch: 18 [17792/50000]\tLoss: 0.8390\tLR: 0.010000\n",
            "Training Epoch: 18 [17920/50000]\tLoss: 0.8129\tLR: 0.010000\n",
            "Training Epoch: 18 [18048/50000]\tLoss: 0.8092\tLR: 0.010000\n",
            "Training Epoch: 18 [18176/50000]\tLoss: 0.8352\tLR: 0.010000\n",
            "Training Epoch: 18 [18304/50000]\tLoss: 0.8209\tLR: 0.010000\n",
            "Training Epoch: 18 [18432/50000]\tLoss: 0.8029\tLR: 0.010000\n",
            "Training Epoch: 18 [18560/50000]\tLoss: 0.8579\tLR: 0.010000\n",
            "Training Epoch: 18 [18688/50000]\tLoss: 0.9267\tLR: 0.010000\n",
            "Training Epoch: 18 [18816/50000]\tLoss: 0.8988\tLR: 0.010000\n",
            "Training Epoch: 18 [18944/50000]\tLoss: 0.7389\tLR: 0.010000\n",
            "Training Epoch: 18 [19072/50000]\tLoss: 0.6545\tLR: 0.010000\n",
            "Training Epoch: 18 [19200/50000]\tLoss: 0.6625\tLR: 0.010000\n",
            "Training Epoch: 18 [19328/50000]\tLoss: 0.9217\tLR: 0.010000\n",
            "Training Epoch: 18 [19456/50000]\tLoss: 0.7212\tLR: 0.010000\n",
            "Training Epoch: 18 [19584/50000]\tLoss: 0.9390\tLR: 0.010000\n",
            "Training Epoch: 18 [19712/50000]\tLoss: 1.0196\tLR: 0.010000\n",
            "Training Epoch: 18 [19840/50000]\tLoss: 0.8095\tLR: 0.010000\n",
            "Training Epoch: 18 [19968/50000]\tLoss: 0.9407\tLR: 0.010000\n",
            "Training Epoch: 18 [20096/50000]\tLoss: 0.7701\tLR: 0.010000\n",
            "Training Epoch: 18 [20224/50000]\tLoss: 0.8288\tLR: 0.010000\n",
            "Training Epoch: 18 [20352/50000]\tLoss: 0.8131\tLR: 0.010000\n",
            "Training Epoch: 18 [20480/50000]\tLoss: 0.9526\tLR: 0.010000\n",
            "Training Epoch: 18 [20608/50000]\tLoss: 1.0076\tLR: 0.010000\n",
            "Training Epoch: 18 [20736/50000]\tLoss: 0.9565\tLR: 0.010000\n",
            "Training Epoch: 18 [20864/50000]\tLoss: 0.9084\tLR: 0.010000\n",
            "Training Epoch: 18 [20992/50000]\tLoss: 0.8519\tLR: 0.010000\n",
            "Training Epoch: 18 [21120/50000]\tLoss: 0.8327\tLR: 0.010000\n",
            "Training Epoch: 18 [21248/50000]\tLoss: 0.8845\tLR: 0.010000\n",
            "Training Epoch: 18 [21376/50000]\tLoss: 0.9992\tLR: 0.010000\n",
            "Training Epoch: 18 [21504/50000]\tLoss: 0.8964\tLR: 0.010000\n",
            "Training Epoch: 18 [21632/50000]\tLoss: 0.8558\tLR: 0.010000\n",
            "Training Epoch: 18 [21760/50000]\tLoss: 0.6785\tLR: 0.010000\n",
            "Training Epoch: 18 [21888/50000]\tLoss: 0.8401\tLR: 0.010000\n",
            "Training Epoch: 18 [22016/50000]\tLoss: 0.7570\tLR: 0.010000\n",
            "Training Epoch: 18 [22144/50000]\tLoss: 1.1570\tLR: 0.010000\n",
            "Training Epoch: 18 [22272/50000]\tLoss: 0.7480\tLR: 0.010000\n",
            "Training Epoch: 18 [22400/50000]\tLoss: 0.9885\tLR: 0.010000\n",
            "Training Epoch: 18 [22528/50000]\tLoss: 0.7886\tLR: 0.010000\n",
            "Training Epoch: 18 [22656/50000]\tLoss: 0.7350\tLR: 0.010000\n",
            "Training Epoch: 18 [22784/50000]\tLoss: 0.7106\tLR: 0.010000\n",
            "Training Epoch: 18 [22912/50000]\tLoss: 0.6207\tLR: 0.010000\n",
            "Training Epoch: 18 [23040/50000]\tLoss: 0.7105\tLR: 0.010000\n",
            "Training Epoch: 18 [23168/50000]\tLoss: 0.9938\tLR: 0.010000\n",
            "Training Epoch: 18 [23296/50000]\tLoss: 0.8800\tLR: 0.010000\n",
            "Training Epoch: 18 [23424/50000]\tLoss: 0.8943\tLR: 0.010000\n",
            "Training Epoch: 18 [23552/50000]\tLoss: 1.0302\tLR: 0.010000\n",
            "Training Epoch: 18 [23680/50000]\tLoss: 0.9120\tLR: 0.010000\n",
            "Training Epoch: 18 [23808/50000]\tLoss: 0.8567\tLR: 0.010000\n",
            "Training Epoch: 18 [23936/50000]\tLoss: 1.0376\tLR: 0.010000\n",
            "Training Epoch: 18 [24064/50000]\tLoss: 1.0258\tLR: 0.010000\n",
            "Training Epoch: 18 [24192/50000]\tLoss: 0.6926\tLR: 0.010000\n",
            "Training Epoch: 18 [24320/50000]\tLoss: 0.7047\tLR: 0.010000\n",
            "Training Epoch: 18 [24448/50000]\tLoss: 0.9520\tLR: 0.010000\n",
            "Training Epoch: 18 [24576/50000]\tLoss: 0.7600\tLR: 0.010000\n",
            "Training Epoch: 18 [24704/50000]\tLoss: 0.8776\tLR: 0.010000\n",
            "Training Epoch: 18 [24832/50000]\tLoss: 0.7787\tLR: 0.010000\n",
            "Training Epoch: 18 [24960/50000]\tLoss: 0.7743\tLR: 0.010000\n",
            "Training Epoch: 18 [25088/50000]\tLoss: 0.8357\tLR: 0.010000\n",
            "Training Epoch: 18 [25216/50000]\tLoss: 1.0140\tLR: 0.010000\n",
            "Training Epoch: 18 [25344/50000]\tLoss: 0.7819\tLR: 0.010000\n",
            "Training Epoch: 18 [25472/50000]\tLoss: 0.7397\tLR: 0.010000\n",
            "Training Epoch: 18 [25600/50000]\tLoss: 0.8636\tLR: 0.010000\n",
            "Training Epoch: 18 [25728/50000]\tLoss: 0.8103\tLR: 0.010000\n",
            "Training Epoch: 18 [25856/50000]\tLoss: 0.8645\tLR: 0.010000\n",
            "Training Epoch: 18 [25984/50000]\tLoss: 0.8461\tLR: 0.010000\n",
            "Training Epoch: 18 [26112/50000]\tLoss: 0.7245\tLR: 0.010000\n",
            "Training Epoch: 18 [26240/50000]\tLoss: 0.8476\tLR: 0.010000\n",
            "Training Epoch: 18 [26368/50000]\tLoss: 0.8154\tLR: 0.010000\n",
            "Training Epoch: 18 [26496/50000]\tLoss: 0.7792\tLR: 0.010000\n",
            "Training Epoch: 18 [26624/50000]\tLoss: 0.7272\tLR: 0.010000\n",
            "Training Epoch: 18 [26752/50000]\tLoss: 0.9890\tLR: 0.010000\n",
            "Training Epoch: 18 [26880/50000]\tLoss: 0.9365\tLR: 0.010000\n",
            "Training Epoch: 18 [27008/50000]\tLoss: 0.9133\tLR: 0.010000\n",
            "Training Epoch: 18 [27136/50000]\tLoss: 0.7366\tLR: 0.010000\n",
            "Training Epoch: 18 [27264/50000]\tLoss: 0.7697\tLR: 0.010000\n",
            "Training Epoch: 18 [27392/50000]\tLoss: 0.7180\tLR: 0.010000\n",
            "Training Epoch: 18 [27520/50000]\tLoss: 0.7543\tLR: 0.010000\n",
            "Training Epoch: 18 [27648/50000]\tLoss: 0.8404\tLR: 0.010000\n",
            "Training Epoch: 18 [27776/50000]\tLoss: 0.8280\tLR: 0.010000\n",
            "Training Epoch: 18 [27904/50000]\tLoss: 0.8428\tLR: 0.010000\n",
            "Training Epoch: 18 [28032/50000]\tLoss: 0.8141\tLR: 0.010000\n",
            "Training Epoch: 18 [28160/50000]\tLoss: 1.0179\tLR: 0.010000\n",
            "Training Epoch: 18 [28288/50000]\tLoss: 0.9241\tLR: 0.010000\n",
            "Training Epoch: 18 [28416/50000]\tLoss: 1.0179\tLR: 0.010000\n",
            "Training Epoch: 18 [28544/50000]\tLoss: 0.7209\tLR: 0.010000\n",
            "Training Epoch: 18 [28672/50000]\tLoss: 0.8475\tLR: 0.010000\n",
            "Training Epoch: 18 [28800/50000]\tLoss: 0.8905\tLR: 0.010000\n",
            "Training Epoch: 18 [28928/50000]\tLoss: 1.0327\tLR: 0.010000\n",
            "Training Epoch: 18 [29056/50000]\tLoss: 0.8008\tLR: 0.010000\n",
            "Training Epoch: 18 [29184/50000]\tLoss: 0.9651\tLR: 0.010000\n",
            "Training Epoch: 18 [29312/50000]\tLoss: 0.9532\tLR: 0.010000\n",
            "Training Epoch: 18 [29440/50000]\tLoss: 0.9331\tLR: 0.010000\n",
            "Training Epoch: 18 [29568/50000]\tLoss: 0.8773\tLR: 0.010000\n",
            "Training Epoch: 18 [29696/50000]\tLoss: 0.8300\tLR: 0.010000\n",
            "Training Epoch: 18 [29824/50000]\tLoss: 0.8289\tLR: 0.010000\n",
            "Training Epoch: 18 [29952/50000]\tLoss: 0.8281\tLR: 0.010000\n",
            "Training Epoch: 18 [30080/50000]\tLoss: 0.9535\tLR: 0.010000\n",
            "Training Epoch: 18 [30208/50000]\tLoss: 0.7780\tLR: 0.010000\n",
            "Training Epoch: 18 [30336/50000]\tLoss: 0.8072\tLR: 0.010000\n",
            "Training Epoch: 18 [30464/50000]\tLoss: 0.8849\tLR: 0.010000\n",
            "Training Epoch: 18 [30592/50000]\tLoss: 0.8339\tLR: 0.010000\n",
            "Training Epoch: 18 [30720/50000]\tLoss: 0.9924\tLR: 0.010000\n",
            "Training Epoch: 18 [30848/50000]\tLoss: 0.6224\tLR: 0.010000\n",
            "Training Epoch: 18 [30976/50000]\tLoss: 0.7979\tLR: 0.010000\n",
            "Training Epoch: 18 [31104/50000]\tLoss: 0.9237\tLR: 0.010000\n",
            "Training Epoch: 18 [31232/50000]\tLoss: 0.8265\tLR: 0.010000\n",
            "Training Epoch: 18 [31360/50000]\tLoss: 0.8281\tLR: 0.010000\n",
            "Training Epoch: 18 [31488/50000]\tLoss: 0.8444\tLR: 0.010000\n",
            "Training Epoch: 18 [31616/50000]\tLoss: 0.7757\tLR: 0.010000\n",
            "Training Epoch: 18 [31744/50000]\tLoss: 0.7532\tLR: 0.010000\n",
            "Training Epoch: 18 [31872/50000]\tLoss: 0.7565\tLR: 0.010000\n",
            "Training Epoch: 18 [32000/50000]\tLoss: 0.9299\tLR: 0.010000\n",
            "Training Epoch: 18 [32128/50000]\tLoss: 0.8789\tLR: 0.010000\n",
            "Training Epoch: 18 [32256/50000]\tLoss: 0.7259\tLR: 0.010000\n",
            "Training Epoch: 18 [32384/50000]\tLoss: 0.8479\tLR: 0.010000\n",
            "Training Epoch: 18 [32512/50000]\tLoss: 0.9545\tLR: 0.010000\n",
            "Training Epoch: 18 [32640/50000]\tLoss: 0.8363\tLR: 0.010000\n",
            "Training Epoch: 18 [32768/50000]\tLoss: 0.8412\tLR: 0.010000\n",
            "Training Epoch: 18 [32896/50000]\tLoss: 0.9718\tLR: 0.010000\n",
            "Training Epoch: 18 [33024/50000]\tLoss: 1.0041\tLR: 0.010000\n",
            "Training Epoch: 18 [33152/50000]\tLoss: 0.6229\tLR: 0.010000\n",
            "Training Epoch: 18 [33280/50000]\tLoss: 0.8453\tLR: 0.010000\n",
            "Training Epoch: 18 [33408/50000]\tLoss: 0.7445\tLR: 0.010000\n",
            "Training Epoch: 18 [33536/50000]\tLoss: 0.7849\tLR: 0.010000\n",
            "Training Epoch: 18 [33664/50000]\tLoss: 0.6271\tLR: 0.010000\n",
            "Training Epoch: 18 [33792/50000]\tLoss: 0.8496\tLR: 0.010000\n",
            "Training Epoch: 18 [33920/50000]\tLoss: 0.7857\tLR: 0.010000\n",
            "Training Epoch: 18 [34048/50000]\tLoss: 0.8580\tLR: 0.010000\n",
            "Training Epoch: 18 [34176/50000]\tLoss: 0.8262\tLR: 0.010000\n",
            "Training Epoch: 18 [34304/50000]\tLoss: 0.7915\tLR: 0.010000\n",
            "Training Epoch: 18 [34432/50000]\tLoss: 0.6495\tLR: 0.010000\n",
            "Training Epoch: 18 [34560/50000]\tLoss: 0.8402\tLR: 0.010000\n",
            "Training Epoch: 18 [34688/50000]\tLoss: 0.7776\tLR: 0.010000\n",
            "Training Epoch: 18 [34816/50000]\tLoss: 0.7751\tLR: 0.010000\n",
            "Training Epoch: 18 [34944/50000]\tLoss: 0.7999\tLR: 0.010000\n",
            "Training Epoch: 18 [35072/50000]\tLoss: 0.8599\tLR: 0.010000\n",
            "Training Epoch: 18 [35200/50000]\tLoss: 0.8753\tLR: 0.010000\n",
            "Training Epoch: 18 [35328/50000]\tLoss: 0.9756\tLR: 0.010000\n",
            "Training Epoch: 18 [35456/50000]\tLoss: 0.7514\tLR: 0.010000\n",
            "Training Epoch: 18 [35584/50000]\tLoss: 1.0914\tLR: 0.010000\n",
            "Training Epoch: 18 [35712/50000]\tLoss: 0.9451\tLR: 0.010000\n",
            "Training Epoch: 18 [35840/50000]\tLoss: 0.8246\tLR: 0.010000\n",
            "Training Epoch: 18 [35968/50000]\tLoss: 1.3142\tLR: 0.010000\n",
            "Training Epoch: 18 [36096/50000]\tLoss: 0.8325\tLR: 0.010000\n",
            "Training Epoch: 18 [36224/50000]\tLoss: 0.8457\tLR: 0.010000\n",
            "Training Epoch: 18 [36352/50000]\tLoss: 0.8823\tLR: 0.010000\n",
            "Training Epoch: 18 [36480/50000]\tLoss: 0.7662\tLR: 0.010000\n",
            "Training Epoch: 18 [36608/50000]\tLoss: 0.9935\tLR: 0.010000\n",
            "Training Epoch: 18 [36736/50000]\tLoss: 0.7737\tLR: 0.010000\n",
            "Training Epoch: 18 [36864/50000]\tLoss: 1.0509\tLR: 0.010000\n",
            "Training Epoch: 18 [36992/50000]\tLoss: 0.7658\tLR: 0.010000\n",
            "Training Epoch: 18 [37120/50000]\tLoss: 0.6896\tLR: 0.010000\n",
            "Training Epoch: 18 [37248/50000]\tLoss: 0.9197\tLR: 0.010000\n",
            "Training Epoch: 18 [37376/50000]\tLoss: 0.9279\tLR: 0.010000\n",
            "Training Epoch: 18 [37504/50000]\tLoss: 0.9148\tLR: 0.010000\n",
            "Training Epoch: 18 [37632/50000]\tLoss: 0.6939\tLR: 0.010000\n",
            "Training Epoch: 18 [37760/50000]\tLoss: 0.8882\tLR: 0.010000\n",
            "Training Epoch: 18 [37888/50000]\tLoss: 0.6828\tLR: 0.010000\n",
            "Training Epoch: 18 [38016/50000]\tLoss: 0.7839\tLR: 0.010000\n",
            "Training Epoch: 18 [38144/50000]\tLoss: 1.0147\tLR: 0.010000\n",
            "Training Epoch: 18 [38272/50000]\tLoss: 0.8671\tLR: 0.010000\n",
            "Training Epoch: 18 [38400/50000]\tLoss: 0.9219\tLR: 0.010000\n",
            "Training Epoch: 18 [38528/50000]\tLoss: 0.8645\tLR: 0.010000\n",
            "Training Epoch: 18 [38656/50000]\tLoss: 1.0029\tLR: 0.010000\n",
            "Training Epoch: 18 [38784/50000]\tLoss: 0.7130\tLR: 0.010000\n",
            "Training Epoch: 18 [38912/50000]\tLoss: 0.8849\tLR: 0.010000\n",
            "Training Epoch: 18 [39040/50000]\tLoss: 1.1073\tLR: 0.010000\n",
            "Training Epoch: 18 [39168/50000]\tLoss: 0.8516\tLR: 0.010000\n",
            "Training Epoch: 18 [39296/50000]\tLoss: 0.9708\tLR: 0.010000\n",
            "Training Epoch: 18 [39424/50000]\tLoss: 0.8118\tLR: 0.010000\n",
            "Training Epoch: 18 [39552/50000]\tLoss: 0.7148\tLR: 0.010000\n",
            "Training Epoch: 18 [39680/50000]\tLoss: 1.1603\tLR: 0.010000\n",
            "Training Epoch: 18 [39808/50000]\tLoss: 0.8276\tLR: 0.010000\n",
            "Training Epoch: 18 [39936/50000]\tLoss: 0.8088\tLR: 0.010000\n",
            "Training Epoch: 18 [40064/50000]\tLoss: 0.7905\tLR: 0.010000\n",
            "Training Epoch: 18 [40192/50000]\tLoss: 0.7407\tLR: 0.010000\n",
            "Training Epoch: 18 [40320/50000]\tLoss: 1.0316\tLR: 0.010000\n",
            "Training Epoch: 18 [40448/50000]\tLoss: 0.7786\tLR: 0.010000\n",
            "Training Epoch: 18 [40576/50000]\tLoss: 1.0392\tLR: 0.010000\n",
            "Training Epoch: 18 [40704/50000]\tLoss: 0.7620\tLR: 0.010000\n",
            "Training Epoch: 18 [40832/50000]\tLoss: 0.7597\tLR: 0.010000\n",
            "Training Epoch: 18 [40960/50000]\tLoss: 0.8920\tLR: 0.010000\n",
            "Training Epoch: 18 [41088/50000]\tLoss: 0.9794\tLR: 0.010000\n",
            "Training Epoch: 18 [41216/50000]\tLoss: 0.9661\tLR: 0.010000\n",
            "Training Epoch: 18 [41344/50000]\tLoss: 1.0380\tLR: 0.010000\n",
            "Training Epoch: 18 [41472/50000]\tLoss: 0.9353\tLR: 0.010000\n",
            "Training Epoch: 18 [41600/50000]\tLoss: 0.6937\tLR: 0.010000\n",
            "Training Epoch: 18 [41728/50000]\tLoss: 0.8730\tLR: 0.010000\n",
            "Training Epoch: 18 [41856/50000]\tLoss: 0.8610\tLR: 0.010000\n",
            "Training Epoch: 18 [41984/50000]\tLoss: 0.8907\tLR: 0.010000\n",
            "Training Epoch: 18 [42112/50000]\tLoss: 1.3074\tLR: 0.010000\n",
            "Training Epoch: 18 [42240/50000]\tLoss: 0.9101\tLR: 0.010000\n",
            "Training Epoch: 18 [42368/50000]\tLoss: 0.8496\tLR: 0.010000\n",
            "Training Epoch: 18 [42496/50000]\tLoss: 0.8708\tLR: 0.010000\n",
            "Training Epoch: 18 [42624/50000]\tLoss: 0.8024\tLR: 0.010000\n",
            "Training Epoch: 18 [42752/50000]\tLoss: 0.8320\tLR: 0.010000\n",
            "Training Epoch: 18 [42880/50000]\tLoss: 0.8825\tLR: 0.010000\n",
            "Training Epoch: 18 [43008/50000]\tLoss: 0.8415\tLR: 0.010000\n",
            "Training Epoch: 18 [43136/50000]\tLoss: 1.0819\tLR: 0.010000\n",
            "Training Epoch: 18 [43264/50000]\tLoss: 0.9429\tLR: 0.010000\n",
            "Training Epoch: 18 [43392/50000]\tLoss: 0.6548\tLR: 0.010000\n",
            "Training Epoch: 18 [43520/50000]\tLoss: 0.9693\tLR: 0.010000\n",
            "Training Epoch: 18 [43648/50000]\tLoss: 0.9235\tLR: 0.010000\n",
            "Training Epoch: 18 [43776/50000]\tLoss: 0.8464\tLR: 0.010000\n",
            "Training Epoch: 18 [43904/50000]\tLoss: 0.8762\tLR: 0.010000\n",
            "Training Epoch: 18 [44032/50000]\tLoss: 0.9154\tLR: 0.010000\n",
            "Training Epoch: 18 [44160/50000]\tLoss: 0.8647\tLR: 0.010000\n",
            "Training Epoch: 18 [44288/50000]\tLoss: 0.8430\tLR: 0.010000\n",
            "Training Epoch: 18 [44416/50000]\tLoss: 0.8651\tLR: 0.010000\n",
            "Training Epoch: 18 [44544/50000]\tLoss: 0.8084\tLR: 0.010000\n",
            "Training Epoch: 18 [44672/50000]\tLoss: 0.8633\tLR: 0.010000\n",
            "Training Epoch: 18 [44800/50000]\tLoss: 0.8688\tLR: 0.010000\n",
            "Training Epoch: 18 [44928/50000]\tLoss: 0.9384\tLR: 0.010000\n",
            "Training Epoch: 18 [45056/50000]\tLoss: 0.8863\tLR: 0.010000\n",
            "Training Epoch: 18 [45184/50000]\tLoss: 0.8945\tLR: 0.010000\n",
            "Training Epoch: 18 [45312/50000]\tLoss: 0.8257\tLR: 0.010000\n",
            "Training Epoch: 18 [45440/50000]\tLoss: 1.1090\tLR: 0.010000\n",
            "Training Epoch: 18 [45568/50000]\tLoss: 0.7051\tLR: 0.010000\n",
            "Training Epoch: 18 [45696/50000]\tLoss: 0.8151\tLR: 0.010000\n",
            "Training Epoch: 18 [45824/50000]\tLoss: 0.9806\tLR: 0.010000\n",
            "Training Epoch: 18 [45952/50000]\tLoss: 0.7865\tLR: 0.010000\n",
            "Training Epoch: 18 [46080/50000]\tLoss: 0.8611\tLR: 0.010000\n",
            "Training Epoch: 18 [46208/50000]\tLoss: 1.0894\tLR: 0.010000\n",
            "Training Epoch: 18 [46336/50000]\tLoss: 0.8445\tLR: 0.010000\n",
            "Training Epoch: 18 [46464/50000]\tLoss: 1.1198\tLR: 0.010000\n",
            "Training Epoch: 18 [46592/50000]\tLoss: 0.8003\tLR: 0.010000\n",
            "Training Epoch: 18 [46720/50000]\tLoss: 0.9328\tLR: 0.010000\n",
            "Training Epoch: 18 [46848/50000]\tLoss: 0.9173\tLR: 0.010000\n",
            "Training Epoch: 18 [46976/50000]\tLoss: 0.8277\tLR: 0.010000\n",
            "Training Epoch: 18 [47104/50000]\tLoss: 0.9475\tLR: 0.010000\n",
            "Training Epoch: 18 [47232/50000]\tLoss: 0.9185\tLR: 0.010000\n",
            "Training Epoch: 18 [47360/50000]\tLoss: 0.7534\tLR: 0.010000\n",
            "Training Epoch: 18 [47488/50000]\tLoss: 1.0397\tLR: 0.010000\n",
            "Training Epoch: 18 [47616/50000]\tLoss: 0.9511\tLR: 0.010000\n",
            "Training Epoch: 18 [47744/50000]\tLoss: 0.7442\tLR: 0.010000\n",
            "Training Epoch: 18 [47872/50000]\tLoss: 0.7357\tLR: 0.010000\n",
            "Training Epoch: 18 [48000/50000]\tLoss: 0.9017\tLR: 0.010000\n",
            "Training Epoch: 18 [48128/50000]\tLoss: 0.6897\tLR: 0.010000\n",
            "Training Epoch: 18 [48256/50000]\tLoss: 0.9383\tLR: 0.010000\n",
            "Training Epoch: 18 [48384/50000]\tLoss: 0.9444\tLR: 0.010000\n",
            "Training Epoch: 18 [48512/50000]\tLoss: 0.9611\tLR: 0.010000\n",
            "Training Epoch: 18 [48640/50000]\tLoss: 1.0323\tLR: 0.010000\n",
            "Training Epoch: 18 [48768/50000]\tLoss: 1.0738\tLR: 0.010000\n",
            "Training Epoch: 18 [48896/50000]\tLoss: 0.8503\tLR: 0.010000\n",
            "Training Epoch: 18 [49024/50000]\tLoss: 0.9188\tLR: 0.010000\n",
            "Training Epoch: 18 [49152/50000]\tLoss: 0.7342\tLR: 0.010000\n",
            "Training Epoch: 18 [49280/50000]\tLoss: 0.8241\tLR: 0.010000\n",
            "Training Epoch: 18 [49408/50000]\tLoss: 1.1391\tLR: 0.010000\n",
            "Training Epoch: 18 [49536/50000]\tLoss: 1.0849\tLR: 0.010000\n",
            "Training Epoch: 18 [49664/50000]\tLoss: 0.8207\tLR: 0.010000\n",
            "Training Epoch: 18 [49792/50000]\tLoss: 0.9443\tLR: 0.010000\n",
            "Training Epoch: 18 [49920/50000]\tLoss: 0.9330\tLR: 0.010000\n",
            "Training Epoch: 18 [50000/50000]\tLoss: 0.8065\tLR: 0.010000\n",
            "epoch 18 training time consumed: 206.47s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 151199 GiB | 151199 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 151090 GiB | 151090 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    109 GiB |    109 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 151199 GiB | 151199 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 151090 GiB | 151090 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    109 GiB |    109 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 151184 GiB | 151183 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 151075 GiB | 151074 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |    109 GiB |    109 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB | 109802 GiB | 109802 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB | 109690 GiB | 109689 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |    112 GiB |    112 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    6003 K  |    6002 K  |\n",
            "|       from large pool |      92    |     184    |    3384 K  |    3384 K  |\n",
            "|       from small pool |     519    |     646    |    2618 K  |    2617 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    6003 K  |    6002 K  |\n",
            "|       from large pool |      92    |     184    |    3384 K  |    3384 K  |\n",
            "|       from small pool |     519    |     646    |    2618 K  |    2617 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |     122    |    2533 K  |    2533 K  |\n",
            "|       from large pool |      42    |     111    |    1998 K  |    1998 K  |\n",
            "|       from small pool |       8    |      21    |     535 K  |     535 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0111, Accuracy: 0.6240, Time consumed:11.22s\n",
            "\n",
            "Training Epoch: 19 [128/50000]\tLoss: 0.9501\tLR: 0.010000\n",
            "Training Epoch: 19 [256/50000]\tLoss: 0.5945\tLR: 0.010000\n",
            "Training Epoch: 19 [384/50000]\tLoss: 0.6970\tLR: 0.010000\n",
            "Training Epoch: 19 [512/50000]\tLoss: 0.9296\tLR: 0.010000\n",
            "Training Epoch: 19 [640/50000]\tLoss: 0.8832\tLR: 0.010000\n",
            "Training Epoch: 19 [768/50000]\tLoss: 0.6727\tLR: 0.010000\n",
            "Training Epoch: 19 [896/50000]\tLoss: 0.6566\tLR: 0.010000\n",
            "Training Epoch: 19 [1024/50000]\tLoss: 0.6623\tLR: 0.010000\n",
            "Training Epoch: 19 [1152/50000]\tLoss: 0.8837\tLR: 0.010000\n",
            "Training Epoch: 19 [1280/50000]\tLoss: 0.8481\tLR: 0.010000\n",
            "Training Epoch: 19 [1408/50000]\tLoss: 0.6873\tLR: 0.010000\n",
            "Training Epoch: 19 [1536/50000]\tLoss: 0.8408\tLR: 0.010000\n",
            "Training Epoch: 19 [1664/50000]\tLoss: 0.8199\tLR: 0.010000\n",
            "Training Epoch: 19 [1792/50000]\tLoss: 0.8510\tLR: 0.010000\n",
            "Training Epoch: 19 [1920/50000]\tLoss: 0.7117\tLR: 0.010000\n",
            "Training Epoch: 19 [2048/50000]\tLoss: 0.6865\tLR: 0.010000\n",
            "Training Epoch: 19 [2176/50000]\tLoss: 0.8556\tLR: 0.010000\n",
            "Training Epoch: 19 [2304/50000]\tLoss: 0.4531\tLR: 0.010000\n",
            "Training Epoch: 19 [2432/50000]\tLoss: 0.7860\tLR: 0.010000\n",
            "Training Epoch: 19 [2560/50000]\tLoss: 0.6965\tLR: 0.010000\n",
            "Training Epoch: 19 [2688/50000]\tLoss: 0.6863\tLR: 0.010000\n",
            "Training Epoch: 19 [2816/50000]\tLoss: 0.8827\tLR: 0.010000\n",
            "Training Epoch: 19 [2944/50000]\tLoss: 0.8203\tLR: 0.010000\n",
            "Training Epoch: 19 [3072/50000]\tLoss: 0.7503\tLR: 0.010000\n",
            "Training Epoch: 19 [3200/50000]\tLoss: 0.8188\tLR: 0.010000\n",
            "Training Epoch: 19 [3328/50000]\tLoss: 0.8758\tLR: 0.010000\n",
            "Training Epoch: 19 [3456/50000]\tLoss: 0.8106\tLR: 0.010000\n",
            "Training Epoch: 19 [3584/50000]\tLoss: 0.6992\tLR: 0.010000\n",
            "Training Epoch: 19 [3712/50000]\tLoss: 0.8010\tLR: 0.010000\n",
            "Training Epoch: 19 [3840/50000]\tLoss: 0.7009\tLR: 0.010000\n",
            "Training Epoch: 19 [3968/50000]\tLoss: 0.8246\tLR: 0.010000\n",
            "Training Epoch: 19 [4096/50000]\tLoss: 0.8180\tLR: 0.010000\n",
            "Training Epoch: 19 [4224/50000]\tLoss: 0.8030\tLR: 0.010000\n",
            "Training Epoch: 19 [4352/50000]\tLoss: 0.8821\tLR: 0.010000\n",
            "Training Epoch: 19 [4480/50000]\tLoss: 0.5492\tLR: 0.010000\n",
            "Training Epoch: 19 [4608/50000]\tLoss: 0.8049\tLR: 0.010000\n",
            "Training Epoch: 19 [4736/50000]\tLoss: 0.7774\tLR: 0.010000\n",
            "Training Epoch: 19 [4864/50000]\tLoss: 0.6531\tLR: 0.010000\n",
            "Training Epoch: 19 [4992/50000]\tLoss: 0.7564\tLR: 0.010000\n",
            "Training Epoch: 19 [5120/50000]\tLoss: 0.7943\tLR: 0.010000\n",
            "Training Epoch: 19 [5248/50000]\tLoss: 0.7260\tLR: 0.010000\n",
            "Training Epoch: 19 [5376/50000]\tLoss: 0.7735\tLR: 0.010000\n",
            "Training Epoch: 19 [5504/50000]\tLoss: 0.6498\tLR: 0.010000\n",
            "Training Epoch: 19 [5632/50000]\tLoss: 0.5750\tLR: 0.010000\n",
            "Training Epoch: 19 [5760/50000]\tLoss: 0.7910\tLR: 0.010000\n",
            "Training Epoch: 19 [5888/50000]\tLoss: 0.7288\tLR: 0.010000\n",
            "Training Epoch: 19 [6016/50000]\tLoss: 0.6860\tLR: 0.010000\n",
            "Training Epoch: 19 [6144/50000]\tLoss: 0.7705\tLR: 0.010000\n",
            "Training Epoch: 19 [6272/50000]\tLoss: 0.7414\tLR: 0.010000\n",
            "Training Epoch: 19 [6400/50000]\tLoss: 0.7771\tLR: 0.010000\n",
            "Training Epoch: 19 [6528/50000]\tLoss: 0.6267\tLR: 0.010000\n",
            "Training Epoch: 19 [6656/50000]\tLoss: 0.8654\tLR: 0.010000\n",
            "Training Epoch: 19 [6784/50000]\tLoss: 1.0958\tLR: 0.010000\n",
            "Training Epoch: 19 [6912/50000]\tLoss: 0.8300\tLR: 0.010000\n",
            "Training Epoch: 19 [7040/50000]\tLoss: 0.8264\tLR: 0.010000\n",
            "Training Epoch: 19 [7168/50000]\tLoss: 0.4949\tLR: 0.010000\n",
            "Training Epoch: 19 [7296/50000]\tLoss: 0.6574\tLR: 0.010000\n",
            "Training Epoch: 19 [7424/50000]\tLoss: 0.7272\tLR: 0.010000\n",
            "Training Epoch: 19 [7552/50000]\tLoss: 0.5981\tLR: 0.010000\n",
            "Training Epoch: 19 [7680/50000]\tLoss: 0.7131\tLR: 0.010000\n",
            "Training Epoch: 19 [7808/50000]\tLoss: 0.7629\tLR: 0.010000\n",
            "Training Epoch: 19 [7936/50000]\tLoss: 0.6202\tLR: 0.010000\n",
            "Training Epoch: 19 [8064/50000]\tLoss: 0.7212\tLR: 0.010000\n",
            "Training Epoch: 19 [8192/50000]\tLoss: 0.9353\tLR: 0.010000\n",
            "Training Epoch: 19 [8320/50000]\tLoss: 0.7190\tLR: 0.010000\n",
            "Training Epoch: 19 [8448/50000]\tLoss: 0.6145\tLR: 0.010000\n",
            "Training Epoch: 19 [8576/50000]\tLoss: 0.7382\tLR: 0.010000\n",
            "Training Epoch: 19 [8704/50000]\tLoss: 0.9086\tLR: 0.010000\n",
            "Training Epoch: 19 [8832/50000]\tLoss: 0.8267\tLR: 0.010000\n",
            "Training Epoch: 19 [8960/50000]\tLoss: 0.7123\tLR: 0.010000\n",
            "Training Epoch: 19 [9088/50000]\tLoss: 0.7391\tLR: 0.010000\n",
            "Training Epoch: 19 [9216/50000]\tLoss: 0.7035\tLR: 0.010000\n",
            "Training Epoch: 19 [9344/50000]\tLoss: 1.0024\tLR: 0.010000\n",
            "Training Epoch: 19 [9472/50000]\tLoss: 0.7179\tLR: 0.010000\n",
            "Training Epoch: 19 [9600/50000]\tLoss: 0.7596\tLR: 0.010000\n",
            "Training Epoch: 19 [9728/50000]\tLoss: 0.8001\tLR: 0.010000\n",
            "Training Epoch: 19 [9856/50000]\tLoss: 0.7550\tLR: 0.010000\n",
            "Training Epoch: 19 [9984/50000]\tLoss: 0.7095\tLR: 0.010000\n",
            "Training Epoch: 19 [10112/50000]\tLoss: 0.7329\tLR: 0.010000\n",
            "Training Epoch: 19 [10240/50000]\tLoss: 0.8422\tLR: 0.010000\n",
            "Training Epoch: 19 [10368/50000]\tLoss: 0.8013\tLR: 0.010000\n",
            "Training Epoch: 19 [10496/50000]\tLoss: 0.6878\tLR: 0.010000\n",
            "Training Epoch: 19 [10624/50000]\tLoss: 0.6152\tLR: 0.010000\n",
            "Training Epoch: 19 [10752/50000]\tLoss: 0.6666\tLR: 0.010000\n",
            "Training Epoch: 19 [10880/50000]\tLoss: 0.8083\tLR: 0.010000\n",
            "Training Epoch: 19 [11008/50000]\tLoss: 0.5876\tLR: 0.010000\n",
            "Training Epoch: 19 [11136/50000]\tLoss: 0.5532\tLR: 0.010000\n",
            "Training Epoch: 19 [11264/50000]\tLoss: 0.5885\tLR: 0.010000\n",
            "Training Epoch: 19 [11392/50000]\tLoss: 0.9949\tLR: 0.010000\n",
            "Training Epoch: 19 [11520/50000]\tLoss: 0.6329\tLR: 0.010000\n",
            "Training Epoch: 19 [11648/50000]\tLoss: 0.7345\tLR: 0.010000\n",
            "Training Epoch: 19 [11776/50000]\tLoss: 0.6909\tLR: 0.010000\n",
            "Training Epoch: 19 [11904/50000]\tLoss: 0.5048\tLR: 0.010000\n",
            "Training Epoch: 19 [12032/50000]\tLoss: 0.7941\tLR: 0.010000\n",
            "Training Epoch: 19 [12160/50000]\tLoss: 0.8734\tLR: 0.010000\n",
            "Training Epoch: 19 [12288/50000]\tLoss: 0.6550\tLR: 0.010000\n",
            "Training Epoch: 19 [12416/50000]\tLoss: 0.6187\tLR: 0.010000\n",
            "Training Epoch: 19 [12544/50000]\tLoss: 0.7984\tLR: 0.010000\n",
            "Training Epoch: 19 [12672/50000]\tLoss: 0.6096\tLR: 0.010000\n",
            "Training Epoch: 19 [12800/50000]\tLoss: 0.8621\tLR: 0.010000\n",
            "Training Epoch: 19 [12928/50000]\tLoss: 0.6794\tLR: 0.010000\n",
            "Training Epoch: 19 [13056/50000]\tLoss: 0.9334\tLR: 0.010000\n",
            "Training Epoch: 19 [13184/50000]\tLoss: 0.6742\tLR: 0.010000\n",
            "Training Epoch: 19 [13312/50000]\tLoss: 0.8544\tLR: 0.010000\n",
            "Training Epoch: 19 [13440/50000]\tLoss: 0.8101\tLR: 0.010000\n",
            "Training Epoch: 19 [13568/50000]\tLoss: 0.8790\tLR: 0.010000\n",
            "Training Epoch: 19 [13696/50000]\tLoss: 0.5665\tLR: 0.010000\n",
            "Training Epoch: 19 [13824/50000]\tLoss: 0.7253\tLR: 0.010000\n",
            "Training Epoch: 19 [13952/50000]\tLoss: 0.7783\tLR: 0.010000\n",
            "Training Epoch: 19 [14080/50000]\tLoss: 0.8389\tLR: 0.010000\n",
            "Training Epoch: 19 [14208/50000]\tLoss: 0.5921\tLR: 0.010000\n",
            "Training Epoch: 19 [14336/50000]\tLoss: 0.7017\tLR: 0.010000\n",
            "Training Epoch: 19 [14464/50000]\tLoss: 0.7205\tLR: 0.010000\n",
            "Training Epoch: 19 [14592/50000]\tLoss: 0.6369\tLR: 0.010000\n",
            "Training Epoch: 19 [14720/50000]\tLoss: 0.6865\tLR: 0.010000\n",
            "Training Epoch: 19 [14848/50000]\tLoss: 0.8083\tLR: 0.010000\n",
            "Training Epoch: 19 [14976/50000]\tLoss: 0.5918\tLR: 0.010000\n",
            "Training Epoch: 19 [15104/50000]\tLoss: 0.8372\tLR: 0.010000\n",
            "Training Epoch: 19 [15232/50000]\tLoss: 0.6682\tLR: 0.010000\n",
            "Training Epoch: 19 [15360/50000]\tLoss: 0.8363\tLR: 0.010000\n",
            "Training Epoch: 19 [15488/50000]\tLoss: 0.7333\tLR: 0.010000\n",
            "Training Epoch: 19 [15616/50000]\tLoss: 0.8284\tLR: 0.010000\n",
            "Training Epoch: 19 [15744/50000]\tLoss: 0.7602\tLR: 0.010000\n",
            "Training Epoch: 19 [15872/50000]\tLoss: 0.5846\tLR: 0.010000\n",
            "Training Epoch: 19 [16000/50000]\tLoss: 0.5925\tLR: 0.010000\n",
            "Training Epoch: 19 [16128/50000]\tLoss: 0.7433\tLR: 0.010000\n",
            "Training Epoch: 19 [16256/50000]\tLoss: 0.9796\tLR: 0.010000\n",
            "Training Epoch: 19 [16384/50000]\tLoss: 0.7022\tLR: 0.010000\n",
            "Training Epoch: 19 [16512/50000]\tLoss: 0.6706\tLR: 0.010000\n",
            "Training Epoch: 19 [16640/50000]\tLoss: 0.8458\tLR: 0.010000\n",
            "Training Epoch: 19 [16768/50000]\tLoss: 0.7936\tLR: 0.010000\n",
            "Training Epoch: 19 [16896/50000]\tLoss: 0.9838\tLR: 0.010000\n",
            "Training Epoch: 19 [17024/50000]\tLoss: 0.6473\tLR: 0.010000\n",
            "Training Epoch: 19 [17152/50000]\tLoss: 0.8365\tLR: 0.010000\n",
            "Training Epoch: 19 [17280/50000]\tLoss: 0.7761\tLR: 0.010000\n",
            "Training Epoch: 19 [17408/50000]\tLoss: 0.7684\tLR: 0.010000\n",
            "Training Epoch: 19 [17536/50000]\tLoss: 0.7706\tLR: 0.010000\n",
            "Training Epoch: 19 [17664/50000]\tLoss: 1.0017\tLR: 0.010000\n",
            "Training Epoch: 19 [17792/50000]\tLoss: 0.8665\tLR: 0.010000\n",
            "Training Epoch: 19 [17920/50000]\tLoss: 0.6871\tLR: 0.010000\n",
            "Training Epoch: 19 [18048/50000]\tLoss: 0.8127\tLR: 0.010000\n",
            "Training Epoch: 19 [18176/50000]\tLoss: 0.7841\tLR: 0.010000\n",
            "Training Epoch: 19 [18304/50000]\tLoss: 0.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [18432/50000]\tLoss: 0.8230\tLR: 0.010000\n",
            "Training Epoch: 19 [18560/50000]\tLoss: 0.8690\tLR: 0.010000\n",
            "Training Epoch: 19 [18688/50000]\tLoss: 0.9724\tLR: 0.010000\n",
            "Training Epoch: 19 [18816/50000]\tLoss: 0.8085\tLR: 0.010000\n",
            "Training Epoch: 19 [18944/50000]\tLoss: 0.8604\tLR: 0.010000\n",
            "Training Epoch: 19 [19072/50000]\tLoss: 0.8521\tLR: 0.010000\n",
            "Training Epoch: 19 [19200/50000]\tLoss: 0.7687\tLR: 0.010000\n",
            "Training Epoch: 19 [19328/50000]\tLoss: 0.8481\tLR: 0.010000\n",
            "Training Epoch: 19 [19456/50000]\tLoss: 0.6326\tLR: 0.010000\n",
            "Training Epoch: 19 [19584/50000]\tLoss: 0.8095\tLR: 0.010000\n",
            "Training Epoch: 19 [19712/50000]\tLoss: 0.7991\tLR: 0.010000\n",
            "Training Epoch: 19 [19840/50000]\tLoss: 0.7790\tLR: 0.010000\n",
            "Training Epoch: 19 [19968/50000]\tLoss: 1.0024\tLR: 0.010000\n",
            "Training Epoch: 19 [20096/50000]\tLoss: 0.7562\tLR: 0.010000\n",
            "Training Epoch: 19 [20224/50000]\tLoss: 0.8820\tLR: 0.010000\n",
            "Training Epoch: 19 [20352/50000]\tLoss: 0.8355\tLR: 0.010000\n",
            "Training Epoch: 19 [20480/50000]\tLoss: 0.9560\tLR: 0.010000\n",
            "Training Epoch: 19 [20608/50000]\tLoss: 0.7884\tLR: 0.010000\n",
            "Training Epoch: 19 [20736/50000]\tLoss: 0.6680\tLR: 0.010000\n",
            "Training Epoch: 19 [20864/50000]\tLoss: 0.8810\tLR: 0.010000\n",
            "Training Epoch: 19 [20992/50000]\tLoss: 0.7745\tLR: 0.010000\n",
            "Training Epoch: 19 [21120/50000]\tLoss: 0.8470\tLR: 0.010000\n",
            "Training Epoch: 19 [21248/50000]\tLoss: 0.9888\tLR: 0.010000\n",
            "Training Epoch: 19 [21376/50000]\tLoss: 0.7176\tLR: 0.010000\n",
            "Training Epoch: 19 [21504/50000]\tLoss: 0.6991\tLR: 0.010000\n",
            "Training Epoch: 19 [21632/50000]\tLoss: 0.8715\tLR: 0.010000\n",
            "Training Epoch: 19 [21760/50000]\tLoss: 0.7989\tLR: 0.010000\n",
            "Training Epoch: 19 [21888/50000]\tLoss: 0.7666\tLR: 0.010000\n",
            "Training Epoch: 19 [22016/50000]\tLoss: 0.7332\tLR: 0.010000\n",
            "Training Epoch: 19 [22144/50000]\tLoss: 0.7025\tLR: 0.010000\n",
            "Training Epoch: 19 [22272/50000]\tLoss: 0.7837\tLR: 0.010000\n",
            "Training Epoch: 19 [22400/50000]\tLoss: 0.8198\tLR: 0.010000\n",
            "Training Epoch: 19 [22528/50000]\tLoss: 0.6816\tLR: 0.010000\n",
            "Training Epoch: 19 [22656/50000]\tLoss: 0.6287\tLR: 0.010000\n",
            "Training Epoch: 19 [22784/50000]\tLoss: 0.7305\tLR: 0.010000\n",
            "Training Epoch: 19 [22912/50000]\tLoss: 0.7669\tLR: 0.010000\n",
            "Training Epoch: 19 [23040/50000]\tLoss: 0.8050\tLR: 0.010000\n",
            "Training Epoch: 19 [23168/50000]\tLoss: 0.8231\tLR: 0.010000\n",
            "Training Epoch: 19 [23296/50000]\tLoss: 0.9991\tLR: 0.010000\n",
            "Training Epoch: 19 [23424/50000]\tLoss: 0.6984\tLR: 0.010000\n",
            "Training Epoch: 19 [23552/50000]\tLoss: 0.7638\tLR: 0.010000\n",
            "Training Epoch: 19 [23680/50000]\tLoss: 0.6451\tLR: 0.010000\n",
            "Training Epoch: 19 [23808/50000]\tLoss: 0.8152\tLR: 0.010000\n",
            "Training Epoch: 19 [23936/50000]\tLoss: 0.9047\tLR: 0.010000\n",
            "Training Epoch: 19 [24064/50000]\tLoss: 0.6942\tLR: 0.010000\n",
            "Training Epoch: 19 [24192/50000]\tLoss: 0.7914\tLR: 0.010000\n",
            "Training Epoch: 19 [24320/50000]\tLoss: 0.7454\tLR: 0.010000\n",
            "Training Epoch: 19 [24448/50000]\tLoss: 0.7630\tLR: 0.010000\n",
            "Training Epoch: 19 [24576/50000]\tLoss: 0.9267\tLR: 0.010000\n",
            "Training Epoch: 19 [24704/50000]\tLoss: 0.7256\tLR: 0.010000\n",
            "Training Epoch: 19 [24832/50000]\tLoss: 0.8470\tLR: 0.010000\n",
            "Training Epoch: 19 [24960/50000]\tLoss: 0.8217\tLR: 0.010000\n",
            "Training Epoch: 19 [25088/50000]\tLoss: 0.8116\tLR: 0.010000\n",
            "Training Epoch: 19 [25216/50000]\tLoss: 0.9405\tLR: 0.010000\n",
            "Training Epoch: 19 [25344/50000]\tLoss: 0.7153\tLR: 0.010000\n",
            "Training Epoch: 19 [25472/50000]\tLoss: 0.7031\tLR: 0.010000\n",
            "Training Epoch: 19 [25600/50000]\tLoss: 0.8869\tLR: 0.010000\n",
            "Training Epoch: 19 [25728/50000]\tLoss: 0.6891\tLR: 0.010000\n",
            "Training Epoch: 19 [25856/50000]\tLoss: 0.7989\tLR: 0.010000\n",
            "Training Epoch: 19 [25984/50000]\tLoss: 0.8503\tLR: 0.010000\n",
            "Training Epoch: 19 [26112/50000]\tLoss: 0.9496\tLR: 0.010000\n",
            "Training Epoch: 19 [26240/50000]\tLoss: 0.8012\tLR: 0.010000\n",
            "Training Epoch: 19 [26368/50000]\tLoss: 0.9669\tLR: 0.010000\n",
            "Training Epoch: 19 [26496/50000]\tLoss: 0.8297\tLR: 0.010000\n",
            "Training Epoch: 19 [26624/50000]\tLoss: 0.9559\tLR: 0.010000\n",
            "Training Epoch: 19 [26752/50000]\tLoss: 0.8011\tLR: 0.010000\n",
            "Training Epoch: 19 [26880/50000]\tLoss: 0.9303\tLR: 0.010000\n",
            "Training Epoch: 19 [27008/50000]\tLoss: 0.7908\tLR: 0.010000\n",
            "Training Epoch: 19 [27136/50000]\tLoss: 0.7677\tLR: 0.010000\n",
            "Training Epoch: 19 [27264/50000]\tLoss: 0.7710\tLR: 0.010000\n",
            "Training Epoch: 19 [27392/50000]\tLoss: 0.8296\tLR: 0.010000\n",
            "Training Epoch: 19 [27520/50000]\tLoss: 0.7080\tLR: 0.010000\n",
            "Training Epoch: 19 [27648/50000]\tLoss: 0.8165\tLR: 0.010000\n",
            "Training Epoch: 19 [27776/50000]\tLoss: 0.8668\tLR: 0.010000\n",
            "Training Epoch: 19 [27904/50000]\tLoss: 0.7195\tLR: 0.010000\n",
            "Training Epoch: 19 [28032/50000]\tLoss: 0.7588\tLR: 0.010000\n",
            "Training Epoch: 19 [28160/50000]\tLoss: 0.7588\tLR: 0.010000\n",
            "Training Epoch: 19 [28288/50000]\tLoss: 0.8858\tLR: 0.010000\n",
            "Training Epoch: 19 [28416/50000]\tLoss: 0.9773\tLR: 0.010000\n",
            "Training Epoch: 19 [28544/50000]\tLoss: 0.9365\tLR: 0.010000\n",
            "Training Epoch: 19 [28672/50000]\tLoss: 0.7762\tLR: 0.010000\n",
            "Training Epoch: 19 [28800/50000]\tLoss: 0.9465\tLR: 0.010000\n",
            "Training Epoch: 19 [28928/50000]\tLoss: 0.7875\tLR: 0.010000\n",
            "Training Epoch: 19 [29056/50000]\tLoss: 0.8071\tLR: 0.010000\n",
            "Training Epoch: 19 [29184/50000]\tLoss: 0.8611\tLR: 0.010000\n",
            "Training Epoch: 19 [29312/50000]\tLoss: 0.8569\tLR: 0.010000\n",
            "Training Epoch: 19 [29440/50000]\tLoss: 0.8220\tLR: 0.010000\n",
            "Training Epoch: 19 [29568/50000]\tLoss: 0.7689\tLR: 0.010000\n",
            "Training Epoch: 19 [29696/50000]\tLoss: 0.6433\tLR: 0.010000\n",
            "Training Epoch: 19 [29824/50000]\tLoss: 0.8562\tLR: 0.010000\n",
            "Training Epoch: 19 [29952/50000]\tLoss: 0.7710\tLR: 0.010000\n",
            "Training Epoch: 19 [30080/50000]\tLoss: 0.7030\tLR: 0.010000\n",
            "Training Epoch: 19 [30208/50000]\tLoss: 0.7754\tLR: 0.010000\n",
            "Training Epoch: 19 [30336/50000]\tLoss: 0.7668\tLR: 0.010000\n",
            "Training Epoch: 19 [30464/50000]\tLoss: 0.9827\tLR: 0.010000\n",
            "Training Epoch: 19 [30592/50000]\tLoss: 0.8405\tLR: 0.010000\n",
            "Training Epoch: 19 [30720/50000]\tLoss: 0.9933\tLR: 0.010000\n",
            "Training Epoch: 19 [30848/50000]\tLoss: 0.8849\tLR: 0.010000\n",
            "Training Epoch: 19 [30976/50000]\tLoss: 0.8282\tLR: 0.010000\n",
            "Training Epoch: 19 [31104/50000]\tLoss: 0.7365\tLR: 0.010000\n",
            "Training Epoch: 19 [31232/50000]\tLoss: 0.7221\tLR: 0.010000\n",
            "Training Epoch: 19 [31360/50000]\tLoss: 0.7805\tLR: 0.010000\n",
            "Training Epoch: 19 [31488/50000]\tLoss: 0.5441\tLR: 0.010000\n",
            "Training Epoch: 19 [31616/50000]\tLoss: 0.9542\tLR: 0.010000\n",
            "Training Epoch: 19 [31744/50000]\tLoss: 0.8668\tLR: 0.010000\n",
            "Training Epoch: 19 [31872/50000]\tLoss: 0.8709\tLR: 0.010000\n",
            "Training Epoch: 19 [32000/50000]\tLoss: 0.8043\tLR: 0.010000\n",
            "Training Epoch: 19 [32128/50000]\tLoss: 0.8033\tLR: 0.010000\n",
            "Training Epoch: 19 [32256/50000]\tLoss: 0.8120\tLR: 0.010000\n",
            "Training Epoch: 19 [32384/50000]\tLoss: 0.8187\tLR: 0.010000\n",
            "Training Epoch: 19 [32512/50000]\tLoss: 0.7381\tLR: 0.010000\n",
            "Training Epoch: 19 [32640/50000]\tLoss: 0.6799\tLR: 0.010000\n",
            "Training Epoch: 19 [32768/50000]\tLoss: 0.7182\tLR: 0.010000\n",
            "Training Epoch: 19 [32896/50000]\tLoss: 0.5769\tLR: 0.010000\n",
            "Training Epoch: 19 [33024/50000]\tLoss: 0.8407\tLR: 0.010000\n",
            "Training Epoch: 19 [33152/50000]\tLoss: 0.9027\tLR: 0.010000\n",
            "Training Epoch: 19 [33280/50000]\tLoss: 0.7761\tLR: 0.010000\n",
            "Training Epoch: 19 [33408/50000]\tLoss: 0.9406\tLR: 0.010000\n",
            "Training Epoch: 19 [33536/50000]\tLoss: 0.7273\tLR: 0.010000\n",
            "Training Epoch: 19 [33664/50000]\tLoss: 0.6040\tLR: 0.010000\n",
            "Training Epoch: 19 [33792/50000]\tLoss: 0.8227\tLR: 0.010000\n",
            "Training Epoch: 19 [33920/50000]\tLoss: 0.8605\tLR: 0.010000\n",
            "Training Epoch: 19 [34048/50000]\tLoss: 0.8512\tLR: 0.010000\n",
            "Training Epoch: 19 [34176/50000]\tLoss: 0.9713\tLR: 0.010000\n",
            "Training Epoch: 19 [34304/50000]\tLoss: 0.7999\tLR: 0.010000\n",
            "Training Epoch: 19 [34432/50000]\tLoss: 0.6885\tLR: 0.010000\n",
            "Training Epoch: 19 [34560/50000]\tLoss: 0.6671\tLR: 0.010000\n",
            "Training Epoch: 19 [34688/50000]\tLoss: 0.8376\tLR: 0.010000\n",
            "Training Epoch: 19 [34816/50000]\tLoss: 1.0206\tLR: 0.010000\n",
            "Training Epoch: 19 [34944/50000]\tLoss: 0.7346\tLR: 0.010000\n",
            "Training Epoch: 19 [35072/50000]\tLoss: 0.8978\tLR: 0.010000\n",
            "Training Epoch: 19 [35200/50000]\tLoss: 0.6861\tLR: 0.010000\n",
            "Training Epoch: 19 [35328/50000]\tLoss: 0.9378\tLR: 0.010000\n",
            "Training Epoch: 19 [35456/50000]\tLoss: 0.9686\tLR: 0.010000\n",
            "Training Epoch: 19 [35584/50000]\tLoss: 0.7679\tLR: 0.010000\n",
            "Training Epoch: 19 [35712/50000]\tLoss: 1.0324\tLR: 0.010000\n",
            "Training Epoch: 19 [35840/50000]\tLoss: 0.8942\tLR: 0.010000\n",
            "Training Epoch: 19 [35968/50000]\tLoss: 0.7287\tLR: 0.010000\n",
            "Training Epoch: 19 [36096/50000]\tLoss: 0.7347\tLR: 0.010000\n",
            "Training Epoch: 19 [36224/50000]\tLoss: 0.8396\tLR: 0.010000\n",
            "Training Epoch: 19 [36352/50000]\tLoss: 0.8570\tLR: 0.010000\n",
            "Training Epoch: 19 [36480/50000]\tLoss: 0.8091\tLR: 0.010000\n",
            "Training Epoch: 19 [36608/50000]\tLoss: 0.7660\tLR: 0.010000\n",
            "Training Epoch: 19 [36736/50000]\tLoss: 0.7890\tLR: 0.010000\n",
            "Training Epoch: 19 [36864/50000]\tLoss: 1.0543\tLR: 0.010000\n",
            "Training Epoch: 19 [36992/50000]\tLoss: 0.7312\tLR: 0.010000\n",
            "Training Epoch: 19 [37120/50000]\tLoss: 0.9788\tLR: 0.010000\n",
            "Training Epoch: 19 [37248/50000]\tLoss: 0.7372\tLR: 0.010000\n",
            "Training Epoch: 19 [37376/50000]\tLoss: 0.7426\tLR: 0.010000\n",
            "Training Epoch: 19 [37504/50000]\tLoss: 0.9600\tLR: 0.010000\n",
            "Training Epoch: 19 [37632/50000]\tLoss: 0.7777\tLR: 0.010000\n",
            "Training Epoch: 19 [37760/50000]\tLoss: 0.9338\tLR: 0.010000\n",
            "Training Epoch: 19 [37888/50000]\tLoss: 0.7938\tLR: 0.010000\n",
            "Training Epoch: 19 [38016/50000]\tLoss: 0.8074\tLR: 0.010000\n",
            "Training Epoch: 19 [38144/50000]\tLoss: 0.9252\tLR: 0.010000\n",
            "Training Epoch: 19 [38272/50000]\tLoss: 0.9184\tLR: 0.010000\n",
            "Training Epoch: 19 [38400/50000]\tLoss: 0.8519\tLR: 0.010000\n",
            "Training Epoch: 19 [38528/50000]\tLoss: 0.7747\tLR: 0.010000\n",
            "Training Epoch: 19 [38656/50000]\tLoss: 0.7446\tLR: 0.010000\n",
            "Training Epoch: 19 [38784/50000]\tLoss: 0.9363\tLR: 0.010000\n",
            "Training Epoch: 19 [38912/50000]\tLoss: 0.8414\tLR: 0.010000\n",
            "Training Epoch: 19 [39040/50000]\tLoss: 0.8983\tLR: 0.010000\n",
            "Training Epoch: 19 [39168/50000]\tLoss: 0.7924\tLR: 0.010000\n",
            "Training Epoch: 19 [39296/50000]\tLoss: 0.7525\tLR: 0.010000\n",
            "Training Epoch: 19 [39424/50000]\tLoss: 0.7851\tLR: 0.010000\n",
            "Training Epoch: 19 [39552/50000]\tLoss: 0.7960\tLR: 0.010000\n",
            "Training Epoch: 19 [39680/50000]\tLoss: 0.9142\tLR: 0.010000\n",
            "Training Epoch: 19 [39808/50000]\tLoss: 0.8083\tLR: 0.010000\n",
            "Training Epoch: 19 [39936/50000]\tLoss: 0.6939\tLR: 0.010000\n",
            "Training Epoch: 19 [40064/50000]\tLoss: 0.7128\tLR: 0.010000\n",
            "Training Epoch: 19 [40192/50000]\tLoss: 0.8984\tLR: 0.010000\n",
            "Training Epoch: 19 [40320/50000]\tLoss: 0.7327\tLR: 0.010000\n",
            "Training Epoch: 19 [40448/50000]\tLoss: 0.8225\tLR: 0.010000\n",
            "Training Epoch: 19 [40576/50000]\tLoss: 0.7847\tLR: 0.010000\n",
            "Training Epoch: 19 [40704/50000]\tLoss: 0.6112\tLR: 0.010000\n",
            "Training Epoch: 19 [40832/50000]\tLoss: 0.7837\tLR: 0.010000\n",
            "Training Epoch: 19 [40960/50000]\tLoss: 0.6654\tLR: 0.010000\n",
            "Training Epoch: 19 [41088/50000]\tLoss: 0.7496\tLR: 0.010000\n",
            "Training Epoch: 19 [41216/50000]\tLoss: 0.8791\tLR: 0.010000\n",
            "Training Epoch: 19 [41344/50000]\tLoss: 0.7804\tLR: 0.010000\n",
            "Training Epoch: 19 [41472/50000]\tLoss: 0.9093\tLR: 0.010000\n",
            "Training Epoch: 19 [41600/50000]\tLoss: 0.9080\tLR: 0.010000\n",
            "Training Epoch: 19 [41728/50000]\tLoss: 0.7209\tLR: 0.010000\n",
            "Training Epoch: 19 [41856/50000]\tLoss: 0.7542\tLR: 0.010000\n",
            "Training Epoch: 19 [41984/50000]\tLoss: 0.8119\tLR: 0.010000\n",
            "Training Epoch: 19 [42112/50000]\tLoss: 0.7103\tLR: 0.010000\n",
            "Training Epoch: 19 [42240/50000]\tLoss: 0.9060\tLR: 0.010000\n",
            "Training Epoch: 19 [42368/50000]\tLoss: 0.9603\tLR: 0.010000\n",
            "Training Epoch: 19 [42496/50000]\tLoss: 0.8099\tLR: 0.010000\n",
            "Training Epoch: 19 [42624/50000]\tLoss: 0.7598\tLR: 0.010000\n",
            "Training Epoch: 19 [42752/50000]\tLoss: 0.8260\tLR: 0.010000\n",
            "Training Epoch: 19 [42880/50000]\tLoss: 0.9688\tLR: 0.010000\n",
            "Training Epoch: 19 [43008/50000]\tLoss: 0.8421\tLR: 0.010000\n",
            "Training Epoch: 19 [43136/50000]\tLoss: 0.9344\tLR: 0.010000\n",
            "Training Epoch: 19 [43264/50000]\tLoss: 0.7534\tLR: 0.010000\n",
            "Training Epoch: 19 [43392/50000]\tLoss: 0.8043\tLR: 0.010000\n",
            "Training Epoch: 19 [43520/50000]\tLoss: 0.8484\tLR: 0.010000\n",
            "Training Epoch: 19 [43648/50000]\tLoss: 0.7559\tLR: 0.010000\n",
            "Training Epoch: 19 [43776/50000]\tLoss: 0.8836\tLR: 0.010000\n",
            "Training Epoch: 19 [43904/50000]\tLoss: 0.8755\tLR: 0.010000\n",
            "Training Epoch: 19 [44032/50000]\tLoss: 0.6524\tLR: 0.010000\n",
            "Training Epoch: 19 [44160/50000]\tLoss: 0.8537\tLR: 0.010000\n",
            "Training Epoch: 19 [44288/50000]\tLoss: 0.8360\tLR: 0.010000\n",
            "Training Epoch: 19 [44416/50000]\tLoss: 0.6836\tLR: 0.010000\n",
            "Training Epoch: 19 [44544/50000]\tLoss: 0.7687\tLR: 0.010000\n",
            "Training Epoch: 19 [44672/50000]\tLoss: 0.9651\tLR: 0.010000\n",
            "Training Epoch: 19 [44800/50000]\tLoss: 0.9319\tLR: 0.010000\n",
            "Training Epoch: 19 [44928/50000]\tLoss: 0.7562\tLR: 0.010000\n",
            "Training Epoch: 19 [45056/50000]\tLoss: 0.7050\tLR: 0.010000\n",
            "Training Epoch: 19 [45184/50000]\tLoss: 0.8048\tLR: 0.010000\n",
            "Training Epoch: 19 [45312/50000]\tLoss: 0.8193\tLR: 0.010000\n",
            "Training Epoch: 19 [45440/50000]\tLoss: 0.8758\tLR: 0.010000\n",
            "Training Epoch: 19 [45568/50000]\tLoss: 0.8378\tLR: 0.010000\n",
            "Training Epoch: 19 [45696/50000]\tLoss: 0.8704\tLR: 0.010000\n",
            "Training Epoch: 19 [45824/50000]\tLoss: 0.8031\tLR: 0.010000\n",
            "Training Epoch: 19 [45952/50000]\tLoss: 0.7052\tLR: 0.010000\n",
            "Training Epoch: 19 [46080/50000]\tLoss: 0.7644\tLR: 0.010000\n",
            "Training Epoch: 19 [46208/50000]\tLoss: 0.6314\tLR: 0.010000\n",
            "Training Epoch: 19 [46336/50000]\tLoss: 0.8244\tLR: 0.010000\n",
            "Training Epoch: 19 [46464/50000]\tLoss: 0.8851\tLR: 0.010000\n",
            "Training Epoch: 19 [46592/50000]\tLoss: 0.6579\tLR: 0.010000\n",
            "Training Epoch: 19 [46720/50000]\tLoss: 0.8900\tLR: 0.010000\n",
            "Training Epoch: 19 [46848/50000]\tLoss: 1.0783\tLR: 0.010000\n",
            "Training Epoch: 19 [46976/50000]\tLoss: 0.8221\tLR: 0.010000\n",
            "Training Epoch: 19 [47104/50000]\tLoss: 1.1383\tLR: 0.010000\n",
            "Training Epoch: 19 [47232/50000]\tLoss: 0.6727\tLR: 0.010000\n",
            "Training Epoch: 19 [47360/50000]\tLoss: 0.8794\tLR: 0.010000\n",
            "Training Epoch: 19 [47488/50000]\tLoss: 0.7506\tLR: 0.010000\n",
            "Training Epoch: 19 [47616/50000]\tLoss: 0.9289\tLR: 0.010000\n",
            "Training Epoch: 19 [47744/50000]\tLoss: 0.9967\tLR: 0.010000\n",
            "Training Epoch: 19 [47872/50000]\tLoss: 0.8564\tLR: 0.010000\n",
            "Training Epoch: 19 [48000/50000]\tLoss: 0.6802\tLR: 0.010000\n",
            "Training Epoch: 19 [48128/50000]\tLoss: 0.7079\tLR: 0.010000\n",
            "Training Epoch: 19 [48256/50000]\tLoss: 0.7257\tLR: 0.010000\n",
            "Training Epoch: 19 [48384/50000]\tLoss: 0.8611\tLR: 0.010000\n",
            "Training Epoch: 19 [48512/50000]\tLoss: 0.9906\tLR: 0.010000\n",
            "Training Epoch: 19 [48640/50000]\tLoss: 0.9092\tLR: 0.010000\n",
            "Training Epoch: 19 [48768/50000]\tLoss: 0.9879\tLR: 0.010000\n",
            "Training Epoch: 19 [48896/50000]\tLoss: 0.9887\tLR: 0.010000\n",
            "Training Epoch: 19 [49024/50000]\tLoss: 0.9847\tLR: 0.010000\n",
            "Training Epoch: 19 [49152/50000]\tLoss: 0.8205\tLR: 0.010000\n",
            "Training Epoch: 19 [49280/50000]\tLoss: 0.7872\tLR: 0.010000\n",
            "Training Epoch: 19 [49408/50000]\tLoss: 0.8768\tLR: 0.010000\n",
            "Training Epoch: 19 [49536/50000]\tLoss: 0.7742\tLR: 0.010000\n",
            "Training Epoch: 19 [49664/50000]\tLoss: 0.8447\tLR: 0.010000\n",
            "Training Epoch: 19 [49792/50000]\tLoss: 0.8557\tLR: 0.010000\n",
            "Training Epoch: 19 [49920/50000]\tLoss: 0.8640\tLR: 0.010000\n",
            "Training Epoch: 19 [50000/50000]\tLoss: 0.8545\tLR: 0.010000\n",
            "epoch 19 training time consumed: 205.76s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 159598 GiB | 159598 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 159483 GiB | 159482 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    115 GiB |    115 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 159598 GiB | 159598 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 159483 GiB | 159482 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    115 GiB |    115 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 159582 GiB | 159581 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 159467 GiB | 159466 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |    115 GiB |    115 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB | 115902 GiB | 115902 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB | 115784 GiB | 115783 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |    118 GiB |    118 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    6336 K  |    6336 K  |\n",
            "|       from large pool |      92    |     184    |    3572 K  |    3572 K  |\n",
            "|       from small pool |     519    |     646    |    2763 K  |    2763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    6336 K  |    6336 K  |\n",
            "|       from large pool |      92    |     184    |    3572 K  |    3572 K  |\n",
            "|       from small pool |     519    |     646    |    2763 K  |    2763 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |     122    |    2674 K  |    2673 K  |\n",
            "|       from large pool |      42    |     111    |    2109 K  |    2109 K  |\n",
            "|       from small pool |       9    |      21    |     564 K  |     564 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0107, Accuracy: 0.6436, Time consumed:11.26s\n",
            "\n",
            "Training Epoch: 20 [128/50000]\tLoss: 0.6678\tLR: 0.010000\n",
            "Training Epoch: 20 [256/50000]\tLoss: 0.7054\tLR: 0.010000\n",
            "Training Epoch: 20 [384/50000]\tLoss: 0.8108\tLR: 0.010000\n",
            "Training Epoch: 20 [512/50000]\tLoss: 0.6020\tLR: 0.010000\n",
            "Training Epoch: 20 [640/50000]\tLoss: 0.7527\tLR: 0.010000\n",
            "Training Epoch: 20 [768/50000]\tLoss: 0.7961\tLR: 0.010000\n",
            "Training Epoch: 20 [896/50000]\tLoss: 0.5980\tLR: 0.010000\n",
            "Training Epoch: 20 [1024/50000]\tLoss: 0.8314\tLR: 0.010000\n",
            "Training Epoch: 20 [1152/50000]\tLoss: 0.6463\tLR: 0.010000\n",
            "Training Epoch: 20 [1280/50000]\tLoss: 0.9021\tLR: 0.010000\n",
            "Training Epoch: 20 [1408/50000]\tLoss: 0.8234\tLR: 0.010000\n",
            "Training Epoch: 20 [1536/50000]\tLoss: 0.7500\tLR: 0.010000\n",
            "Training Epoch: 20 [1664/50000]\tLoss: 0.6186\tLR: 0.010000\n",
            "Training Epoch: 20 [1792/50000]\tLoss: 0.6665\tLR: 0.010000\n",
            "Training Epoch: 20 [1920/50000]\tLoss: 0.6705\tLR: 0.010000\n",
            "Training Epoch: 20 [2048/50000]\tLoss: 0.8028\tLR: 0.010000\n",
            "Training Epoch: 20 [2176/50000]\tLoss: 0.7670\tLR: 0.010000\n",
            "Training Epoch: 20 [2304/50000]\tLoss: 0.6751\tLR: 0.010000\n",
            "Training Epoch: 20 [2432/50000]\tLoss: 0.6829\tLR: 0.010000\n",
            "Training Epoch: 20 [2560/50000]\tLoss: 0.6544\tLR: 0.010000\n",
            "Training Epoch: 20 [2688/50000]\tLoss: 0.7231\tLR: 0.010000\n",
            "Training Epoch: 20 [2816/50000]\tLoss: 0.6480\tLR: 0.010000\n",
            "Training Epoch: 20 [2944/50000]\tLoss: 0.7172\tLR: 0.010000\n",
            "Training Epoch: 20 [3072/50000]\tLoss: 0.6289\tLR: 0.010000\n",
            "Training Epoch: 20 [3200/50000]\tLoss: 0.5921\tLR: 0.010000\n",
            "Training Epoch: 20 [3328/50000]\tLoss: 0.8261\tLR: 0.010000\n",
            "Training Epoch: 20 [3456/50000]\tLoss: 0.5027\tLR: 0.010000\n",
            "Training Epoch: 20 [3584/50000]\tLoss: 0.5681\tLR: 0.010000\n",
            "Training Epoch: 20 [3712/50000]\tLoss: 0.6656\tLR: 0.010000\n",
            "Training Epoch: 20 [3840/50000]\tLoss: 0.4710\tLR: 0.010000\n",
            "Training Epoch: 20 [3968/50000]\tLoss: 0.8681\tLR: 0.010000\n",
            "Training Epoch: 20 [4096/50000]\tLoss: 0.6559\tLR: 0.010000\n",
            "Training Epoch: 20 [4224/50000]\tLoss: 0.6072\tLR: 0.010000\n",
            "Training Epoch: 20 [4352/50000]\tLoss: 0.6717\tLR: 0.010000\n",
            "Training Epoch: 20 [4480/50000]\tLoss: 0.7759\tLR: 0.010000\n",
            "Training Epoch: 20 [4608/50000]\tLoss: 0.7036\tLR: 0.010000\n",
            "Training Epoch: 20 [4736/50000]\tLoss: 0.7754\tLR: 0.010000\n",
            "Training Epoch: 20 [4864/50000]\tLoss: 0.5909\tLR: 0.010000\n",
            "Training Epoch: 20 [4992/50000]\tLoss: 0.6094\tLR: 0.010000\n",
            "Training Epoch: 20 [5120/50000]\tLoss: 0.6791\tLR: 0.010000\n",
            "Training Epoch: 20 [5248/50000]\tLoss: 0.6852\tLR: 0.010000\n",
            "Training Epoch: 20 [5376/50000]\tLoss: 0.6620\tLR: 0.010000\n",
            "Training Epoch: 20 [5504/50000]\tLoss: 0.6691\tLR: 0.010000\n",
            "Training Epoch: 20 [5632/50000]\tLoss: 0.6822\tLR: 0.010000\n",
            "Training Epoch: 20 [5760/50000]\tLoss: 0.8640\tLR: 0.010000\n",
            "Training Epoch: 20 [5888/50000]\tLoss: 0.7903\tLR: 0.010000\n",
            "Training Epoch: 20 [6016/50000]\tLoss: 0.6476\tLR: 0.010000\n",
            "Training Epoch: 20 [6144/50000]\tLoss: 0.6502\tLR: 0.010000\n",
            "Training Epoch: 20 [6272/50000]\tLoss: 0.7110\tLR: 0.010000\n",
            "Training Epoch: 20 [6400/50000]\tLoss: 0.8571\tLR: 0.010000\n",
            "Training Epoch: 20 [6528/50000]\tLoss: 0.7059\tLR: 0.010000\n",
            "Training Epoch: 20 [6656/50000]\tLoss: 0.5855\tLR: 0.010000\n",
            "Training Epoch: 20 [6784/50000]\tLoss: 0.5993\tLR: 0.010000\n",
            "Training Epoch: 20 [6912/50000]\tLoss: 0.6105\tLR: 0.010000\n",
            "Training Epoch: 20 [7040/50000]\tLoss: 0.8440\tLR: 0.010000\n",
            "Training Epoch: 20 [7168/50000]\tLoss: 0.6892\tLR: 0.010000\n",
            "Training Epoch: 20 [7296/50000]\tLoss: 0.8135\tLR: 0.010000\n",
            "Training Epoch: 20 [7424/50000]\tLoss: 0.7877\tLR: 0.010000\n",
            "Training Epoch: 20 [7552/50000]\tLoss: 0.6414\tLR: 0.010000\n",
            "Training Epoch: 20 [7680/50000]\tLoss: 0.6189\tLR: 0.010000\n",
            "Training Epoch: 20 [7808/50000]\tLoss: 0.6781\tLR: 0.010000\n",
            "Training Epoch: 20 [7936/50000]\tLoss: 0.7661\tLR: 0.010000\n",
            "Training Epoch: 20 [8064/50000]\tLoss: 0.5387\tLR: 0.010000\n",
            "Training Epoch: 20 [8192/50000]\tLoss: 0.7291\tLR: 0.010000\n",
            "Training Epoch: 20 [8320/50000]\tLoss: 0.5855\tLR: 0.010000\n",
            "Training Epoch: 20 [8448/50000]\tLoss: 0.5738\tLR: 0.010000\n",
            "Training Epoch: 20 [8576/50000]\tLoss: 0.6560\tLR: 0.010000\n",
            "Training Epoch: 20 [8704/50000]\tLoss: 0.6818\tLR: 0.010000\n",
            "Training Epoch: 20 [8832/50000]\tLoss: 0.7665\tLR: 0.010000\n",
            "Training Epoch: 20 [8960/50000]\tLoss: 0.6218\tLR: 0.010000\n",
            "Training Epoch: 20 [9088/50000]\tLoss: 0.8725\tLR: 0.010000\n",
            "Training Epoch: 20 [9216/50000]\tLoss: 0.6564\tLR: 0.010000\n",
            "Training Epoch: 20 [9344/50000]\tLoss: 0.6545\tLR: 0.010000\n",
            "Training Epoch: 20 [9472/50000]\tLoss: 0.6465\tLR: 0.010000\n",
            "Training Epoch: 20 [9600/50000]\tLoss: 0.7530\tLR: 0.010000\n",
            "Training Epoch: 20 [9728/50000]\tLoss: 0.6891\tLR: 0.010000\n",
            "Training Epoch: 20 [9856/50000]\tLoss: 0.6663\tLR: 0.010000\n",
            "Training Epoch: 20 [9984/50000]\tLoss: 0.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [10112/50000]\tLoss: 0.6328\tLR: 0.010000\n",
            "Training Epoch: 20 [10240/50000]\tLoss: 0.8525\tLR: 0.010000\n",
            "Training Epoch: 20 [10368/50000]\tLoss: 0.7535\tLR: 0.010000\n",
            "Training Epoch: 20 [10496/50000]\tLoss: 0.8046\tLR: 0.010000\n",
            "Training Epoch: 20 [10624/50000]\tLoss: 0.6702\tLR: 0.010000\n",
            "Training Epoch: 20 [10752/50000]\tLoss: 0.7100\tLR: 0.010000\n",
            "Training Epoch: 20 [10880/50000]\tLoss: 0.5756\tLR: 0.010000\n",
            "Training Epoch: 20 [11008/50000]\tLoss: 0.8375\tLR: 0.010000\n",
            "Training Epoch: 20 [11136/50000]\tLoss: 0.6690\tLR: 0.010000\n",
            "Training Epoch: 20 [11264/50000]\tLoss: 0.6447\tLR: 0.010000\n",
            "Training Epoch: 20 [11392/50000]\tLoss: 0.7233\tLR: 0.010000\n",
            "Training Epoch: 20 [11520/50000]\tLoss: 0.6131\tLR: 0.010000\n",
            "Training Epoch: 20 [11648/50000]\tLoss: 0.6682\tLR: 0.010000\n",
            "Training Epoch: 20 [11776/50000]\tLoss: 0.9098\tLR: 0.010000\n",
            "Training Epoch: 20 [11904/50000]\tLoss: 0.7069\tLR: 0.010000\n",
            "Training Epoch: 20 [12032/50000]\tLoss: 0.7318\tLR: 0.010000\n",
            "Training Epoch: 20 [12160/50000]\tLoss: 0.7561\tLR: 0.010000\n",
            "Training Epoch: 20 [12288/50000]\tLoss: 0.9122\tLR: 0.010000\n",
            "Training Epoch: 20 [12416/50000]\tLoss: 0.4909\tLR: 0.010000\n",
            "Training Epoch: 20 [12544/50000]\tLoss: 0.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [12672/50000]\tLoss: 0.8652\tLR: 0.010000\n",
            "Training Epoch: 20 [12800/50000]\tLoss: 0.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [12928/50000]\tLoss: 0.7404\tLR: 0.010000\n",
            "Training Epoch: 20 [13056/50000]\tLoss: 0.6508\tLR: 0.010000\n",
            "Training Epoch: 20 [13184/50000]\tLoss: 0.5830\tLR: 0.010000\n",
            "Training Epoch: 20 [13312/50000]\tLoss: 0.7207\tLR: 0.010000\n",
            "Training Epoch: 20 [13440/50000]\tLoss: 0.9121\tLR: 0.010000\n",
            "Training Epoch: 20 [13568/50000]\tLoss: 0.6946\tLR: 0.010000\n",
            "Training Epoch: 20 [13696/50000]\tLoss: 0.6453\tLR: 0.010000\n",
            "Training Epoch: 20 [13824/50000]\tLoss: 0.7534\tLR: 0.010000\n",
            "Training Epoch: 20 [13952/50000]\tLoss: 0.6586\tLR: 0.010000\n",
            "Training Epoch: 20 [14080/50000]\tLoss: 0.7756\tLR: 0.010000\n",
            "Training Epoch: 20 [14208/50000]\tLoss: 0.5878\tLR: 0.010000\n",
            "Training Epoch: 20 [14336/50000]\tLoss: 0.7589\tLR: 0.010000\n",
            "Training Epoch: 20 [14464/50000]\tLoss: 0.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [14592/50000]\tLoss: 0.6739\tLR: 0.010000\n",
            "Training Epoch: 20 [14720/50000]\tLoss: 0.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [14848/50000]\tLoss: 0.8412\tLR: 0.010000\n",
            "Training Epoch: 20 [14976/50000]\tLoss: 0.8272\tLR: 0.010000\n",
            "Training Epoch: 20 [15104/50000]\tLoss: 0.7791\tLR: 0.010000\n",
            "Training Epoch: 20 [15232/50000]\tLoss: 0.7718\tLR: 0.010000\n",
            "Training Epoch: 20 [15360/50000]\tLoss: 0.7689\tLR: 0.010000\n",
            "Training Epoch: 20 [15488/50000]\tLoss: 0.6720\tLR: 0.010000\n",
            "Training Epoch: 20 [15616/50000]\tLoss: 0.7265\tLR: 0.010000\n",
            "Training Epoch: 20 [15744/50000]\tLoss: 0.7168\tLR: 0.010000\n",
            "Training Epoch: 20 [15872/50000]\tLoss: 0.7386\tLR: 0.010000\n",
            "Training Epoch: 20 [16000/50000]\tLoss: 0.5663\tLR: 0.010000\n",
            "Training Epoch: 20 [16128/50000]\tLoss: 0.9472\tLR: 0.010000\n",
            "Training Epoch: 20 [16256/50000]\tLoss: 0.7897\tLR: 0.010000\n",
            "Training Epoch: 20 [16384/50000]\tLoss: 0.7574\tLR: 0.010000\n",
            "Training Epoch: 20 [16512/50000]\tLoss: 0.7485\tLR: 0.010000\n",
            "Training Epoch: 20 [16640/50000]\tLoss: 0.7846\tLR: 0.010000\n",
            "Training Epoch: 20 [16768/50000]\tLoss: 0.6980\tLR: 0.010000\n",
            "Training Epoch: 20 [16896/50000]\tLoss: 0.7264\tLR: 0.010000\n",
            "Training Epoch: 20 [17024/50000]\tLoss: 0.7748\tLR: 0.010000\n",
            "Training Epoch: 20 [17152/50000]\tLoss: 0.9492\tLR: 0.010000\n",
            "Training Epoch: 20 [17280/50000]\tLoss: 0.9873\tLR: 0.010000\n",
            "Training Epoch: 20 [17408/50000]\tLoss: 0.6926\tLR: 0.010000\n",
            "Training Epoch: 20 [17536/50000]\tLoss: 0.6493\tLR: 0.010000\n",
            "Training Epoch: 20 [17664/50000]\tLoss: 0.7084\tLR: 0.010000\n",
            "Training Epoch: 20 [17792/50000]\tLoss: 0.6923\tLR: 0.010000\n",
            "Training Epoch: 20 [17920/50000]\tLoss: 0.7891\tLR: 0.010000\n",
            "Training Epoch: 20 [18048/50000]\tLoss: 0.9379\tLR: 0.010000\n",
            "Training Epoch: 20 [18176/50000]\tLoss: 0.5826\tLR: 0.010000\n",
            "Training Epoch: 20 [18304/50000]\tLoss: 0.7778\tLR: 0.010000\n",
            "Training Epoch: 20 [18432/50000]\tLoss: 0.7315\tLR: 0.010000\n",
            "Training Epoch: 20 [18560/50000]\tLoss: 0.7539\tLR: 0.010000\n",
            "Training Epoch: 20 [18688/50000]\tLoss: 0.7176\tLR: 0.010000\n",
            "Training Epoch: 20 [18816/50000]\tLoss: 0.5738\tLR: 0.010000\n",
            "Training Epoch: 20 [18944/50000]\tLoss: 0.7428\tLR: 0.010000\n",
            "Training Epoch: 20 [19072/50000]\tLoss: 0.7047\tLR: 0.010000\n",
            "Training Epoch: 20 [19200/50000]\tLoss: 0.9208\tLR: 0.010000\n",
            "Training Epoch: 20 [19328/50000]\tLoss: 0.7437\tLR: 0.010000\n",
            "Training Epoch: 20 [19456/50000]\tLoss: 0.7963\tLR: 0.010000\n",
            "Training Epoch: 20 [19584/50000]\tLoss: 0.7593\tLR: 0.010000\n",
            "Training Epoch: 20 [19712/50000]\tLoss: 0.7246\tLR: 0.010000\n",
            "Training Epoch: 20 [19840/50000]\tLoss: 0.6916\tLR: 0.010000\n",
            "Training Epoch: 20 [19968/50000]\tLoss: 0.7633\tLR: 0.010000\n",
            "Training Epoch: 20 [20096/50000]\tLoss: 0.5594\tLR: 0.010000\n",
            "Training Epoch: 20 [20224/50000]\tLoss: 0.9820\tLR: 0.010000\n",
            "Training Epoch: 20 [20352/50000]\tLoss: 0.6297\tLR: 0.010000\n",
            "Training Epoch: 20 [20480/50000]\tLoss: 0.7287\tLR: 0.010000\n",
            "Training Epoch: 20 [20608/50000]\tLoss: 0.8448\tLR: 0.010000\n",
            "Training Epoch: 20 [20736/50000]\tLoss: 0.6367\tLR: 0.010000\n",
            "Training Epoch: 20 [20864/50000]\tLoss: 0.7759\tLR: 0.010000\n",
            "Training Epoch: 20 [20992/50000]\tLoss: 0.8248\tLR: 0.010000\n",
            "Training Epoch: 20 [21120/50000]\tLoss: 0.8548\tLR: 0.010000\n",
            "Training Epoch: 20 [21248/50000]\tLoss: 0.6485\tLR: 0.010000\n",
            "Training Epoch: 20 [21376/50000]\tLoss: 0.6175\tLR: 0.010000\n",
            "Training Epoch: 20 [21504/50000]\tLoss: 0.7959\tLR: 0.010000\n",
            "Training Epoch: 20 [21632/50000]\tLoss: 0.6143\tLR: 0.010000\n",
            "Training Epoch: 20 [21760/50000]\tLoss: 0.7218\tLR: 0.010000\n",
            "Training Epoch: 20 [21888/50000]\tLoss: 0.7506\tLR: 0.010000\n",
            "Training Epoch: 20 [22016/50000]\tLoss: 0.5919\tLR: 0.010000\n",
            "Training Epoch: 20 [22144/50000]\tLoss: 0.7973\tLR: 0.010000\n",
            "Training Epoch: 20 [22272/50000]\tLoss: 0.9839\tLR: 0.010000\n",
            "Training Epoch: 20 [22400/50000]\tLoss: 0.8148\tLR: 0.010000\n",
            "Training Epoch: 20 [22528/50000]\tLoss: 0.8777\tLR: 0.010000\n",
            "Training Epoch: 20 [22656/50000]\tLoss: 0.7709\tLR: 0.010000\n",
            "Training Epoch: 20 [22784/50000]\tLoss: 0.7928\tLR: 0.010000\n",
            "Training Epoch: 20 [22912/50000]\tLoss: 0.6200\tLR: 0.010000\n",
            "Training Epoch: 20 [23040/50000]\tLoss: 0.6183\tLR: 0.010000\n",
            "Training Epoch: 20 [23168/50000]\tLoss: 0.7861\tLR: 0.010000\n",
            "Training Epoch: 20 [23296/50000]\tLoss: 0.8039\tLR: 0.010000\n",
            "Training Epoch: 20 [23424/50000]\tLoss: 0.7612\tLR: 0.010000\n",
            "Training Epoch: 20 [23552/50000]\tLoss: 0.7510\tLR: 0.010000\n",
            "Training Epoch: 20 [23680/50000]\tLoss: 0.6818\tLR: 0.010000\n",
            "Training Epoch: 20 [23808/50000]\tLoss: 0.8601\tLR: 0.010000\n",
            "Training Epoch: 20 [23936/50000]\tLoss: 0.9642\tLR: 0.010000\n",
            "Training Epoch: 20 [24064/50000]\tLoss: 0.8088\tLR: 0.010000\n",
            "Training Epoch: 20 [24192/50000]\tLoss: 0.7975\tLR: 0.010000\n",
            "Training Epoch: 20 [24320/50000]\tLoss: 0.7795\tLR: 0.010000\n",
            "Training Epoch: 20 [24448/50000]\tLoss: 0.9004\tLR: 0.010000\n",
            "Training Epoch: 20 [24576/50000]\tLoss: 0.8550\tLR: 0.010000\n",
            "Training Epoch: 20 [24704/50000]\tLoss: 0.7583\tLR: 0.010000\n",
            "Training Epoch: 20 [24832/50000]\tLoss: 0.8063\tLR: 0.010000\n",
            "Training Epoch: 20 [24960/50000]\tLoss: 0.7258\tLR: 0.010000\n",
            "Training Epoch: 20 [25088/50000]\tLoss: 0.5577\tLR: 0.010000\n",
            "Training Epoch: 20 [25216/50000]\tLoss: 0.6199\tLR: 0.010000\n",
            "Training Epoch: 20 [25344/50000]\tLoss: 0.6586\tLR: 0.010000\n",
            "Training Epoch: 20 [25472/50000]\tLoss: 0.8838\tLR: 0.010000\n",
            "Training Epoch: 20 [25600/50000]\tLoss: 0.9766\tLR: 0.010000\n",
            "Training Epoch: 20 [25728/50000]\tLoss: 0.8324\tLR: 0.010000\n",
            "Training Epoch: 20 [25856/50000]\tLoss: 0.6312\tLR: 0.010000\n",
            "Training Epoch: 20 [25984/50000]\tLoss: 0.6351\tLR: 0.010000\n",
            "Training Epoch: 20 [26112/50000]\tLoss: 0.6806\tLR: 0.010000\n",
            "Training Epoch: 20 [26240/50000]\tLoss: 0.7521\tLR: 0.010000\n",
            "Training Epoch: 20 [26368/50000]\tLoss: 0.6396\tLR: 0.010000\n",
            "Training Epoch: 20 [26496/50000]\tLoss: 0.7147\tLR: 0.010000\n",
            "Training Epoch: 20 [26624/50000]\tLoss: 0.9237\tLR: 0.010000\n",
            "Training Epoch: 20 [26752/50000]\tLoss: 0.6897\tLR: 0.010000\n",
            "Training Epoch: 20 [26880/50000]\tLoss: 0.8899\tLR: 0.010000\n",
            "Training Epoch: 20 [27008/50000]\tLoss: 0.7835\tLR: 0.010000\n",
            "Training Epoch: 20 [27136/50000]\tLoss: 0.7952\tLR: 0.010000\n",
            "Training Epoch: 20 [27264/50000]\tLoss: 0.7849\tLR: 0.010000\n",
            "Training Epoch: 20 [27392/50000]\tLoss: 0.7503\tLR: 0.010000\n",
            "Training Epoch: 20 [27520/50000]\tLoss: 0.8344\tLR: 0.010000\n",
            "Training Epoch: 20 [27648/50000]\tLoss: 0.7982\tLR: 0.010000\n",
            "Training Epoch: 20 [27776/50000]\tLoss: 0.7843\tLR: 0.010000\n",
            "Training Epoch: 20 [27904/50000]\tLoss: 0.7785\tLR: 0.010000\n",
            "Training Epoch: 20 [28032/50000]\tLoss: 0.7920\tLR: 0.010000\n",
            "Training Epoch: 20 [28160/50000]\tLoss: 0.6229\tLR: 0.010000\n",
            "Training Epoch: 20 [28288/50000]\tLoss: 0.6992\tLR: 0.010000\n",
            "Training Epoch: 20 [28416/50000]\tLoss: 0.7767\tLR: 0.010000\n",
            "Training Epoch: 20 [28544/50000]\tLoss: 0.8750\tLR: 0.010000\n",
            "Training Epoch: 20 [28672/50000]\tLoss: 0.8592\tLR: 0.010000\n",
            "Training Epoch: 20 [28800/50000]\tLoss: 0.5828\tLR: 0.010000\n",
            "Training Epoch: 20 [28928/50000]\tLoss: 0.7478\tLR: 0.010000\n",
            "Training Epoch: 20 [29056/50000]\tLoss: 0.8231\tLR: 0.010000\n",
            "Training Epoch: 20 [29184/50000]\tLoss: 0.8307\tLR: 0.010000\n",
            "Training Epoch: 20 [29312/50000]\tLoss: 0.5633\tLR: 0.010000\n",
            "Training Epoch: 20 [29440/50000]\tLoss: 0.6851\tLR: 0.010000\n",
            "Training Epoch: 20 [29568/50000]\tLoss: 0.6606\tLR: 0.010000\n",
            "Training Epoch: 20 [29696/50000]\tLoss: 0.7762\tLR: 0.010000\n",
            "Training Epoch: 20 [29824/50000]\tLoss: 0.8378\tLR: 0.010000\n",
            "Training Epoch: 20 [29952/50000]\tLoss: 0.8273\tLR: 0.010000\n",
            "Training Epoch: 20 [30080/50000]\tLoss: 0.6842\tLR: 0.010000\n",
            "Training Epoch: 20 [30208/50000]\tLoss: 0.9517\tLR: 0.010000\n",
            "Training Epoch: 20 [30336/50000]\tLoss: 0.6190\tLR: 0.010000\n",
            "Training Epoch: 20 [30464/50000]\tLoss: 0.7907\tLR: 0.010000\n",
            "Training Epoch: 20 [30592/50000]\tLoss: 0.7059\tLR: 0.010000\n",
            "Training Epoch: 20 [30720/50000]\tLoss: 0.7612\tLR: 0.010000\n",
            "Training Epoch: 20 [30848/50000]\tLoss: 0.7965\tLR: 0.010000\n",
            "Training Epoch: 20 [30976/50000]\tLoss: 0.8485\tLR: 0.010000\n",
            "Training Epoch: 20 [31104/50000]\tLoss: 0.8302\tLR: 0.010000\n",
            "Training Epoch: 20 [31232/50000]\tLoss: 0.7679\tLR: 0.010000\n",
            "Training Epoch: 20 [31360/50000]\tLoss: 0.7953\tLR: 0.010000\n",
            "Training Epoch: 20 [31488/50000]\tLoss: 0.9209\tLR: 0.010000\n",
            "Training Epoch: 20 [31616/50000]\tLoss: 0.8085\tLR: 0.010000\n",
            "Training Epoch: 20 [31744/50000]\tLoss: 0.8067\tLR: 0.010000\n",
            "Training Epoch: 20 [31872/50000]\tLoss: 0.8316\tLR: 0.010000\n",
            "Training Epoch: 20 [32000/50000]\tLoss: 0.8635\tLR: 0.010000\n",
            "Training Epoch: 20 [32128/50000]\tLoss: 0.6700\tLR: 0.010000\n",
            "Training Epoch: 20 [32256/50000]\tLoss: 0.7246\tLR: 0.010000\n",
            "Training Epoch: 20 [32384/50000]\tLoss: 0.7661\tLR: 0.010000\n",
            "Training Epoch: 20 [32512/50000]\tLoss: 0.7342\tLR: 0.010000\n",
            "Training Epoch: 20 [32640/50000]\tLoss: 0.8976\tLR: 0.010000\n",
            "Training Epoch: 20 [32768/50000]\tLoss: 0.7506\tLR: 0.010000\n",
            "Training Epoch: 20 [32896/50000]\tLoss: 0.7828\tLR: 0.010000\n",
            "Training Epoch: 20 [33024/50000]\tLoss: 0.7900\tLR: 0.010000\n",
            "Training Epoch: 20 [33152/50000]\tLoss: 0.6305\tLR: 0.010000\n",
            "Training Epoch: 20 [33280/50000]\tLoss: 0.8787\tLR: 0.010000\n",
            "Training Epoch: 20 [33408/50000]\tLoss: 0.8542\tLR: 0.010000\n",
            "Training Epoch: 20 [33536/50000]\tLoss: 0.9732\tLR: 0.010000\n",
            "Training Epoch: 20 [33664/50000]\tLoss: 0.7131\tLR: 0.010000\n",
            "Training Epoch: 20 [33792/50000]\tLoss: 0.6456\tLR: 0.010000\n",
            "Training Epoch: 20 [33920/50000]\tLoss: 0.7316\tLR: 0.010000\n",
            "Training Epoch: 20 [34048/50000]\tLoss: 0.8123\tLR: 0.010000\n",
            "Training Epoch: 20 [34176/50000]\tLoss: 0.8931\tLR: 0.010000\n",
            "Training Epoch: 20 [34304/50000]\tLoss: 0.6770\tLR: 0.010000\n",
            "Training Epoch: 20 [34432/50000]\tLoss: 0.6631\tLR: 0.010000\n",
            "Training Epoch: 20 [34560/50000]\tLoss: 0.7288\tLR: 0.010000\n",
            "Training Epoch: 20 [34688/50000]\tLoss: 0.9649\tLR: 0.010000\n",
            "Training Epoch: 20 [34816/50000]\tLoss: 0.7110\tLR: 0.010000\n",
            "Training Epoch: 20 [34944/50000]\tLoss: 0.7964\tLR: 0.010000\n",
            "Training Epoch: 20 [35072/50000]\tLoss: 0.7935\tLR: 0.010000\n",
            "Training Epoch: 20 [35200/50000]\tLoss: 0.6567\tLR: 0.010000\n",
            "Training Epoch: 20 [35328/50000]\tLoss: 0.6680\tLR: 0.010000\n",
            "Training Epoch: 20 [35456/50000]\tLoss: 0.8193\tLR: 0.010000\n",
            "Training Epoch: 20 [35584/50000]\tLoss: 0.6526\tLR: 0.010000\n",
            "Training Epoch: 20 [35712/50000]\tLoss: 0.6138\tLR: 0.010000\n",
            "Training Epoch: 20 [35840/50000]\tLoss: 0.5861\tLR: 0.010000\n",
            "Training Epoch: 20 [35968/50000]\tLoss: 0.9683\tLR: 0.010000\n",
            "Training Epoch: 20 [36096/50000]\tLoss: 0.7585\tLR: 0.010000\n",
            "Training Epoch: 20 [36224/50000]\tLoss: 0.7309\tLR: 0.010000\n",
            "Training Epoch: 20 [36352/50000]\tLoss: 0.8006\tLR: 0.010000\n",
            "Training Epoch: 20 [36480/50000]\tLoss: 0.7610\tLR: 0.010000\n",
            "Training Epoch: 20 [36608/50000]\tLoss: 0.9201\tLR: 0.010000\n",
            "Training Epoch: 20 [36736/50000]\tLoss: 0.6398\tLR: 0.010000\n",
            "Training Epoch: 20 [36864/50000]\tLoss: 0.7859\tLR: 0.010000\n",
            "Training Epoch: 20 [36992/50000]\tLoss: 0.7592\tLR: 0.010000\n",
            "Training Epoch: 20 [37120/50000]\tLoss: 0.7926\tLR: 0.010000\n",
            "Training Epoch: 20 [37248/50000]\tLoss: 0.8530\tLR: 0.010000\n",
            "Training Epoch: 20 [37376/50000]\tLoss: 0.7817\tLR: 0.010000\n",
            "Training Epoch: 20 [37504/50000]\tLoss: 0.7570\tLR: 0.010000\n",
            "Training Epoch: 20 [37632/50000]\tLoss: 0.7612\tLR: 0.010000\n",
            "Training Epoch: 20 [37760/50000]\tLoss: 0.7663\tLR: 0.010000\n",
            "Training Epoch: 20 [37888/50000]\tLoss: 0.8575\tLR: 0.010000\n",
            "Training Epoch: 20 [38016/50000]\tLoss: 0.7741\tLR: 0.010000\n",
            "Training Epoch: 20 [38144/50000]\tLoss: 0.8286\tLR: 0.010000\n",
            "Training Epoch: 20 [38272/50000]\tLoss: 0.7155\tLR: 0.010000\n",
            "Training Epoch: 20 [38400/50000]\tLoss: 0.7122\tLR: 0.010000\n",
            "Training Epoch: 20 [38528/50000]\tLoss: 0.6565\tLR: 0.010000\n",
            "Training Epoch: 20 [38656/50000]\tLoss: 1.0739\tLR: 0.010000\n",
            "Training Epoch: 20 [38784/50000]\tLoss: 0.6333\tLR: 0.010000\n",
            "Training Epoch: 20 [38912/50000]\tLoss: 0.8416\tLR: 0.010000\n",
            "Training Epoch: 20 [39040/50000]\tLoss: 0.8284\tLR: 0.010000\n",
            "Training Epoch: 20 [39168/50000]\tLoss: 0.8806\tLR: 0.010000\n",
            "Training Epoch: 20 [39296/50000]\tLoss: 0.8209\tLR: 0.010000\n",
            "Training Epoch: 20 [39424/50000]\tLoss: 0.8208\tLR: 0.010000\n",
            "Training Epoch: 20 [39552/50000]\tLoss: 0.6923\tLR: 0.010000\n",
            "Training Epoch: 20 [39680/50000]\tLoss: 0.9144\tLR: 0.010000\n",
            "Training Epoch: 20 [39808/50000]\tLoss: 0.7512\tLR: 0.010000\n",
            "Training Epoch: 20 [39936/50000]\tLoss: 0.7618\tLR: 0.010000\n",
            "Training Epoch: 20 [40064/50000]\tLoss: 0.8331\tLR: 0.010000\n",
            "Training Epoch: 20 [40192/50000]\tLoss: 0.8092\tLR: 0.010000\n",
            "Training Epoch: 20 [40320/50000]\tLoss: 0.6833\tLR: 0.010000\n",
            "Training Epoch: 20 [40448/50000]\tLoss: 0.9843\tLR: 0.010000\n",
            "Training Epoch: 20 [40576/50000]\tLoss: 0.7582\tLR: 0.010000\n",
            "Training Epoch: 20 [40704/50000]\tLoss: 0.8154\tLR: 0.010000\n",
            "Training Epoch: 20 [40832/50000]\tLoss: 0.5893\tLR: 0.010000\n",
            "Training Epoch: 20 [40960/50000]\tLoss: 0.5797\tLR: 0.010000\n",
            "Training Epoch: 20 [41088/50000]\tLoss: 0.7422\tLR: 0.010000\n",
            "Training Epoch: 20 [41216/50000]\tLoss: 0.9089\tLR: 0.010000\n",
            "Training Epoch: 20 [41344/50000]\tLoss: 0.7746\tLR: 0.010000\n",
            "Training Epoch: 20 [41472/50000]\tLoss: 0.7132\tLR: 0.010000\n",
            "Training Epoch: 20 [41600/50000]\tLoss: 0.9802\tLR: 0.010000\n",
            "Training Epoch: 20 [41728/50000]\tLoss: 0.6986\tLR: 0.010000\n",
            "Training Epoch: 20 [41856/50000]\tLoss: 0.7934\tLR: 0.010000\n",
            "Training Epoch: 20 [41984/50000]\tLoss: 0.7765\tLR: 0.010000\n",
            "Training Epoch: 20 [42112/50000]\tLoss: 0.9164\tLR: 0.010000\n",
            "Training Epoch: 20 [42240/50000]\tLoss: 0.7566\tLR: 0.010000\n",
            "Training Epoch: 20 [42368/50000]\tLoss: 0.6563\tLR: 0.010000\n",
            "Training Epoch: 20 [42496/50000]\tLoss: 0.7788\tLR: 0.010000\n",
            "Training Epoch: 20 [42624/50000]\tLoss: 0.7856\tLR: 0.010000\n",
            "Training Epoch: 20 [42752/50000]\tLoss: 0.9746\tLR: 0.010000\n",
            "Training Epoch: 20 [42880/50000]\tLoss: 0.6793\tLR: 0.010000\n",
            "Training Epoch: 20 [43008/50000]\tLoss: 0.8467\tLR: 0.010000\n",
            "Training Epoch: 20 [43136/50000]\tLoss: 0.9621\tLR: 0.010000\n",
            "Training Epoch: 20 [43264/50000]\tLoss: 0.6689\tLR: 0.010000\n",
            "Training Epoch: 20 [43392/50000]\tLoss: 0.6489\tLR: 0.010000\n",
            "Training Epoch: 20 [43520/50000]\tLoss: 1.0091\tLR: 0.010000\n",
            "Training Epoch: 20 [43648/50000]\tLoss: 0.9117\tLR: 0.010000\n",
            "Training Epoch: 20 [43776/50000]\tLoss: 0.8643\tLR: 0.010000\n",
            "Training Epoch: 20 [43904/50000]\tLoss: 0.6496\tLR: 0.010000\n",
            "Training Epoch: 20 [44032/50000]\tLoss: 0.7490\tLR: 0.010000\n",
            "Training Epoch: 20 [44160/50000]\tLoss: 0.8467\tLR: 0.010000\n",
            "Training Epoch: 20 [44288/50000]\tLoss: 0.7489\tLR: 0.010000\n",
            "Training Epoch: 20 [44416/50000]\tLoss: 0.7710\tLR: 0.010000\n",
            "Training Epoch: 20 [44544/50000]\tLoss: 0.6996\tLR: 0.010000\n",
            "Training Epoch: 20 [44672/50000]\tLoss: 0.8767\tLR: 0.010000\n",
            "Training Epoch: 20 [44800/50000]\tLoss: 0.8082\tLR: 0.010000\n",
            "Training Epoch: 20 [44928/50000]\tLoss: 0.7971\tLR: 0.010000\n",
            "Training Epoch: 20 [45056/50000]\tLoss: 0.7710\tLR: 0.010000\n",
            "Training Epoch: 20 [45184/50000]\tLoss: 0.7122\tLR: 0.010000\n",
            "Training Epoch: 20 [45312/50000]\tLoss: 0.8233\tLR: 0.010000\n",
            "Training Epoch: 20 [45440/50000]\tLoss: 0.6813\tLR: 0.010000\n",
            "Training Epoch: 20 [45568/50000]\tLoss: 1.0042\tLR: 0.010000\n",
            "Training Epoch: 20 [45696/50000]\tLoss: 0.7684\tLR: 0.010000\n",
            "Training Epoch: 20 [45824/50000]\tLoss: 0.7760\tLR: 0.010000\n",
            "Training Epoch: 20 [45952/50000]\tLoss: 0.9185\tLR: 0.010000\n",
            "Training Epoch: 20 [46080/50000]\tLoss: 0.7083\tLR: 0.010000\n",
            "Training Epoch: 20 [46208/50000]\tLoss: 0.7324\tLR: 0.010000\n",
            "Training Epoch: 20 [46336/50000]\tLoss: 1.0564\tLR: 0.010000\n",
            "Training Epoch: 20 [46464/50000]\tLoss: 0.8716\tLR: 0.010000\n",
            "Training Epoch: 20 [46592/50000]\tLoss: 0.7763\tLR: 0.010000\n",
            "Training Epoch: 20 [46720/50000]\tLoss: 0.7965\tLR: 0.010000\n",
            "Training Epoch: 20 [46848/50000]\tLoss: 0.7546\tLR: 0.010000\n",
            "Training Epoch: 20 [46976/50000]\tLoss: 0.7373\tLR: 0.010000\n",
            "Training Epoch: 20 [47104/50000]\tLoss: 0.7733\tLR: 0.010000\n",
            "Training Epoch: 20 [47232/50000]\tLoss: 0.7660\tLR: 0.010000\n",
            "Training Epoch: 20 [47360/50000]\tLoss: 0.8000\tLR: 0.010000\n",
            "Training Epoch: 20 [47488/50000]\tLoss: 0.9014\tLR: 0.010000\n",
            "Training Epoch: 20 [47616/50000]\tLoss: 0.6947\tLR: 0.010000\n",
            "Training Epoch: 20 [47744/50000]\tLoss: 0.8080\tLR: 0.010000\n",
            "Training Epoch: 20 [47872/50000]\tLoss: 0.9420\tLR: 0.010000\n",
            "Training Epoch: 20 [48000/50000]\tLoss: 0.8374\tLR: 0.010000\n",
            "Training Epoch: 20 [48128/50000]\tLoss: 0.9357\tLR: 0.010000\n",
            "Training Epoch: 20 [48256/50000]\tLoss: 0.8132\tLR: 0.010000\n",
            "Training Epoch: 20 [48384/50000]\tLoss: 0.7073\tLR: 0.010000\n",
            "Training Epoch: 20 [48512/50000]\tLoss: 0.6835\tLR: 0.010000\n",
            "Training Epoch: 20 [48640/50000]\tLoss: 1.0768\tLR: 0.010000\n",
            "Training Epoch: 20 [48768/50000]\tLoss: 0.7957\tLR: 0.010000\n",
            "Training Epoch: 20 [48896/50000]\tLoss: 0.6548\tLR: 0.010000\n",
            "Training Epoch: 20 [49024/50000]\tLoss: 0.7135\tLR: 0.010000\n",
            "Training Epoch: 20 [49152/50000]\tLoss: 0.5539\tLR: 0.010000\n",
            "Training Epoch: 20 [49280/50000]\tLoss: 0.7342\tLR: 0.010000\n",
            "Training Epoch: 20 [49408/50000]\tLoss: 0.7801\tLR: 0.010000\n",
            "Training Epoch: 20 [49536/50000]\tLoss: 0.9544\tLR: 0.010000\n",
            "Training Epoch: 20 [49664/50000]\tLoss: 0.8034\tLR: 0.010000\n",
            "Training Epoch: 20 [49792/50000]\tLoss: 0.6767\tLR: 0.010000\n",
            "Training Epoch: 20 [49920/50000]\tLoss: 0.7856\tLR: 0.010000\n",
            "Training Epoch: 20 [50000/50000]\tLoss: 1.2078\tLR: 0.010000\n",
            "epoch 20 training time consumed: 206.24s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 266495 KiB |   4709 MiB | 167997 GiB | 167997 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 167875 GiB | 167875 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    121 GiB |    121 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 266495 KiB |   4709 MiB | 167997 GiB | 167997 GiB |\n",
            "|       from large pool | 253964 KiB |   4700 MiB | 167875 GiB | 167875 GiB |\n",
            "|       from small pool |  12531 KiB |     27 MiB |    121 GiB |    121 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 263321 KiB |   4707 MiB | 167980 GiB | 167979 GiB |\n",
            "|       from large pool | 250887 KiB |   4698 MiB | 167858 GiB | 167858 GiB |\n",
            "|       from small pool |  12434 KiB |     27 MiB |    121 GiB |    121 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   6534 MiB |   6534 MiB |   6534 MiB |      0 B   |\n",
            "|       from large pool |   6504 MiB |   6504 MiB |   6504 MiB |      0 B   |\n",
            "|       from small pool |     30 MiB |     30 MiB |     30 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 454400 KiB |   2381 MiB | 122002 GiB | 122002 GiB |\n",
            "|       from large pool | 450548 KiB |   2375 MiB | 121877 GiB | 121877 GiB |\n",
            "|       from small pool |   3852 KiB |      9 MiB |    125 GiB |    125 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     611    |     768    |    6670 K  |    6669 K  |\n",
            "|       from large pool |      92    |     184    |    3760 K  |    3760 K  |\n",
            "|       from small pool |     519    |     646    |    2909 K  |    2908 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     611    |     768    |    6670 K  |    6669 K  |\n",
            "|       from large pool |      92    |     184    |    3760 K  |    3760 K  |\n",
            "|       from small pool |     519    |     646    |    2909 K  |    2908 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     136    |     136    |     136    |       0    |\n",
            "|       from large pool |     121    |     121    |     121    |       0    |\n",
            "|       from small pool |      15    |      15    |      15    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |     122    |    2814 K  |    2814 K  |\n",
            "|       from large pool |      42    |     111    |    2220 K  |    2220 K  |\n",
            "|       from small pool |       8    |      21    |     594 K  |     594 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0108, Accuracy: 0.6390, Time consumed:11.22s\n",
            "\n",
            "saving weights file to checkpoint/xception/Tuesday_25_July_2023_11h_34m_35s/xception-20-regular.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test.py -net xception -weights checkpoint/xception/Tuesday_25_July_2023_11h_34m_35s/xception-20-regular.pth"
      ],
      "metadata": {
        "id": "HyVmQ_f0DMcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "854bfae3-5292-4cb2-8891-ff32917e50e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Xception(\n",
            "  (entry_flow): EntryFlow(\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "    )\n",
            "    (conv3_residual): Sequential(\n",
            "      (0): SeperableConv2d(\n",
            "        (depthwise): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "        (pointwise): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): SeperableConv2d(\n",
            "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (pointwise): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (conv3_shortcut): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv4_residual): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): SeperableConv2d(\n",
            "        (depthwise): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (pointwise): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): SeperableConv2d(\n",
            "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (pointwise): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (conv4_shortcut): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv5_residual): Sequential(\n",
            "      (0): ReLU(inplace=True)\n",
            "      (1): SeperableConv2d(\n",
            "        (depthwise): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (pointwise): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU(inplace=True)\n",
            "      (4): SeperableConv2d(\n",
            "        (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (5): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (conv5_shortcut): Sequential(\n",
            "      (0): Conv2d(256, 728, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (1): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (middel_flow): MiddleFlow(\n",
            "    (middel_block): Sequential(\n",
            "      (0): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (2): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (3): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (4): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (5): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (6): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (7): MiddleFLowBlock(\n",
            "        (shortcut): Sequential()\n",
            "        (conv1): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv2): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "        (conv3): Sequential(\n",
            "          (0): ReLU(inplace=True)\n",
            "          (1): SeperableConv2d(\n",
            "            (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "            (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          )\n",
            "          (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (exit_flow): ExitFLow(\n",
            "    (residual): Sequential(\n",
            "      (0): ReLU()\n",
            "      (1): SeperableConv2d(\n",
            "        (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "        (pointwise): Conv2d(728, 728, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (2): BatchNorm2d(728, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): ReLU()\n",
            "      (4): SeperableConv2d(\n",
            "        (depthwise): Conv2d(728, 728, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=728, bias=False)\n",
            "        (pointwise): Conv2d(728, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (shortcut): Sequential(\n",
            "      (0): Conv2d(728, 1024, kernel_size=(1, 1), stride=(2, 2))\n",
            "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv): Sequential(\n",
            "      (0): SeperableConv2d(\n",
            "        (depthwise): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
            "        (pointwise): Conv2d(1024, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU(inplace=True)\n",
            "      (3): SeperableConv2d(\n",
            "        (depthwise): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "        (pointwise): Conv2d(1536, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      )\n",
            "      (4): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (5): ReLU(inplace=True)\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  )\n",
            "  (fc): Linear(in_features=2048, out_features=100, bias=True)\n",
            ")\n",
            "iteration: 1\ttotal 625 iterations\n",
            "iteration: 2\ttotal 625 iterations\n",
            "iteration: 3\ttotal 625 iterations\n",
            "iteration: 4\ttotal 625 iterations\n",
            "iteration: 5\ttotal 625 iterations\n",
            "iteration: 6\ttotal 625 iterations\n",
            "iteration: 7\ttotal 625 iterations\n",
            "iteration: 8\ttotal 625 iterations\n",
            "iteration: 9\ttotal 625 iterations\n",
            "iteration: 10\ttotal 625 iterations\n",
            "iteration: 11\ttotal 625 iterations\n",
            "iteration: 12\ttotal 625 iterations\n",
            "iteration: 13\ttotal 625 iterations\n",
            "iteration: 14\ttotal 625 iterations\n",
            "iteration: 15\ttotal 625 iterations\n",
            "iteration: 16\ttotal 625 iterations\n",
            "iteration: 17\ttotal 625 iterations\n",
            "iteration: 18\ttotal 625 iterations\n",
            "iteration: 19\ttotal 625 iterations\n",
            "iteration: 20\ttotal 625 iterations\n",
            "iteration: 21\ttotal 625 iterations\n",
            "iteration: 22\ttotal 625 iterations\n",
            "iteration: 23\ttotal 625 iterations\n",
            "iteration: 24\ttotal 625 iterations\n",
            "iteration: 25\ttotal 625 iterations\n",
            "iteration: 26\ttotal 625 iterations\n",
            "iteration: 27\ttotal 625 iterations\n",
            "iteration: 28\ttotal 625 iterations\n",
            "iteration: 29\ttotal 625 iterations\n",
            "iteration: 30\ttotal 625 iterations\n",
            "iteration: 31\ttotal 625 iterations\n",
            "iteration: 32\ttotal 625 iterations\n",
            "iteration: 33\ttotal 625 iterations\n",
            "iteration: 34\ttotal 625 iterations\n",
            "iteration: 35\ttotal 625 iterations\n",
            "iteration: 36\ttotal 625 iterations\n",
            "iteration: 37\ttotal 625 iterations\n",
            "iteration: 38\ttotal 625 iterations\n",
            "iteration: 39\ttotal 625 iterations\n",
            "iteration: 40\ttotal 625 iterations\n",
            "iteration: 41\ttotal 625 iterations\n",
            "iteration: 42\ttotal 625 iterations\n",
            "iteration: 43\ttotal 625 iterations\n",
            "iteration: 44\ttotal 625 iterations\n",
            "iteration: 45\ttotal 625 iterations\n",
            "iteration: 46\ttotal 625 iterations\n",
            "iteration: 47\ttotal 625 iterations\n",
            "iteration: 48\ttotal 625 iterations\n",
            "iteration: 49\ttotal 625 iterations\n",
            "iteration: 50\ttotal 625 iterations\n",
            "iteration: 51\ttotal 625 iterations\n",
            "iteration: 52\ttotal 625 iterations\n",
            "iteration: 53\ttotal 625 iterations\n",
            "iteration: 54\ttotal 625 iterations\n",
            "iteration: 55\ttotal 625 iterations\n",
            "iteration: 56\ttotal 625 iterations\n",
            "iteration: 57\ttotal 625 iterations\n",
            "iteration: 58\ttotal 625 iterations\n",
            "iteration: 59\ttotal 625 iterations\n",
            "iteration: 60\ttotal 625 iterations\n",
            "iteration: 61\ttotal 625 iterations\n",
            "iteration: 62\ttotal 625 iterations\n",
            "iteration: 63\ttotal 625 iterations\n",
            "iteration: 64\ttotal 625 iterations\n",
            "iteration: 65\ttotal 625 iterations\n",
            "iteration: 66\ttotal 625 iterations\n",
            "iteration: 67\ttotal 625 iterations\n",
            "iteration: 68\ttotal 625 iterations\n",
            "iteration: 69\ttotal 625 iterations\n",
            "iteration: 70\ttotal 625 iterations\n",
            "iteration: 71\ttotal 625 iterations\n",
            "iteration: 72\ttotal 625 iterations\n",
            "iteration: 73\ttotal 625 iterations\n",
            "iteration: 74\ttotal 625 iterations\n",
            "iteration: 75\ttotal 625 iterations\n",
            "iteration: 76\ttotal 625 iterations\n",
            "iteration: 77\ttotal 625 iterations\n",
            "iteration: 78\ttotal 625 iterations\n",
            "iteration: 79\ttotal 625 iterations\n",
            "iteration: 80\ttotal 625 iterations\n",
            "iteration: 81\ttotal 625 iterations\n",
            "iteration: 82\ttotal 625 iterations\n",
            "iteration: 83\ttotal 625 iterations\n",
            "iteration: 84\ttotal 625 iterations\n",
            "iteration: 85\ttotal 625 iterations\n",
            "iteration: 86\ttotal 625 iterations\n",
            "iteration: 87\ttotal 625 iterations\n",
            "iteration: 88\ttotal 625 iterations\n",
            "iteration: 89\ttotal 625 iterations\n",
            "iteration: 90\ttotal 625 iterations\n",
            "iteration: 91\ttotal 625 iterations\n",
            "iteration: 92\ttotal 625 iterations\n",
            "iteration: 93\ttotal 625 iterations\n",
            "iteration: 94\ttotal 625 iterations\n",
            "iteration: 95\ttotal 625 iterations\n",
            "iteration: 96\ttotal 625 iterations\n",
            "iteration: 97\ttotal 625 iterations\n",
            "iteration: 98\ttotal 625 iterations\n",
            "iteration: 99\ttotal 625 iterations\n",
            "iteration: 100\ttotal 625 iterations\n",
            "iteration: 101\ttotal 625 iterations\n",
            "iteration: 102\ttotal 625 iterations\n",
            "iteration: 103\ttotal 625 iterations\n",
            "iteration: 104\ttotal 625 iterations\n",
            "iteration: 105\ttotal 625 iterations\n",
            "iteration: 106\ttotal 625 iterations\n",
            "iteration: 107\ttotal 625 iterations\n",
            "iteration: 108\ttotal 625 iterations\n",
            "iteration: 109\ttotal 625 iterations\n",
            "iteration: 110\ttotal 625 iterations\n",
            "iteration: 111\ttotal 625 iterations\n",
            "iteration: 112\ttotal 625 iterations\n",
            "iteration: 113\ttotal 625 iterations\n",
            "iteration: 114\ttotal 625 iterations\n",
            "iteration: 115\ttotal 625 iterations\n",
            "iteration: 116\ttotal 625 iterations\n",
            "iteration: 117\ttotal 625 iterations\n",
            "iteration: 118\ttotal 625 iterations\n",
            "iteration: 119\ttotal 625 iterations\n",
            "iteration: 120\ttotal 625 iterations\n",
            "iteration: 121\ttotal 625 iterations\n",
            "iteration: 122\ttotal 625 iterations\n",
            "iteration: 123\ttotal 625 iterations\n",
            "iteration: 124\ttotal 625 iterations\n",
            "iteration: 125\ttotal 625 iterations\n",
            "iteration: 126\ttotal 625 iterations\n",
            "iteration: 127\ttotal 625 iterations\n",
            "iteration: 128\ttotal 625 iterations\n",
            "iteration: 129\ttotal 625 iterations\n",
            "iteration: 130\ttotal 625 iterations\n",
            "iteration: 131\ttotal 625 iterations\n",
            "iteration: 132\ttotal 625 iterations\n",
            "iteration: 133\ttotal 625 iterations\n",
            "iteration: 134\ttotal 625 iterations\n",
            "iteration: 135\ttotal 625 iterations\n",
            "iteration: 136\ttotal 625 iterations\n",
            "iteration: 137\ttotal 625 iterations\n",
            "iteration: 138\ttotal 625 iterations\n",
            "iteration: 139\ttotal 625 iterations\n",
            "iteration: 140\ttotal 625 iterations\n",
            "iteration: 141\ttotal 625 iterations\n",
            "iteration: 142\ttotal 625 iterations\n",
            "iteration: 143\ttotal 625 iterations\n",
            "iteration: 144\ttotal 625 iterations\n",
            "iteration: 145\ttotal 625 iterations\n",
            "iteration: 146\ttotal 625 iterations\n",
            "iteration: 147\ttotal 625 iterations\n",
            "iteration: 148\ttotal 625 iterations\n",
            "iteration: 149\ttotal 625 iterations\n",
            "iteration: 150\ttotal 625 iterations\n",
            "iteration: 151\ttotal 625 iterations\n",
            "iteration: 152\ttotal 625 iterations\n",
            "iteration: 153\ttotal 625 iterations\n",
            "iteration: 154\ttotal 625 iterations\n",
            "iteration: 155\ttotal 625 iterations\n",
            "iteration: 156\ttotal 625 iterations\n",
            "iteration: 157\ttotal 625 iterations\n",
            "iteration: 158\ttotal 625 iterations\n",
            "iteration: 159\ttotal 625 iterations\n",
            "iteration: 160\ttotal 625 iterations\n",
            "iteration: 161\ttotal 625 iterations\n",
            "iteration: 162\ttotal 625 iterations\n",
            "iteration: 163\ttotal 625 iterations\n",
            "iteration: 164\ttotal 625 iterations\n",
            "iteration: 165\ttotal 625 iterations\n",
            "iteration: 166\ttotal 625 iterations\n",
            "iteration: 167\ttotal 625 iterations\n",
            "iteration: 168\ttotal 625 iterations\n",
            "iteration: 169\ttotal 625 iterations\n",
            "iteration: 170\ttotal 625 iterations\n",
            "iteration: 171\ttotal 625 iterations\n",
            "iteration: 172\ttotal 625 iterations\n",
            "iteration: 173\ttotal 625 iterations\n",
            "iteration: 174\ttotal 625 iterations\n",
            "iteration: 175\ttotal 625 iterations\n",
            "iteration: 176\ttotal 625 iterations\n",
            "iteration: 177\ttotal 625 iterations\n",
            "iteration: 178\ttotal 625 iterations\n",
            "iteration: 179\ttotal 625 iterations\n",
            "iteration: 180\ttotal 625 iterations\n",
            "iteration: 181\ttotal 625 iterations\n",
            "iteration: 182\ttotal 625 iterations\n",
            "iteration: 183\ttotal 625 iterations\n",
            "iteration: 184\ttotal 625 iterations\n",
            "iteration: 185\ttotal 625 iterations\n",
            "iteration: 186\ttotal 625 iterations\n",
            "iteration: 187\ttotal 625 iterations\n",
            "iteration: 188\ttotal 625 iterations\n",
            "iteration: 189\ttotal 625 iterations\n",
            "iteration: 190\ttotal 625 iterations\n",
            "iteration: 191\ttotal 625 iterations\n",
            "iteration: 192\ttotal 625 iterations\n",
            "iteration: 193\ttotal 625 iterations\n",
            "iteration: 194\ttotal 625 iterations\n",
            "iteration: 195\ttotal 625 iterations\n",
            "iteration: 196\ttotal 625 iterations\n",
            "iteration: 197\ttotal 625 iterations\n",
            "iteration: 198\ttotal 625 iterations\n",
            "iteration: 199\ttotal 625 iterations\n",
            "iteration: 200\ttotal 625 iterations\n",
            "iteration: 201\ttotal 625 iterations\n",
            "iteration: 202\ttotal 625 iterations\n",
            "iteration: 203\ttotal 625 iterations\n",
            "iteration: 204\ttotal 625 iterations\n",
            "iteration: 205\ttotal 625 iterations\n",
            "iteration: 206\ttotal 625 iterations\n",
            "iteration: 207\ttotal 625 iterations\n",
            "iteration: 208\ttotal 625 iterations\n",
            "iteration: 209\ttotal 625 iterations\n",
            "iteration: 210\ttotal 625 iterations\n",
            "iteration: 211\ttotal 625 iterations\n",
            "iteration: 212\ttotal 625 iterations\n",
            "iteration: 213\ttotal 625 iterations\n",
            "iteration: 214\ttotal 625 iterations\n",
            "iteration: 215\ttotal 625 iterations\n",
            "iteration: 216\ttotal 625 iterations\n",
            "iteration: 217\ttotal 625 iterations\n",
            "iteration: 218\ttotal 625 iterations\n",
            "iteration: 219\ttotal 625 iterations\n",
            "iteration: 220\ttotal 625 iterations\n",
            "iteration: 221\ttotal 625 iterations\n",
            "iteration: 222\ttotal 625 iterations\n",
            "iteration: 223\ttotal 625 iterations\n",
            "iteration: 224\ttotal 625 iterations\n",
            "iteration: 225\ttotal 625 iterations\n",
            "iteration: 226\ttotal 625 iterations\n",
            "iteration: 227\ttotal 625 iterations\n",
            "iteration: 228\ttotal 625 iterations\n",
            "iteration: 229\ttotal 625 iterations\n",
            "iteration: 230\ttotal 625 iterations\n",
            "iteration: 231\ttotal 625 iterations\n",
            "iteration: 232\ttotal 625 iterations\n",
            "iteration: 233\ttotal 625 iterations\n",
            "iteration: 234\ttotal 625 iterations\n",
            "iteration: 235\ttotal 625 iterations\n",
            "iteration: 236\ttotal 625 iterations\n",
            "iteration: 237\ttotal 625 iterations\n",
            "iteration: 238\ttotal 625 iterations\n",
            "iteration: 239\ttotal 625 iterations\n",
            "iteration: 240\ttotal 625 iterations\n",
            "iteration: 241\ttotal 625 iterations\n",
            "iteration: 242\ttotal 625 iterations\n",
            "iteration: 243\ttotal 625 iterations\n",
            "iteration: 244\ttotal 625 iterations\n",
            "iteration: 245\ttotal 625 iterations\n",
            "iteration: 246\ttotal 625 iterations\n",
            "iteration: 247\ttotal 625 iterations\n",
            "iteration: 248\ttotal 625 iterations\n",
            "iteration: 249\ttotal 625 iterations\n",
            "iteration: 250\ttotal 625 iterations\n",
            "iteration: 251\ttotal 625 iterations\n",
            "iteration: 252\ttotal 625 iterations\n",
            "iteration: 253\ttotal 625 iterations\n",
            "iteration: 254\ttotal 625 iterations\n",
            "iteration: 255\ttotal 625 iterations\n",
            "iteration: 256\ttotal 625 iterations\n",
            "iteration: 257\ttotal 625 iterations\n",
            "iteration: 258\ttotal 625 iterations\n",
            "iteration: 259\ttotal 625 iterations\n",
            "iteration: 260\ttotal 625 iterations\n",
            "iteration: 261\ttotal 625 iterations\n",
            "iteration: 262\ttotal 625 iterations\n",
            "iteration: 263\ttotal 625 iterations\n",
            "iteration: 264\ttotal 625 iterations\n",
            "iteration: 265\ttotal 625 iterations\n",
            "iteration: 266\ttotal 625 iterations\n",
            "iteration: 267\ttotal 625 iterations\n",
            "iteration: 268\ttotal 625 iterations\n",
            "iteration: 269\ttotal 625 iterations\n",
            "iteration: 270\ttotal 625 iterations\n",
            "iteration: 271\ttotal 625 iterations\n",
            "iteration: 272\ttotal 625 iterations\n",
            "iteration: 273\ttotal 625 iterations\n",
            "iteration: 274\ttotal 625 iterations\n",
            "iteration: 275\ttotal 625 iterations\n",
            "iteration: 276\ttotal 625 iterations\n",
            "iteration: 277\ttotal 625 iterations\n",
            "iteration: 278\ttotal 625 iterations\n",
            "iteration: 279\ttotal 625 iterations\n",
            "iteration: 280\ttotal 625 iterations\n",
            "iteration: 281\ttotal 625 iterations\n",
            "iteration: 282\ttotal 625 iterations\n",
            "iteration: 283\ttotal 625 iterations\n",
            "iteration: 284\ttotal 625 iterations\n",
            "iteration: 285\ttotal 625 iterations\n",
            "iteration: 286\ttotal 625 iterations\n",
            "iteration: 287\ttotal 625 iterations\n",
            "iteration: 288\ttotal 625 iterations\n",
            "iteration: 289\ttotal 625 iterations\n",
            "iteration: 290\ttotal 625 iterations\n",
            "iteration: 291\ttotal 625 iterations\n",
            "iteration: 292\ttotal 625 iterations\n",
            "iteration: 293\ttotal 625 iterations\n",
            "iteration: 294\ttotal 625 iterations\n",
            "iteration: 295\ttotal 625 iterations\n",
            "iteration: 296\ttotal 625 iterations\n",
            "iteration: 297\ttotal 625 iterations\n",
            "iteration: 298\ttotal 625 iterations\n",
            "iteration: 299\ttotal 625 iterations\n",
            "iteration: 300\ttotal 625 iterations\n",
            "iteration: 301\ttotal 625 iterations\n",
            "iteration: 302\ttotal 625 iterations\n",
            "iteration: 303\ttotal 625 iterations\n",
            "iteration: 304\ttotal 625 iterations\n",
            "iteration: 305\ttotal 625 iterations\n",
            "iteration: 306\ttotal 625 iterations\n",
            "iteration: 307\ttotal 625 iterations\n",
            "iteration: 308\ttotal 625 iterations\n",
            "iteration: 309\ttotal 625 iterations\n",
            "iteration: 310\ttotal 625 iterations\n",
            "iteration: 311\ttotal 625 iterations\n",
            "iteration: 312\ttotal 625 iterations\n",
            "iteration: 313\ttotal 625 iterations\n",
            "iteration: 314\ttotal 625 iterations\n",
            "iteration: 315\ttotal 625 iterations\n",
            "iteration: 316\ttotal 625 iterations\n",
            "iteration: 317\ttotal 625 iterations\n",
            "iteration: 318\ttotal 625 iterations\n",
            "iteration: 319\ttotal 625 iterations\n",
            "iteration: 320\ttotal 625 iterations\n",
            "iteration: 321\ttotal 625 iterations\n",
            "iteration: 322\ttotal 625 iterations\n",
            "iteration: 323\ttotal 625 iterations\n",
            "iteration: 324\ttotal 625 iterations\n",
            "iteration: 325\ttotal 625 iterations\n",
            "iteration: 326\ttotal 625 iterations\n",
            "iteration: 327\ttotal 625 iterations\n",
            "iteration: 328\ttotal 625 iterations\n",
            "iteration: 329\ttotal 625 iterations\n",
            "iteration: 330\ttotal 625 iterations\n",
            "iteration: 331\ttotal 625 iterations\n",
            "iteration: 332\ttotal 625 iterations\n",
            "iteration: 333\ttotal 625 iterations\n",
            "iteration: 334\ttotal 625 iterations\n",
            "iteration: 335\ttotal 625 iterations\n",
            "iteration: 336\ttotal 625 iterations\n",
            "iteration: 337\ttotal 625 iterations\n",
            "iteration: 338\ttotal 625 iterations\n",
            "iteration: 339\ttotal 625 iterations\n",
            "iteration: 340\ttotal 625 iterations\n",
            "iteration: 341\ttotal 625 iterations\n",
            "iteration: 342\ttotal 625 iterations\n",
            "iteration: 343\ttotal 625 iterations\n",
            "iteration: 344\ttotal 625 iterations\n",
            "iteration: 345\ttotal 625 iterations\n",
            "iteration: 346\ttotal 625 iterations\n",
            "iteration: 347\ttotal 625 iterations\n",
            "iteration: 348\ttotal 625 iterations\n",
            "iteration: 349\ttotal 625 iterations\n",
            "iteration: 350\ttotal 625 iterations\n",
            "iteration: 351\ttotal 625 iterations\n",
            "iteration: 352\ttotal 625 iterations\n",
            "iteration: 353\ttotal 625 iterations\n",
            "iteration: 354\ttotal 625 iterations\n",
            "iteration: 355\ttotal 625 iterations\n",
            "iteration: 356\ttotal 625 iterations\n",
            "iteration: 357\ttotal 625 iterations\n",
            "iteration: 358\ttotal 625 iterations\n",
            "iteration: 359\ttotal 625 iterations\n",
            "iteration: 360\ttotal 625 iterations\n",
            "iteration: 361\ttotal 625 iterations\n",
            "iteration: 362\ttotal 625 iterations\n",
            "iteration: 363\ttotal 625 iterations\n",
            "iteration: 364\ttotal 625 iterations\n",
            "iteration: 365\ttotal 625 iterations\n",
            "iteration: 366\ttotal 625 iterations\n",
            "iteration: 367\ttotal 625 iterations\n",
            "iteration: 368\ttotal 625 iterations\n",
            "iteration: 369\ttotal 625 iterations\n",
            "iteration: 370\ttotal 625 iterations\n",
            "iteration: 371\ttotal 625 iterations\n",
            "iteration: 372\ttotal 625 iterations\n",
            "iteration: 373\ttotal 625 iterations\n",
            "iteration: 374\ttotal 625 iterations\n",
            "iteration: 375\ttotal 625 iterations\n",
            "iteration: 376\ttotal 625 iterations\n",
            "iteration: 377\ttotal 625 iterations\n",
            "iteration: 378\ttotal 625 iterations\n",
            "iteration: 379\ttotal 625 iterations\n",
            "iteration: 380\ttotal 625 iterations\n",
            "iteration: 381\ttotal 625 iterations\n",
            "iteration: 382\ttotal 625 iterations\n",
            "iteration: 383\ttotal 625 iterations\n",
            "iteration: 384\ttotal 625 iterations\n",
            "iteration: 385\ttotal 625 iterations\n",
            "iteration: 386\ttotal 625 iterations\n",
            "iteration: 387\ttotal 625 iterations\n",
            "iteration: 388\ttotal 625 iterations\n",
            "iteration: 389\ttotal 625 iterations\n",
            "iteration: 390\ttotal 625 iterations\n",
            "iteration: 391\ttotal 625 iterations\n",
            "iteration: 392\ttotal 625 iterations\n",
            "iteration: 393\ttotal 625 iterations\n",
            "iteration: 394\ttotal 625 iterations\n",
            "iteration: 395\ttotal 625 iterations\n",
            "iteration: 396\ttotal 625 iterations\n",
            "iteration: 397\ttotal 625 iterations\n",
            "iteration: 398\ttotal 625 iterations\n",
            "iteration: 399\ttotal 625 iterations\n",
            "iteration: 400\ttotal 625 iterations\n",
            "iteration: 401\ttotal 625 iterations\n",
            "iteration: 402\ttotal 625 iterations\n",
            "iteration: 403\ttotal 625 iterations\n",
            "iteration: 404\ttotal 625 iterations\n",
            "iteration: 405\ttotal 625 iterations\n",
            "iteration: 406\ttotal 625 iterations\n",
            "iteration: 407\ttotal 625 iterations\n",
            "iteration: 408\ttotal 625 iterations\n",
            "iteration: 409\ttotal 625 iterations\n",
            "iteration: 410\ttotal 625 iterations\n",
            "iteration: 411\ttotal 625 iterations\n",
            "iteration: 412\ttotal 625 iterations\n",
            "iteration: 413\ttotal 625 iterations\n",
            "iteration: 414\ttotal 625 iterations\n",
            "iteration: 415\ttotal 625 iterations\n",
            "iteration: 416\ttotal 625 iterations\n",
            "iteration: 417\ttotal 625 iterations\n",
            "iteration: 418\ttotal 625 iterations\n",
            "iteration: 419\ttotal 625 iterations\n",
            "iteration: 420\ttotal 625 iterations\n",
            "iteration: 421\ttotal 625 iterations\n",
            "iteration: 422\ttotal 625 iterations\n",
            "iteration: 423\ttotal 625 iterations\n",
            "iteration: 424\ttotal 625 iterations\n",
            "iteration: 425\ttotal 625 iterations\n",
            "iteration: 426\ttotal 625 iterations\n",
            "iteration: 427\ttotal 625 iterations\n",
            "iteration: 428\ttotal 625 iterations\n",
            "iteration: 429\ttotal 625 iterations\n",
            "iteration: 430\ttotal 625 iterations\n",
            "iteration: 431\ttotal 625 iterations\n",
            "iteration: 432\ttotal 625 iterations\n",
            "iteration: 433\ttotal 625 iterations\n",
            "iteration: 434\ttotal 625 iterations\n",
            "iteration: 435\ttotal 625 iterations\n",
            "iteration: 436\ttotal 625 iterations\n",
            "iteration: 437\ttotal 625 iterations\n",
            "iteration: 438\ttotal 625 iterations\n",
            "iteration: 439\ttotal 625 iterations\n",
            "iteration: 440\ttotal 625 iterations\n",
            "iteration: 441\ttotal 625 iterations\n",
            "iteration: 442\ttotal 625 iterations\n",
            "iteration: 443\ttotal 625 iterations\n",
            "iteration: 444\ttotal 625 iterations\n",
            "iteration: 445\ttotal 625 iterations\n",
            "iteration: 446\ttotal 625 iterations\n",
            "iteration: 447\ttotal 625 iterations\n",
            "iteration: 448\ttotal 625 iterations\n",
            "iteration: 449\ttotal 625 iterations\n",
            "iteration: 450\ttotal 625 iterations\n",
            "iteration: 451\ttotal 625 iterations\n",
            "iteration: 452\ttotal 625 iterations\n",
            "iteration: 453\ttotal 625 iterations\n",
            "iteration: 454\ttotal 625 iterations\n",
            "iteration: 455\ttotal 625 iterations\n",
            "iteration: 456\ttotal 625 iterations\n",
            "iteration: 457\ttotal 625 iterations\n",
            "iteration: 458\ttotal 625 iterations\n",
            "iteration: 459\ttotal 625 iterations\n",
            "iteration: 460\ttotal 625 iterations\n",
            "iteration: 461\ttotal 625 iterations\n",
            "iteration: 462\ttotal 625 iterations\n",
            "iteration: 463\ttotal 625 iterations\n",
            "iteration: 464\ttotal 625 iterations\n",
            "iteration: 465\ttotal 625 iterations\n",
            "iteration: 466\ttotal 625 iterations\n",
            "iteration: 467\ttotal 625 iterations\n",
            "iteration: 468\ttotal 625 iterations\n",
            "iteration: 469\ttotal 625 iterations\n",
            "iteration: 470\ttotal 625 iterations\n",
            "iteration: 471\ttotal 625 iterations\n",
            "iteration: 472\ttotal 625 iterations\n",
            "iteration: 473\ttotal 625 iterations\n",
            "iteration: 474\ttotal 625 iterations\n",
            "iteration: 475\ttotal 625 iterations\n",
            "iteration: 476\ttotal 625 iterations\n",
            "iteration: 477\ttotal 625 iterations\n",
            "iteration: 478\ttotal 625 iterations\n",
            "iteration: 479\ttotal 625 iterations\n",
            "iteration: 480\ttotal 625 iterations\n",
            "iteration: 481\ttotal 625 iterations\n",
            "iteration: 482\ttotal 625 iterations\n",
            "iteration: 483\ttotal 625 iterations\n",
            "iteration: 484\ttotal 625 iterations\n",
            "iteration: 485\ttotal 625 iterations\n",
            "iteration: 486\ttotal 625 iterations\n",
            "iteration: 487\ttotal 625 iterations\n",
            "iteration: 488\ttotal 625 iterations\n",
            "iteration: 489\ttotal 625 iterations\n",
            "iteration: 490\ttotal 625 iterations\n",
            "iteration: 491\ttotal 625 iterations\n",
            "iteration: 492\ttotal 625 iterations\n",
            "iteration: 493\ttotal 625 iterations\n",
            "iteration: 494\ttotal 625 iterations\n",
            "iteration: 495\ttotal 625 iterations\n",
            "iteration: 496\ttotal 625 iterations\n",
            "iteration: 497\ttotal 625 iterations\n",
            "iteration: 498\ttotal 625 iterations\n",
            "iteration: 499\ttotal 625 iterations\n",
            "iteration: 500\ttotal 625 iterations\n",
            "iteration: 501\ttotal 625 iterations\n",
            "iteration: 502\ttotal 625 iterations\n",
            "iteration: 503\ttotal 625 iterations\n",
            "iteration: 504\ttotal 625 iterations\n",
            "iteration: 505\ttotal 625 iterations\n",
            "iteration: 506\ttotal 625 iterations\n",
            "iteration: 507\ttotal 625 iterations\n",
            "iteration: 508\ttotal 625 iterations\n",
            "iteration: 509\ttotal 625 iterations\n",
            "iteration: 510\ttotal 625 iterations\n",
            "iteration: 511\ttotal 625 iterations\n",
            "iteration: 512\ttotal 625 iterations\n",
            "iteration: 513\ttotal 625 iterations\n",
            "iteration: 514\ttotal 625 iterations\n",
            "iteration: 515\ttotal 625 iterations\n",
            "iteration: 516\ttotal 625 iterations\n",
            "iteration: 517\ttotal 625 iterations\n",
            "iteration: 518\ttotal 625 iterations\n",
            "iteration: 519\ttotal 625 iterations\n",
            "iteration: 520\ttotal 625 iterations\n",
            "iteration: 521\ttotal 625 iterations\n",
            "iteration: 522\ttotal 625 iterations\n",
            "iteration: 523\ttotal 625 iterations\n",
            "iteration: 524\ttotal 625 iterations\n",
            "iteration: 525\ttotal 625 iterations\n",
            "iteration: 526\ttotal 625 iterations\n",
            "iteration: 527\ttotal 625 iterations\n",
            "iteration: 528\ttotal 625 iterations\n",
            "iteration: 529\ttotal 625 iterations\n",
            "iteration: 530\ttotal 625 iterations\n",
            "iteration: 531\ttotal 625 iterations\n",
            "iteration: 532\ttotal 625 iterations\n",
            "iteration: 533\ttotal 625 iterations\n",
            "iteration: 534\ttotal 625 iterations\n",
            "iteration: 535\ttotal 625 iterations\n",
            "iteration: 536\ttotal 625 iterations\n",
            "iteration: 537\ttotal 625 iterations\n",
            "iteration: 538\ttotal 625 iterations\n",
            "iteration: 539\ttotal 625 iterations\n",
            "iteration: 540\ttotal 625 iterations\n",
            "iteration: 541\ttotal 625 iterations\n",
            "iteration: 542\ttotal 625 iterations\n",
            "iteration: 543\ttotal 625 iterations\n",
            "iteration: 544\ttotal 625 iterations\n",
            "iteration: 545\ttotal 625 iterations\n",
            "iteration: 546\ttotal 625 iterations\n",
            "iteration: 547\ttotal 625 iterations\n",
            "iteration: 548\ttotal 625 iterations\n",
            "iteration: 549\ttotal 625 iterations\n",
            "iteration: 550\ttotal 625 iterations\n",
            "iteration: 551\ttotal 625 iterations\n",
            "iteration: 552\ttotal 625 iterations\n",
            "iteration: 553\ttotal 625 iterations\n",
            "iteration: 554\ttotal 625 iterations\n",
            "iteration: 555\ttotal 625 iterations\n",
            "iteration: 556\ttotal 625 iterations\n",
            "iteration: 557\ttotal 625 iterations\n",
            "iteration: 558\ttotal 625 iterations\n",
            "iteration: 559\ttotal 625 iterations\n",
            "iteration: 560\ttotal 625 iterations\n",
            "iteration: 561\ttotal 625 iterations\n",
            "iteration: 562\ttotal 625 iterations\n",
            "iteration: 563\ttotal 625 iterations\n",
            "iteration: 564\ttotal 625 iterations\n",
            "iteration: 565\ttotal 625 iterations\n",
            "iteration: 566\ttotal 625 iterations\n",
            "iteration: 567\ttotal 625 iterations\n",
            "iteration: 568\ttotal 625 iterations\n",
            "iteration: 569\ttotal 625 iterations\n",
            "iteration: 570\ttotal 625 iterations\n",
            "iteration: 571\ttotal 625 iterations\n",
            "iteration: 572\ttotal 625 iterations\n",
            "iteration: 573\ttotal 625 iterations\n",
            "iteration: 574\ttotal 625 iterations\n",
            "iteration: 575\ttotal 625 iterations\n",
            "iteration: 576\ttotal 625 iterations\n",
            "iteration: 577\ttotal 625 iterations\n",
            "iteration: 578\ttotal 625 iterations\n",
            "iteration: 579\ttotal 625 iterations\n",
            "iteration: 580\ttotal 625 iterations\n",
            "iteration: 581\ttotal 625 iterations\n",
            "iteration: 582\ttotal 625 iterations\n",
            "iteration: 583\ttotal 625 iterations\n",
            "iteration: 584\ttotal 625 iterations\n",
            "iteration: 585\ttotal 625 iterations\n",
            "iteration: 586\ttotal 625 iterations\n",
            "iteration: 587\ttotal 625 iterations\n",
            "iteration: 588\ttotal 625 iterations\n",
            "iteration: 589\ttotal 625 iterations\n",
            "iteration: 590\ttotal 625 iterations\n",
            "iteration: 591\ttotal 625 iterations\n",
            "iteration: 592\ttotal 625 iterations\n",
            "iteration: 593\ttotal 625 iterations\n",
            "iteration: 594\ttotal 625 iterations\n",
            "iteration: 595\ttotal 625 iterations\n",
            "iteration: 596\ttotal 625 iterations\n",
            "iteration: 597\ttotal 625 iterations\n",
            "iteration: 598\ttotal 625 iterations\n",
            "iteration: 599\ttotal 625 iterations\n",
            "iteration: 600\ttotal 625 iterations\n",
            "iteration: 601\ttotal 625 iterations\n",
            "iteration: 602\ttotal 625 iterations\n",
            "iteration: 603\ttotal 625 iterations\n",
            "iteration: 604\ttotal 625 iterations\n",
            "iteration: 605\ttotal 625 iterations\n",
            "iteration: 606\ttotal 625 iterations\n",
            "iteration: 607\ttotal 625 iterations\n",
            "iteration: 608\ttotal 625 iterations\n",
            "iteration: 609\ttotal 625 iterations\n",
            "iteration: 610\ttotal 625 iterations\n",
            "iteration: 611\ttotal 625 iterations\n",
            "iteration: 612\ttotal 625 iterations\n",
            "iteration: 613\ttotal 625 iterations\n",
            "iteration: 614\ttotal 625 iterations\n",
            "iteration: 615\ttotal 625 iterations\n",
            "iteration: 616\ttotal 625 iterations\n",
            "iteration: 617\ttotal 625 iterations\n",
            "iteration: 618\ttotal 625 iterations\n",
            "iteration: 619\ttotal 625 iterations\n",
            "iteration: 620\ttotal 625 iterations\n",
            "iteration: 621\ttotal 625 iterations\n",
            "iteration: 622\ttotal 625 iterations\n",
            "iteration: 623\ttotal 625 iterations\n",
            "iteration: 624\ttotal 625 iterations\n",
            "iteration: 625\ttotal 625 iterations\n",
            "\n",
            "Top 1 err:  tensor(0.3610)\n",
            "Top 5 err:  tensor(0.1109)\n",
            "Parameter numbers: 21013988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without batch norm\n",
        "!python train.py -net xception -gpu"
      ],
      "metadata": {
        "id": "FC4vw8IdDMQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e257cf-2a62-442b-861d-da1e31d1aa07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Training Epoch: 9 [40832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [40960/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [41088/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [41216/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [41344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [41472/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [41600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 9 [41728/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [41856/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [41984/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 9 [42112/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 9 [42240/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [42368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 9 [42496/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [42624/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 9 [42752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [42880/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [43008/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [43136/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [43264/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [43392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [43520/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [43648/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [43776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [43904/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [44032/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [44160/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [44288/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [44416/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [44544/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [44672/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [44800/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [44928/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [45184/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45312/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 9 [45440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45568/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [45696/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [45824/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [45952/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 9 [46080/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 9 [46208/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [46336/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [46464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [46592/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [46720/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [46848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 9 [46976/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [47104/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [47232/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [47360/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [47488/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 9 [47616/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 9 [47744/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 9 [47872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [48000/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 9 [48128/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [48256/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [48384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 9 [48512/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 9 [48640/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 9 [48768/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 9 [48896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 9 [49024/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [49152/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [49280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 9 [49408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [49536/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 9 [49664/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 9 [49792/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 9 [49920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 9 [50000/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "epoch 9 training time consumed: 184.54s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB |  68288 GiB |  68287 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  68235 GiB |  68235 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     52 GiB |     52 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB |  68288 GiB |  68287 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  68235 GiB |  68235 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     52 GiB |     52 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB |  68280 GiB |  68280 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB |  68228 GiB |  68228 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     52 GiB |     52 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  54260 GiB |  54260 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  54201 GiB |  54201 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     59 GiB |     59 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    1847 K  |    1847 K  |\n",
            "|       from large pool |      92    |     145    |    1382 K  |    1382 K  |\n",
            "|       from small pool |     159    |     206    |     464 K  |     464 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    1847 K  |    1847 K  |\n",
            "|       from large pool |      92    |     145    |    1382 K  |    1382 K  |\n",
            "|       from small pool |     159    |     206    |     464 K  |     464 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      82    |     776 K  |     776 K  |\n",
            "|       from large pool |      29    |      73    |     725 K  |     725 K  |\n",
            "|       from small pool |       9    |      12    |      51 K  |      51 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 9, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.22s\n",
            "\n",
            "Training Epoch: 10 [128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [512/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [640/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [768/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [1024/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [1152/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [1280/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [1408/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [1536/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [1664/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [1792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [1920/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [2048/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [2176/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [2304/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [2432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [2560/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [2688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [2816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [2944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [3072/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [3200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [3328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [3456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [3584/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [3712/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [3840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [3968/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [4096/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [4224/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [4352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [4480/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [4608/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [4736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [4864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [4992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [5120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [5248/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [5376/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [5504/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [5632/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [5760/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [5888/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [6016/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [6144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [6272/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [6400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [6528/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [6656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [6784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [6912/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [7040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [7168/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [7296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [7424/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [7552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [7680/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [7808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [7936/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [8064/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [8192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [8320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [8448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [8576/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [8704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [8832/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [8960/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [9088/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [9216/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [9344/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [9472/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [9600/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [9728/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [9856/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [9984/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [10112/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [10240/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [10368/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [10496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [10624/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [10752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [10880/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [11008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [11264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [11392/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [11520/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [11648/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [11776/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [11904/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [12032/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [12160/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [12288/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [12416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [12544/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [12672/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [12800/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [12928/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [13056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [13184/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [13312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [13440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [13568/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [13696/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [13824/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [13952/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [14080/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [14208/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [14336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [14464/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [14592/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [14720/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [14848/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [14976/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [15104/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [15232/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [15360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [15488/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [15616/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [15744/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [15872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [16000/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 10 [16128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [16256/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [16384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [16512/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [16640/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [16768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [16896/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [17024/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [17152/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [17280/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [17408/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [17536/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [17664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [17792/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [17920/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [18048/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 10 [18176/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [18304/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [18432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [18560/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [18688/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [18816/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [18944/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [19072/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [19200/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [19328/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [19456/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [19584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [19712/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [19840/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [19968/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [20096/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [20224/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [20352/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [20480/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [20608/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [20736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [20864/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [20992/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 10 [21120/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [21248/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [21376/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [21504/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [21632/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [21760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [21888/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [22016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [22144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [22272/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [22400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [22528/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [22656/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [22784/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [22912/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [23040/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [23168/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [23296/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [23424/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [23552/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [23680/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 10 [23808/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 10 [23936/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [24064/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [24192/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [24320/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [24448/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [24576/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [24704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [24832/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [24960/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [25088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [25216/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [25344/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [25472/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [25600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [25728/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [25856/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 10 [25984/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [26112/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 10 [26240/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [26368/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [26496/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [26624/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [26752/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [26880/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [27008/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 10 [27136/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [27264/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [27392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [27520/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [27648/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 10 [27776/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [27904/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [28032/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [28160/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [28288/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [28416/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [28544/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [28672/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [28800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [28928/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [29056/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [29184/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [29312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [29440/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [29568/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [29696/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [29824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [29952/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [30080/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [30208/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [30336/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [30464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [30592/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [30720/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 10 [30848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [30976/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [31104/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [31232/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [31360/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [31488/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [31616/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [31744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [31872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [32000/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 10 [32128/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [32256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [32384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [32512/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [32640/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [32768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [32896/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 10 [33024/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [33152/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [33280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [33408/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [33536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [33664/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [33792/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [33920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [34048/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [34176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [34304/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [34432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [34560/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [34688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [34816/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [34944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [35072/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [35200/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [35328/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [35456/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [35584/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [35712/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [35840/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [35968/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [36096/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [36224/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [36352/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 10 [36480/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [36608/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [36736/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [36864/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [36992/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [37120/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [37248/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [37376/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [37504/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [37632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [37760/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 10 [37888/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [38016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [38144/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 10 [38272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [38400/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [38528/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [38656/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [38784/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [38912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [39040/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [39168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [39296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [39424/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [39552/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [39680/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [39808/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [39936/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [40064/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [40192/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [40320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [40448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [40576/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 10 [40704/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [40832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 10 [40960/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 10 [41088/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [41216/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 10 [41344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [41472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [41600/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 10 [41728/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [41856/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [41984/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [42112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [42240/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [42368/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [42496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [42624/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 10 [42752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [42880/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [43008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [43136/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [43264/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 10 [43392/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 10 [43520/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [43648/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [43776/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [43904/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [44032/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [44160/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [44288/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [44416/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [44544/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [44672/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [44800/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [44928/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [45056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [45184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [45312/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 10 [45440/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [45568/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [45696/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [45824/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 10 [45952/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [46080/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [46208/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [46336/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [46464/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [46592/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [46720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [46848/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [46976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [47104/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 10 [47232/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 10 [47360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [47488/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 10 [47616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [47744/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [47872/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [48000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [48128/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 10 [48256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [48384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [48512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [48640/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [48768/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 10 [48896/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [49024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [49152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 10 [49280/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 10 [49408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 10 [49536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 10 [49664/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 10 [49792/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 10 [49920/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 10 [50000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "epoch 10 training time consumed: 184.35s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB |  75873 GiB |  75873 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  75815 GiB |  75814 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB |  75873 GiB |  75873 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  75815 GiB |  75814 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB |  75865 GiB |  75865 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB |  75807 GiB |  75806 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     58 GiB |     58 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  60286 GiB |  60285 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  60220 GiB |  60219 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     65 GiB |     65 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    2052 K  |    2052 K  |\n",
            "|       from large pool |      92    |     145    |    1536 K  |    1536 K  |\n",
            "|       from small pool |     159    |     206    |     516 K  |     516 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    2052 K  |    2052 K  |\n",
            "|       from large pool |      92    |     145    |    1536 K  |    1536 K  |\n",
            "|       from small pool |     159    |     206    |     516 K  |     516 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |     863 K  |     863 K  |\n",
            "|       from large pool |      29    |      73    |     805 K  |     805 K  |\n",
            "|       from small pool |       8    |      12    |      57 K  |      57 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 10, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.28s\n",
            "\n",
            "saving weights file to checkpoint/xception/Tuesday_25_July_2023_14h_29m_51s/xception-10-regular.pth\n",
            "Training Epoch: 11 [128/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [256/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [384/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [512/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [768/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [896/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [1024/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [1152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [1280/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [1408/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [1536/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [1664/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [1792/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [1920/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [2048/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [2176/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [2304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [2432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [2560/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [2688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [2816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [2944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [3072/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [3200/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [3328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [3456/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [3584/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [3712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [3840/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [3968/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [4096/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [4224/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [4352/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [4480/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [4608/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [4736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [4864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [4992/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [5120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [5248/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [5376/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [5504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [5632/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [5760/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [5888/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [6016/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [6144/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [6272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [6400/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [6528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [6656/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [6784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [6912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [7168/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [7296/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [7424/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [7552/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [7680/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [7808/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 11 [7936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [8064/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [8192/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [8320/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [8448/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [8576/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [8704/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [8832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [8960/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [9088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [9216/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [9344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [9472/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [9600/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [9728/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [9856/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [9984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [10112/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [10240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [10368/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [10496/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [10624/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [10752/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [10880/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [11008/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [11264/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [11392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [11520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [11648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [11776/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [11904/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [12032/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [12160/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [12288/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [12416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [12544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [12672/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 11 [12800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [12928/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [13056/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [13184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [13312/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [13440/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [13568/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [13696/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [13824/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [13952/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 11 [14080/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [14208/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [14336/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [14464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [14592/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [14720/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [14848/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [14976/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [15104/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [15232/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [15360/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [15488/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [15616/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [15744/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [15872/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [16000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [16128/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [16256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [16384/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [16512/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [16640/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [16768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [16896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [17024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [17152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [17280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [17408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [17536/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [17664/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [17792/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [17920/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [18048/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [18176/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [18304/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [18432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [18560/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [18688/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [18816/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [18944/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [19072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [19200/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [19328/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [19456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [19584/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [19712/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [19840/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [19968/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [20096/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [20224/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [20352/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [20480/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [20608/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [20736/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [20864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [20992/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [21120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [21248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [21376/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [21504/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [21632/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [21760/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [21888/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [22016/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [22144/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 11 [22272/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [22400/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [22528/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [22656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [22784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [22912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [23040/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [23168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [23296/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [23424/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 11 [23552/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [23680/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [23808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [23936/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [24064/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [24192/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [24320/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [24448/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [24576/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [24704/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [24832/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [24960/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [25088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [25216/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [25344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [25472/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [25600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [25728/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [25856/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 11 [25984/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [26112/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [26240/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [26368/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [26496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [26624/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [26752/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [26880/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [27008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [27136/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [27264/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [27392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [27520/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [27648/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [27776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [27904/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [28032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [28160/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 11 [28288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [28416/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [28544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [28672/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 11 [28800/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [28928/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [29056/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [29184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [29312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [29440/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [29568/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [29696/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [29824/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [29952/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [30080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [30208/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [30336/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [30464/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 11 [30592/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [30720/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [30848/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [30976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [31104/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [31232/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [31360/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 11 [31488/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [31616/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [31744/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [31872/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [32000/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [32128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [32256/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [32384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [32512/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [32640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [32768/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [32896/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [33024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [33152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [33280/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 11 [33408/50000]\tLoss: 4.6033\tLR: 0.010000\n",
            "Training Epoch: 11 [33536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [33664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [33792/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [33920/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [34048/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [34176/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [34304/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [34432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [34560/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [34688/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [34816/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [34944/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [35072/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [35200/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [35328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [35456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [35584/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [35712/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [35840/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [35968/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [36096/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [36224/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 11 [36352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [36480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [36608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [36736/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [36864/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [36992/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [37120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [37248/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [37376/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [37504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [37632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [37760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [37888/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [38016/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [38144/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [38272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [38400/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [38528/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [38656/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [38784/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [38912/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [39040/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [39168/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [39296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [39424/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [39552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [39680/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 11 [39808/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [39936/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 11 [40064/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [40192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [40320/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [40448/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [40576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [40704/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [40832/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [40960/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [41088/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [41216/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [41472/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [41600/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [41728/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [41856/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [41984/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [42112/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [42240/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [42368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [42496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [42624/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [42752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [42880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [43008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [43136/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [43264/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [43392/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [43520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [43648/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [43776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [43904/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 11 [44032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [44160/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [44288/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 11 [44416/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [44544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [44672/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [44800/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [44928/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [45056/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [45184/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [45312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 11 [45440/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 11 [45568/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [45696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [45824/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 11 [45952/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 11 [46080/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [46208/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 11 [46336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [46464/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [46592/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [46720/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [46848/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [46976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [47104/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [47232/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [47360/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 11 [47488/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 11 [47616/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [47744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 11 [47872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 11 [48000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [48128/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 11 [48256/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [48384/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [48512/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [48640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [48768/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 11 [48896/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 11 [49024/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 11 [49152/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 11 [49280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [49408/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [49536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 11 [49664/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 11 [49792/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 11 [49920/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 11 [50000/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "epoch 11 training time consumed: 184.43s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB |  83458 GiB |  83458 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  83394 GiB |  83394 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     64 GiB |     64 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB |  83458 GiB |  83458 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  83394 GiB |  83394 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     64 GiB |     64 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB |  83449 GiB |  83449 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB |  83385 GiB |  83385 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     64 GiB |     64 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  66311 GiB |  66311 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  66238 GiB |  66238 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     72 GiB |     72 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    2257 K  |    2257 K  |\n",
            "|       from large pool |      92    |     145    |    1689 K  |    1689 K  |\n",
            "|       from small pool |     159    |     206    |     567 K  |     567 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    2257 K  |    2257 K  |\n",
            "|       from large pool |      92    |     145    |    1689 K  |    1689 K  |\n",
            "|       from small pool |     159    |     206    |     567 K  |     567 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |     949 K  |     949 K  |\n",
            "|       from large pool |      29    |      73    |     886 K  |     886 K  |\n",
            "|       from small pool |       8    |      12    |      63 K  |      63 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 11, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.24s\n",
            "\n",
            "Training Epoch: 12 [128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [512/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [896/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [1024/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [1152/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [1280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [1408/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [1536/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [1664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [1920/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [2048/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [2176/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [2304/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [2432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [2560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [2688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [2816/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [2944/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [3072/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [3200/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [3328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [3456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [3584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [3712/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [3840/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [3968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [4096/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [4224/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [4352/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [4480/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [4608/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [4736/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [4864/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [4992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [5120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [5248/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [5376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [5504/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [5632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [5760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [5888/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [6016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [6144/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [6272/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [6400/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [6528/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [6656/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [6784/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [6912/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [7168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [7296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [7424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [7552/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [7680/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [7808/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [7936/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [8064/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [8192/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [8320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [8448/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [8576/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [8704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [8832/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [8960/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [9088/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [9216/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [9344/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [9472/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [9600/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [9728/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [9856/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [9984/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [10112/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [10240/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [10496/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [10624/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [10752/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [10880/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 12 [11008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [11136/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [11264/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [11392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [11520/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [11648/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [11776/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [11904/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [12032/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [12160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [12288/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [12416/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [12544/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [12672/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [12800/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [12928/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [13056/50000]\tLoss: 4.6036\tLR: 0.010000\n",
            "Training Epoch: 12 [13184/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [13312/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [13440/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [13568/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [13696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [13824/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [13952/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [14080/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [14208/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [14336/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [14464/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [14592/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [14720/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [14848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [14976/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [15104/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [15232/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [15360/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [15488/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [15616/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [15744/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [15872/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [16000/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [16128/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 12 [16256/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [16384/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [16512/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [16640/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [16768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [16896/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [17024/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [17152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [17280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [17408/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [17536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [17664/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [17792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [17920/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 12 [18048/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [18176/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [18304/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [18432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [18560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [18688/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 12 [18816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [18944/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [19072/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [19200/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [19328/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [19456/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [19584/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [19712/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 12 [19840/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [19968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [20096/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [20224/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [20352/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [20480/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [20608/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [20736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [20864/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [20992/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [21120/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [21248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [21376/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [21504/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [21632/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [21760/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [21888/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [22016/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [22144/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [22272/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [22400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [22528/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [22656/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [22784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [22912/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [23040/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [23168/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [23296/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [23424/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [23552/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [23680/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [23808/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [23936/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 12 [24064/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [24192/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [24320/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [24448/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [24576/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 12 [24704/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [24832/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [24960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [25088/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [25216/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [25344/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [25472/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [25600/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [25728/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [25856/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [25984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [26112/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [26240/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [26368/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [26496/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [26624/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [26752/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [26880/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [27008/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [27136/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [27264/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [27392/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [27520/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [27648/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [27776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [27904/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [28032/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [28160/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [28288/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [28416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [28544/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [28672/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [28800/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [28928/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [29056/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [29184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [29312/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [29440/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [29568/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [29696/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [29824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [29952/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [30080/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [30208/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [30336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [30464/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [30592/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [30720/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [30848/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [30976/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [31104/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [31232/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [31360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [31488/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [31616/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [31744/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [31872/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 12 [32000/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [32128/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [32256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [32384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [32512/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [32640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [32768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [32896/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [33024/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 12 [33152/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [33280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [33408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [33536/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [33664/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 12 [33792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [33920/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [34048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [34176/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [34304/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 12 [34432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [34560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [34688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [34816/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [34944/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [35072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [35200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [35328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [35456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [35584/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [35712/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [35840/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [35968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [36096/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [36224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [36352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [36480/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [36608/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [36736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [36864/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [36992/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [37120/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 12 [37248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [37376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [37504/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [37632/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 12 [37760/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [37888/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [38016/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [38144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [38272/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [38400/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [38528/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [38656/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [38784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [38912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [39040/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 12 [39168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [39296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [39424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [39552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [39680/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [39808/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 12 [39936/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [40064/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [40192/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 12 [40320/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [40448/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [40576/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [40704/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [40832/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 12 [40960/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [41088/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [41216/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [41344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [41472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [41600/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [41728/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [41856/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [41984/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 12 [42112/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [42240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [42368/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [42496/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [42624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [42752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [42880/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [43008/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 12 [43136/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [43264/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [43392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [43520/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [43648/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [43776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [43904/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [44032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [44160/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [44288/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [44416/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [44544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [44672/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [44800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [44928/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [45056/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 12 [45184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [45312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [45440/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [45568/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [45696/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [45824/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [45952/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [46080/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [46208/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 12 [46336/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [46464/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [46592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [46720/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [46848/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 12 [46976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [47104/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 12 [47232/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [47360/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [47488/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [47616/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 12 [47744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [47872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [48000/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [48128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 12 [48256/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [48384/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [48512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [48640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 12 [48768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [48896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [49024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 12 [49152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 12 [49280/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 12 [49408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 12 [49536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 12 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 12 [49792/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 12 [49920/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 12 [50000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "epoch 12 training time consumed: 184.45s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB |  91044 GiB |  91043 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  90974 GiB |  90973 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     69 GiB |     69 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB |  91044 GiB |  91043 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  90974 GiB |  90973 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     69 GiB |     69 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB |  91034 GiB |  91034 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB |  90964 GiB |  90964 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     69 GiB |     69 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  72336 GiB |  72336 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  72257 GiB |  72257 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     78 GiB |     78 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    2462 K  |    2462 K  |\n",
            "|       from large pool |      92    |     145    |    1843 K  |    1843 K  |\n",
            "|       from small pool |     159    |     206    |     619 K  |     619 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    2462 K  |    2462 K  |\n",
            "|       from large pool |      92    |     145    |    1843 K  |    1843 K  |\n",
            "|       from small pool |     159    |     206    |     619 K  |     619 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      82    |    1035 K  |    1035 K  |\n",
            "|       from large pool |      29    |      73    |     966 K  |     966 K  |\n",
            "|       from small pool |       9    |      12    |      68 K  |      68 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 12, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.24s\n",
            "\n",
            "Training Epoch: 13 [128/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [512/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [640/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [768/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [896/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [1024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [1152/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [1280/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [1408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [1536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [1664/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [1792/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [1920/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [2048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [2176/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [2304/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [2432/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [2560/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [2688/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [2816/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [2944/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [3072/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [3200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [3328/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [3456/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [3584/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [3712/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [3840/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [3968/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [4096/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [4224/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [4352/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [4480/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [4608/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [4736/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [4864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [4992/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [5120/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [5248/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [5376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [5504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [5632/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [5760/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [5888/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [6016/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [6144/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [6272/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [6400/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [6528/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [6656/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [6784/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [6912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [7040/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [7168/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [7296/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [7424/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [7552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [7680/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [7808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [7936/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [8064/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [8192/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [8320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [8448/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [8576/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [8704/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [8832/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [8960/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [9088/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 13 [9216/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [9344/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [9472/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [9600/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [9728/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [9856/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [9984/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [10112/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [10240/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [10368/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [10496/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [10624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [10752/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [10880/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [11008/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [11136/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [11264/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [11392/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [11520/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [11648/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [11776/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [11904/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [12032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [12160/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [12288/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [12416/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [12544/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [12672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [12800/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [12928/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [13056/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [13184/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [13312/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [13440/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [13568/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [13696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [13824/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [13952/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [14080/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [14208/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [14336/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [14464/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [14592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [14720/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [14848/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [14976/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [15104/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [15232/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [15360/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 13 [15488/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [15616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [15744/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [15872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [16000/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [16128/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [16256/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [16384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [16512/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [16640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [16768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [16896/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [17024/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [17152/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [17280/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [17408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [17536/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [17664/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [17792/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [17920/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [18048/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [18176/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [18304/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [18432/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [18560/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [18688/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [18816/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [18944/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [19072/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [19200/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [19328/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [19456/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [19584/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [19712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [19840/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [19968/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [20096/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [20224/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [20352/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [20480/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [20608/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [20736/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [20864/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [20992/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [21120/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [21248/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [21376/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [21504/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 13 [21632/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [21760/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [21888/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [22016/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [22144/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [22272/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [22400/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [22528/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [22656/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 13 [22784/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [22912/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [23040/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [23168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [23296/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [23424/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [23552/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [23680/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 13 [23808/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [23936/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [24064/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [24192/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [24320/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [24448/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [24576/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [24704/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [24832/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [24960/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [25088/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [25216/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [25344/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [25472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [25600/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [25728/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [25856/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [25984/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [26112/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [26240/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [26368/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [26496/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [26624/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [26752/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 13 [26880/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [27008/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [27136/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [27264/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [27392/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [27520/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [27648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [27776/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [27904/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [28032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [28160/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [28288/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [28416/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [28544/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 13 [28672/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [28800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [28928/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [29056/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [29184/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [29312/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [29440/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [29568/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [29696/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 13 [29824/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [29952/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [30080/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [30208/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [30336/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [30464/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [30592/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [30720/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [30848/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [30976/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [31104/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [31232/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [31360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [31488/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [31616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [31744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [31872/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [32000/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [32128/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [32256/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [32384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [32512/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [32640/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [32768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [32896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [33024/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [33152/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [33280/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 13 [33408/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 13 [33536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [33664/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [33792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [33920/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [34048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [34176/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [34304/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [34432/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [34560/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [34688/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [34816/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [34944/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [35072/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [35200/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [35328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [35456/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 13 [35584/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [35712/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 13 [35840/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [35968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [36096/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [36224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [36352/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 13 [36480/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [36608/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [36736/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [36864/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 13 [36992/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [37120/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [37248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [37376/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [37504/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [37632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [37760/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 13 [37888/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [38016/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [38144/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 13 [38272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [38400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [38528/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [38656/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 13 [38784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [38912/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [39040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [39168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [39296/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [39424/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 13 [39552/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [39680/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 13 [39808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [39936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [40064/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [40192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [40320/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [40448/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [40576/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [40704/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [40832/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [40960/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [41088/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [41216/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [41344/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 13 [41472/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [41600/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [41728/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [41856/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [41984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [42112/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [42240/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [42368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [42496/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [42624/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [42752/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [42880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [43008/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [43136/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [43264/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [43392/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [43520/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 13 [43648/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [43776/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [43904/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [44032/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [44160/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 13 [44288/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [44416/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 13 [44544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [44672/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [44800/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [44928/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [45056/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [45184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [45312/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [45440/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [45568/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [45696/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [45824/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [45952/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 13 [46080/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 13 [46208/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [46336/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [46464/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [46592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [46720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 13 [46848/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [46976/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [47104/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [47232/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [47360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [47488/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 13 [47616/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [47744/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [47872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 13 [48000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [48128/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 13 [48256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 13 [48384/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [48512/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [48640/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [48768/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 13 [48896/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [49024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 13 [49152/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [49280/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 13 [49408/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 13 [49536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 13 [49664/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [49792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 13 [49920/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 13 [50000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "epoch 13 training time consumed: 184.51s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB |  98629 GiB |  98629 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  98553 GiB |  98553 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     75 GiB |     75 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB |  98629 GiB |  98629 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB |  98553 GiB |  98553 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     75 GiB |     75 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB |  98619 GiB |  98618 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB |  98543 GiB |  98543 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     75 GiB |     75 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  78361 GiB |  78361 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  78276 GiB |  78276 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     85 GiB |     85 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    2667 K  |    2667 K  |\n",
            "|       from large pool |      92    |     145    |    1997 K  |    1996 K  |\n",
            "|       from small pool |     159    |     206    |     670 K  |     670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    2667 K  |    2667 K  |\n",
            "|       from large pool |      92    |     145    |    1997 K  |    1996 K  |\n",
            "|       from small pool |     159    |     206    |     670 K  |     670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1121 K  |    1121 K  |\n",
            "|       from large pool |      29    |      73    |    1047 K  |    1047 K  |\n",
            "|       from small pool |       8    |      12    |      74 K  |      74 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 13, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.24s\n",
            "\n",
            "Training Epoch: 14 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [384/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [512/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [768/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [1024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [1152/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [1280/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [1408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [1536/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [1664/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [1920/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [2048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [2176/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [2304/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [2432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [2560/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [2688/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [2816/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [2944/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [3072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [3200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [3328/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [3456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [3584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [3712/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [3840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [3968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [4096/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [4224/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [4352/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [4480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [4608/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [4736/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [4864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [4992/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [5120/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [5248/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [5376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [5504/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [5632/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [5760/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [5888/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [6016/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [6144/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [6272/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [6400/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [6528/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [6656/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [6784/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [6912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [7040/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [7168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [7296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [7424/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [7552/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [7680/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [7808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [7936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [8064/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [8192/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [8320/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [8448/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [8576/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [8704/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [8832/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [8960/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [9088/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [9216/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [9344/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [9472/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [9600/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [9728/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [9856/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [9984/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [10112/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [10240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [10496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [10624/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [10752/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [10880/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [11008/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [11264/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [11392/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [11520/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [11648/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [11776/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [11904/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [12032/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [12160/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [12288/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [12416/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [12544/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [12672/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [12800/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [12928/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [13056/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [13184/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [13312/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 14 [13440/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [13568/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [13696/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [13824/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [13952/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [14080/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [14208/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [14336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [14464/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 14 [14592/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [14720/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [14848/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [14976/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [15104/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [15232/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [15360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [15488/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [15616/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [15744/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [15872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [16000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [16128/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [16256/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [16384/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [16512/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [16640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [16768/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [16896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [17024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [17152/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [17280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [17408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [17536/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [17664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [17792/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 14 [17920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [18048/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [18176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [18304/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [18432/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [18560/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [18688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [18816/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [18944/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [19072/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [19200/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [19328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [19456/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [19584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [19712/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 14 [19840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [19968/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [20096/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [20224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [20352/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [20480/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [20608/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [20736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [20864/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [20992/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [21120/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [21248/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [21376/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [21504/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [21632/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [21760/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [21888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [22016/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [22144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [22272/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [22400/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [22528/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [22656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [22784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [22912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [23040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [23168/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [23296/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [23424/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [23552/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [23680/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [23808/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [23936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [24064/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [24192/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [24320/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [24448/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [24576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [24704/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 14 [24832/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [24960/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [25088/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [25216/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [25344/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [25472/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [25600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [25728/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [25856/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [25984/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [26112/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [26240/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [26368/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 14 [26496/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [26624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [26752/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [26880/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [27008/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [27136/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 14 [27264/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [27392/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [27520/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [27648/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 14 [27776/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [27904/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [28032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [28160/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [28288/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [28416/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [28544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [28672/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [28800/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [28928/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [29056/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [29184/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [29312/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [29440/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [29568/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [29696/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [29824/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [29952/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [30080/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [30208/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [30336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [30464/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [30592/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 14 [30720/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [30848/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [30976/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [31104/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [31232/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [31360/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [31488/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 14 [31616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [31744/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [31872/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [32000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [32128/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [32256/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [32384/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [32512/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [32640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [32768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [32896/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [33024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [33152/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 14 [33280/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 14 [33408/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [33536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [33664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [33792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [33920/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [34048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [34176/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [34304/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [34432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [34560/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [34688/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 14 [34816/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [34944/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [35072/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [35200/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [35328/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 14 [35456/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [35584/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [35712/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [35840/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [35968/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [36096/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [36224/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [36352/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [36480/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [36608/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [36736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [36864/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [36992/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [37120/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [37248/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [37376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [37504/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [37632/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [37760/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [37888/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [38016/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [38144/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [38272/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [38400/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [38528/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [38656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [38784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [38912/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [39040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [39168/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [39296/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [39424/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [39552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [39680/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [39808/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [39936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [40064/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [40192/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [40320/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [40448/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [40576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [40704/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [40832/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [40960/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [41088/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [41216/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 14 [41344/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [41472/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [41600/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [41728/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [41856/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [41984/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [42112/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [42240/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [42368/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [42496/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [42624/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [42752/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [42880/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [43008/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [43136/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 14 [43264/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 14 [43392/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 14 [43520/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [43648/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [43776/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [43904/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [44032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [44160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [44288/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [44416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [44544/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 14 [44672/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [44800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [44928/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [45056/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [45184/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [45312/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [45440/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [45568/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [45696/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [45824/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 14 [45952/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [46080/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [46208/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [46336/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 14 [46464/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [46592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [46720/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [46848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [46976/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [47104/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [47232/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [47360/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 14 [47488/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [47616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [47744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [47872/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 14 [48000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 14 [48128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 14 [48256/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [48384/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 14 [48512/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 14 [48640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 14 [48768/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 14 [48896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 14 [49024/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 14 [49152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [49280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 14 [49408/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [49536/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 14 [49792/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 14 [49920/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 14 [50000/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "epoch 14 training time consumed: 184.31s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 106214 GiB | 106214 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 106133 GiB | 106133 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     81 GiB |     81 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 106214 GiB | 106214 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 106133 GiB | 106133 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     81 GiB |     81 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 106203 GiB | 106203 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 106122 GiB | 106121 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     81 GiB |     81 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  84387 GiB |  84386 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  84295 GiB |  84294 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     92 GiB |     92 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    2873 K  |    2872 K  |\n",
            "|       from large pool |      92    |     145    |    2150 K  |    2150 K  |\n",
            "|       from small pool |     159    |     206    |     722 K  |     722 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    2873 K  |    2872 K  |\n",
            "|       from large pool |      92    |     145    |    2150 K  |    2150 K  |\n",
            "|       from small pool |     159    |     206    |     722 K  |     722 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1207 K  |    1207 K  |\n",
            "|       from large pool |      29    |      73    |    1127 K  |    1127 K  |\n",
            "|       from small pool |       8    |      12    |      80 K  |      80 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 14, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.23s\n",
            "\n",
            "Training Epoch: 15 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [256/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [512/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [768/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [896/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [1024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [1152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [1280/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [1408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [1536/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [1664/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [1792/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [1920/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [2048/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [2176/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [2304/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [2432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [2560/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [2688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [2816/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [2944/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [3072/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [3200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [3328/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [3456/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [3584/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [3712/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [3840/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [3968/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [4096/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [4224/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [4352/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [4480/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [4608/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [4736/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [4864/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [4992/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [5120/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [5248/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [5376/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [5504/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [5632/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [5760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [5888/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [6016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [6144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [6272/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [6400/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [6528/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [6656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [6784/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [6912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [7040/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [7168/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [7296/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [7424/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [7552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [7680/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [7808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [7936/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [8064/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [8192/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [8320/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [8448/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [8576/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [8704/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [8832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [8960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [9088/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [9216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [9344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [9472/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [9600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [9728/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [9856/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [9984/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [10112/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [10240/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [10496/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [10624/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [10752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [10880/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [11008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [11136/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [11264/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [11392/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [11520/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [11648/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [11776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [11904/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [12032/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [12160/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [12288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [12416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [12544/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [12672/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [12800/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [12928/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [13056/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [13184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [13312/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [13440/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [13568/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [13696/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [13824/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [13952/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [14080/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [14208/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [14336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [14464/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [14592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [14720/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [14848/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [14976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [15104/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [15232/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [15360/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [15488/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [15616/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [15744/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [15872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [16000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [16128/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [16256/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [16384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [16512/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [16640/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [16768/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [16896/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [17024/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [17152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [17280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [17408/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [17536/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [17664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [17792/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [17920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [18048/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [18176/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [18304/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [18432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [18560/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [18688/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [18816/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [18944/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [19072/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [19200/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [19328/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [19456/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [19584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [19712/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 15 [19840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [19968/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [20096/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [20224/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [20352/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [20480/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [20608/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [20736/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [20864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [20992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [21120/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [21248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [21376/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [21504/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [21632/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [21760/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [21888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [22016/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [22144/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [22272/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [22400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [22528/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [22656/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [22784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [22912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [23040/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [23168/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [23296/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [23424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [23552/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [23680/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 15 [23808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [23936/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [24064/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [24192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [24320/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [24448/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [24576/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [24704/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [24832/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [24960/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [25088/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [25216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [25344/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [25472/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 15 [25600/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [25728/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [25856/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [25984/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [26112/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [26240/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [26368/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 15 [26496/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 15 [26624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [26752/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [26880/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [27008/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [27136/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [27264/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [27392/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [27520/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 15 [27648/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [27776/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [27904/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [28032/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 15 [28160/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [28288/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [28416/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [28544/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [28672/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [28800/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [28928/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 15 [29056/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [29184/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 15 [29312/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [29440/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [29568/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [29696/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [29824/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [29952/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [30080/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [30208/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [30336/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [30464/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [30592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [30720/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [30848/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [30976/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [31104/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [31232/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [31360/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [31488/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [31616/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [31744/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [31872/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [32000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [32128/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [32256/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [32384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [32512/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 15 [32640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [32768/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [32896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [33024/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [33152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [33280/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [33408/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 15 [33536/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [33664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [33792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [33920/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [34048/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [34176/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [34304/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [34432/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 15 [34560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [34688/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [34816/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [34944/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [35072/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [35200/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [35328/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [35456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [35584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [35712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [35840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [35968/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [36096/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 15 [36224/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [36352/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 15 [36480/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [36608/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [36736/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [36864/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [36992/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 15 [37120/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [37248/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [37376/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 15 [37504/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [37632/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [37760/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [37888/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [38016/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [38144/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [38272/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [38400/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [38528/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [38656/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [38784/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [38912/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [39040/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [39168/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [39296/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [39424/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [39552/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [39680/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [39808/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [39936/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [40064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [40192/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [40320/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [40448/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [40576/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [40704/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [40832/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 15 [40960/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [41088/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [41216/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [41344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [41472/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [41600/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [41728/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [41856/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [41984/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [42112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [42240/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 15 [42368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [42496/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [42624/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [42752/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [42880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [43008/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [43136/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [43264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [43392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [43520/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [43648/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 15 [43776/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [43904/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [44032/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [44160/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [44288/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [44416/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 15 [44544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [44672/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [44800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [44928/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [45056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [45184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [45312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [45440/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [45568/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [45696/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 15 [45824/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [45952/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [46080/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 15 [46208/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [46336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [46464/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [46592/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [46720/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [46848/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [46976/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [47104/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [47232/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [47360/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 15 [47488/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [47616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [47744/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [47872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [48000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [48128/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 15 [48256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 15 [48384/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [48512/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 15 [48640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 15 [48768/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 15 [48896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 15 [49024/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 15 [49152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [49280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 15 [49408/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [49536/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [49664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 15 [49792/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 15 [49920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 15 [50000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "epoch 15 training time consumed: 184.45s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 113800 GiB | 113800 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 113712 GiB | 113712 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     87 GiB |     87 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 113800 GiB | 113800 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 113712 GiB | 113712 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     87 GiB |     87 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 113788 GiB | 113787 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 113700 GiB | 113700 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     87 GiB |     87 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  90412 GiB |  90412 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  90313 GiB |  90313 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |     98 GiB |     98 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    3078 K  |    3078 K  |\n",
            "|       from large pool |      92    |     145    |    2304 K  |    2304 K  |\n",
            "|       from small pool |     159    |     206    |     774 K  |     773 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    3078 K  |    3078 K  |\n",
            "|       from large pool |      92    |     145    |    2304 K  |    2304 K  |\n",
            "|       from small pool |     159    |     206    |     774 K  |     773 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      82    |    1293 K  |    1293 K  |\n",
            "|       from large pool |      29    |      73    |    1208 K  |    1208 K  |\n",
            "|       from small pool |       9    |      12    |      85 K  |      85 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 15, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.21s\n",
            "\n",
            "Training Epoch: 16 [128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [256/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [384/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [512/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [896/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [1024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [1152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [1280/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [1408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [1536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [1664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [1920/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [2048/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [2176/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [2304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [2432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [2560/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [2688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [2816/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [2944/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [3072/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [3200/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [3328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [3456/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [3584/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [3712/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [3840/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [3968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [4096/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [4224/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [4352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [4480/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [4608/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [4736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [4864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [4992/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [5120/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [5248/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [5376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [5504/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [5632/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [5760/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [5888/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [6016/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [6144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [6272/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [6400/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [6528/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [6656/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [6784/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [6912/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [7168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [7296/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [7424/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [7552/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [7680/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [7808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [7936/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [8064/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [8192/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [8320/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [8448/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [8576/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [8704/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [8832/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [8960/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [9088/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [9216/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [9344/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [9472/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [9600/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [9728/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [9856/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [9984/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [10112/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [10240/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [10368/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [10496/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [10624/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [10752/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [10880/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [11008/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [11136/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [11264/50000]\tLoss: 4.6030\tLR: 0.010000\n",
            "Training Epoch: 16 [11392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [11520/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [11648/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [11776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [11904/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [12032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [12160/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [12288/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [12416/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [12544/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [12672/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [12800/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [12928/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [13056/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 16 [13184/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [13312/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [13440/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [13568/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [13696/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [13824/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [13952/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [14080/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [14208/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 16 [14336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [14464/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [14592/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [14720/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [14848/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [14976/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [15104/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [15232/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [15360/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [15488/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [15616/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [15744/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [15872/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [16000/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [16128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [16256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [16384/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 16 [16512/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [16640/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [16768/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [16896/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [17024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [17152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [17280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [17408/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [17536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [17664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [17792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [17920/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [18048/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [18176/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [18304/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [18432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [18560/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 16 [18688/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [18816/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [18944/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [19072/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [19200/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [19328/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [19456/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [19584/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [19712/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [19840/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [19968/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [20096/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [20224/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [20352/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [20480/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [20608/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [20736/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [20864/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 16 [20992/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [21120/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [21248/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [21376/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [21504/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [21632/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [21760/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [21888/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [22016/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [22144/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [22272/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [22400/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [22528/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [22656/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [22784/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [22912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [23040/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [23168/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [23296/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [23424/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [23552/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [23680/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [23808/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [23936/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [24064/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [24192/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [24320/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [24448/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 16 [24576/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [24704/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [24832/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [24960/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [25088/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [25216/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [25344/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [25472/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [25600/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [25728/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [25856/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [25984/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [26112/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [26240/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [26368/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [26496/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [26624/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [26752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [26880/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [27008/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [27136/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [27264/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [27392/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [27520/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [27648/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [27776/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [27904/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [28032/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [28160/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [28288/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [28416/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [28544/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [28672/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [28800/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 16 [28928/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 16 [29056/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [29184/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [29312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [29440/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [29568/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [29696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [29824/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [29952/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [30080/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [30208/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [30336/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [30464/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [30592/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [30720/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [30848/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [30976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [31104/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [31232/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [31360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [31488/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [31616/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [31744/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 16 [31872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [32000/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [32128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [32256/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [32384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [32512/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [32640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [32768/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [32896/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [33024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [33152/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [33280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [33408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [33536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [33664/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [33792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [33920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [34048/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [34176/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 16 [34304/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [34432/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [34560/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [34688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [34816/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 16 [34944/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [35072/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 16 [35200/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [35328/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [35456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 16 [35584/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [35712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [35840/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [35968/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [36096/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [36224/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [36352/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [36480/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [36608/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [36736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [36864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [36992/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [37120/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [37248/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [37376/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [37504/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [37632/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 16 [37760/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [37888/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [38016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [38144/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [38272/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [38400/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 16 [38528/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [38656/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [38784/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 16 [38912/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [39040/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [39168/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [39296/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [39424/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [39552/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [39680/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 16 [39808/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [39936/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [40064/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [40192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [40320/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [40448/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [40576/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [40704/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [40832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [40960/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [41088/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [41216/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [41344/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [41472/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [41600/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [41728/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [41856/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [41984/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [42112/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [42240/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [42368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [42496/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [42624/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [42752/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [42880/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [43008/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [43136/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [43264/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [43392/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [43520/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [43648/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 16 [43776/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [43904/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [44032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [44160/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [44288/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [44416/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [44544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [44672/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [44800/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [44928/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 16 [45056/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [45184/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 16 [45312/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [45440/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [45568/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [45696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [45824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [45952/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [46080/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 16 [46208/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [46336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [46464/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 16 [46592/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [46720/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [46848/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [46976/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [47104/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [47232/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [47360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 16 [47488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [47616/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [47744/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 16 [47872/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 16 [48000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [48128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [48256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [48384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 16 [48512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 16 [48640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [48768/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [48896/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 16 [49024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [49152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 16 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [49408/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 16 [49536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 16 [49664/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 16 [49792/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 16 [49920/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 16 [50000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "epoch 16 training time consumed: 184.69s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 121385 GiB | 121385 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 121292 GiB | 121292 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     93 GiB |     93 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 121385 GiB | 121385 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 121292 GiB | 121292 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     93 GiB |     93 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 121372 GiB | 121372 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 121279 GiB | 121279 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     93 GiB |     93 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB |  96437 GiB |  96437 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB |  96332 GiB |  96332 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |    105 GiB |    105 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    3283 K  |    3283 K  |\n",
            "|       from large pool |      92    |     145    |    2457 K  |    2457 K  |\n",
            "|       from small pool |     159    |     206    |     825 K  |     825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    3283 K  |    3283 K  |\n",
            "|       from large pool |      92    |     145    |    2457 K  |    2457 K  |\n",
            "|       from small pool |     159    |     206    |     825 K  |     825 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1380 K  |    1380 K  |\n",
            "|       from large pool |      29    |      73    |    1288 K  |    1288 K  |\n",
            "|       from small pool |       8    |      12    |      91 K  |      91 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 16, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.22s\n",
            "\n",
            "Training Epoch: 17 [128/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [384/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [512/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [640/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [768/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [896/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [1024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [1152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [1280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [1408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [1536/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [1664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [1792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [1920/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [2048/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [2176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2432/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [2560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [2688/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [2816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [2944/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [3072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [3200/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [3328/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [3456/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [3584/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [3712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [3840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [3968/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [4096/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [4224/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [4352/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [4480/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [4608/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [4736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [4864/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [4992/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [5120/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [5248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [5376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [5504/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [5632/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [5760/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [5888/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [6016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [6144/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [6272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [6400/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [6528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [6656/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [6784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [6912/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [7168/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [7296/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [7424/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [7552/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [7680/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [7808/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [7936/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [8064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [8192/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [8320/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [8448/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [8576/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [8704/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [8832/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [8960/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [9088/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [9216/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [9344/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [9472/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [9600/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [9728/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [9856/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [9984/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [10112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [10240/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [10496/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [10624/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [10752/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [10880/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [11008/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [11136/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [11264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [11392/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [11520/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [11648/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 17 [11776/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [11904/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [12032/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 17 [12160/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [12288/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [12416/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [12544/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [12672/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [12800/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [12928/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [13056/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [13184/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [13312/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [13440/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [13568/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [13696/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [13824/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [13952/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [14080/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [14208/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [14336/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [14464/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 17 [14592/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [14720/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 17 [14848/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [14976/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [15104/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [15232/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [15360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [15488/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [15616/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [15744/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [15872/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [16000/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 17 [16128/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [16256/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [16384/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [16512/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 17 [16640/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [16768/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [16896/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [17024/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [17152/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [17280/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [17408/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [17536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [17664/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [17792/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [17920/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [18048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [18176/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [18304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [18432/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [18560/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [18688/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [18816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [18944/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [19072/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [19200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [19328/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [19456/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [19584/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [19712/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [19840/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [19968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [20096/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [20224/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [20352/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [20480/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [20608/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [20736/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [20864/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [20992/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [21120/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [21248/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [21376/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [21504/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [21632/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [21760/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [21888/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [22016/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [22144/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [22272/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [22400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [22528/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [22656/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 17 [22784/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [22912/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 17 [23040/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [23168/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [23296/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [23424/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [23552/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [23680/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [23808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [23936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [24064/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [24192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [24320/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [24448/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [24576/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [24704/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [24832/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [24960/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [25088/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [25216/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [25344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [25472/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [25600/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 17 [25728/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [25856/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [25984/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [26112/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [26240/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [26368/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [26496/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [26624/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [26752/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [26880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [27008/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [27136/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [27264/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [27392/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [27520/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 17 [27648/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [27776/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [27904/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [28032/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [28160/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [28288/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [28416/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [28544/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [28672/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [28800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [28928/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [29056/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [29184/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [29312/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [29440/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [29568/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [29696/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [29824/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [29952/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [30080/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 17 [30208/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [30336/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 17 [30464/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [30592/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [30720/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [30848/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [30976/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 17 [31104/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [31232/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [31360/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [31488/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [31616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [31744/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [31872/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [32000/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [32128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [32256/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [32384/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [32512/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [32640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [32768/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [32896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [33024/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [33152/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [33280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [33408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [33536/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [33664/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [33792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [33920/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [34048/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [34176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [34304/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [34432/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [34560/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 17 [34688/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [34816/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 17 [34944/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [35072/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [35200/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [35328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [35456/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [35584/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [35712/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [35840/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [35968/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [36096/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [36224/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [36352/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [36480/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [36608/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [36736/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [36864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [36992/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [37120/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [37248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [37376/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [37504/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [37632/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [37760/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [37888/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 17 [38016/50000]\tLoss: 4.6074\tLR: 0.010000\n",
            "Training Epoch: 17 [38144/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [38272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [38400/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [38528/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [38656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [38784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [38912/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [39040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [39168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [39296/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [39424/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [39552/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [39680/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 17 [39808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [39936/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [40064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [40192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [40320/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [40448/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [40576/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [40704/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [40832/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [40960/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [41088/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [41216/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [41344/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [41472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [41600/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 17 [41728/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [41856/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [41984/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 17 [42112/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 17 [42240/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [42368/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [42496/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [42624/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 17 [42752/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [42880/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [43008/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [43136/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 17 [43264/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [43392/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [43520/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [43648/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [43776/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [43904/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [44032/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [44160/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [44288/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [44416/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [44544/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [44672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [44800/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [44928/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [45056/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [45184/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [45312/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 17 [45440/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [45568/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [45696/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 17 [45824/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [45952/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [46080/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [46208/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [46336/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 17 [46464/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 17 [46592/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [46720/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [46848/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [46976/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [47104/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 17 [47232/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 17 [47360/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [47488/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [47616/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [47744/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [47872/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [48000/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 17 [48128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [48256/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [48384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 17 [48512/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [48640/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 17 [48768/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [48896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 17 [49024/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 17 [49152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 17 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [49408/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [49536/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 17 [49664/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 17 [49792/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 17 [49920/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 17 [50000/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "epoch 17 training time consumed: 185.05s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 128971 GiB | 128970 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 128871 GiB | 128871 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     99 GiB |     99 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 128971 GiB | 128970 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 128871 GiB | 128871 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |     99 GiB |     99 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 128957 GiB | 128957 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 128858 GiB | 128858 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |     98 GiB |     98 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB | 102463 GiB | 102462 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB | 102351 GiB | 102351 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |    111 GiB |    111 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    3488 K  |    3488 K  |\n",
            "|       from large pool |      92    |     145    |    2611 K  |    2611 K  |\n",
            "|       from small pool |     159    |     206    |     877 K  |     877 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    3488 K  |    3488 K  |\n",
            "|       from large pool |      92    |     145    |    2611 K  |    2611 K  |\n",
            "|       from small pool |     159    |     206    |     877 K  |     877 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1466 K  |    1466 K  |\n",
            "|       from large pool |      29    |      73    |    1369 K  |    1369 K  |\n",
            "|       from small pool |       8    |      12    |      97 K  |      97 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 17, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.18s\n",
            "\n",
            "Training Epoch: 18 [128/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [256/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [512/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [640/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [768/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [896/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [1024/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [1152/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [1280/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [1408/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [1536/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [1664/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [1792/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [1920/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [2048/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [2176/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [2304/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [2432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [2560/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [2688/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [2816/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [2944/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [3072/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [3200/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [3328/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [3456/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [3584/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [3712/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [3840/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [3968/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [4096/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [4224/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [4352/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [4480/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [4608/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 18 [4736/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [4864/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [4992/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [5120/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [5248/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [5376/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [5504/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [5632/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [5760/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [5888/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [6016/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [6144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [6272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [6400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [6528/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [6656/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [6784/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 18 [6912/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [7040/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [7168/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [7296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [7424/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [7552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [7680/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [7808/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [7936/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [8064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [8192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [8320/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [8448/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [8576/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [8704/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [8832/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [8960/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [9088/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 18 [9216/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [9344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [9472/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [9600/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [9728/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [9856/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [9984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [10112/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [10240/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [10368/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [10496/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [10624/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [10752/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [10880/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [11008/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [11136/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [11264/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [11392/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [11520/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [11648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [11776/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [11904/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [12032/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [12160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [12288/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [12416/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [12544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [12672/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [12800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [12928/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [13056/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 18 [13184/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [13312/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [13440/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [13568/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [13696/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [13824/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [13952/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [14080/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [14208/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [14336/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [14464/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [14592/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 18 [14720/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [14848/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [14976/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [15104/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [15232/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [15360/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [15488/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [15616/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [15744/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 18 [15872/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [16000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [16128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [16256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [16384/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [16512/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [16640/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [16768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [16896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [17024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [17152/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [17280/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [17408/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [17536/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [17664/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 18 [17792/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [17920/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [18048/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [18176/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [18304/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [18432/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [18560/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [18688/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [18816/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [18944/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [19072/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [19200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [19328/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [19456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [19584/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [19712/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 18 [19840/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [19968/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [20096/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [20224/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [20352/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [20480/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [20608/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [20736/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [20864/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [20992/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [21120/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [21248/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [21376/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [21504/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [21632/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [21760/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [21888/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [22016/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [22144/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [22272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [22400/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 18 [22528/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [22656/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [22784/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [22912/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [23040/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [23168/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [23296/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [23424/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [23552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [23680/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [23808/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [23936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [24064/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [24192/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [24320/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [24448/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [24576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [24704/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [24832/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [24960/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [25088/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [25216/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [25344/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 18 [25472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [25600/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [25728/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 18 [25856/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [25984/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [26112/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [26240/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [26368/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 18 [26496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [26624/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [26752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [26880/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [27008/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [27136/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [27264/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [27392/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [27520/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [27648/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [27776/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [27904/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [28032/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [28160/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [28288/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [28416/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [28544/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 18 [28672/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [28800/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [28928/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [29056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [29184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [29312/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [29440/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [29568/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [29696/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [29824/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [29952/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [30080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [30208/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [30336/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 18 [30464/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [30592/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [30720/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [30848/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [30976/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [31104/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [31232/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [31360/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [31488/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [31616/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [31744/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [31872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [32000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [32128/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [32256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [32384/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [32512/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 18 [32640/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 18 [32768/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [32896/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [33024/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [33152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [33280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [33408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [33536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [33664/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [33792/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [33920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [34048/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [34176/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [34304/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 18 [34432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [34560/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [34688/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [34816/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 18 [34944/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [35072/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [35200/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [35328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [35456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [35584/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [35712/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [35840/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [35968/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [36096/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [36224/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [36352/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [36480/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 18 [36608/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [36736/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [36864/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [36992/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [37120/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [37248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [37376/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [37504/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [37632/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [37760/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [37888/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [38016/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [38144/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [38272/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [38400/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [38528/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [38656/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [38784/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [38912/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [39040/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 18 [39168/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [39296/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [39424/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [39552/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [39680/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [39808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [39936/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [40064/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [40192/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [40320/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [40448/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [40576/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [40704/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [40832/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [40960/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [41088/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [41216/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [41344/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [41472/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 18 [41600/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [41728/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 18 [41856/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [41984/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [42112/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [42240/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [42368/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [42496/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [42624/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [42752/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [42880/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [43008/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 18 [43136/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [43264/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [43392/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [43520/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [43648/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [43776/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [43904/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [44032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [44160/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [44288/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [44416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [44544/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 18 [44672/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [44800/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [44928/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [45056/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [45184/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 18 [45312/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [45440/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [45568/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [45696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [45824/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [45952/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [46080/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [46208/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [46336/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [46464/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [46592/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [46720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [46848/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [46976/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [47104/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [47232/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 18 [47360/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [47488/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [47616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [47744/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [47872/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 18 [48000/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [48128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 18 [48256/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 18 [48384/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [48512/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [48640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 18 [48768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [48896/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [49024/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 18 [49152/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 18 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 18 [49408/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 18 [49536/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [49664/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [49792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 18 [49920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 18 [50000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "epoch 18 training time consumed: 185.27s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 136556 GiB | 136556 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 136451 GiB | 136451 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    104 GiB |    104 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 136556 GiB | 136556 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 136451 GiB | 136451 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    104 GiB |    104 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 136541 GiB | 136541 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 136437 GiB | 136436 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |    104 GiB |    104 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB | 108488 GiB | 108488 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB | 108370 GiB | 108369 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |    118 GiB |    118 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    3693 K  |    3693 K  |\n",
            "|       from large pool |      92    |     145    |    2765 K  |    2764 K  |\n",
            "|       from small pool |     159    |     206    |     928 K  |     928 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    3693 K  |    3693 K  |\n",
            "|       from large pool |      92    |     145    |    2765 K  |    2764 K  |\n",
            "|       from small pool |     159    |     206    |     928 K  |     928 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      82    |    1552 K  |    1552 K  |\n",
            "|       from large pool |      29    |      73    |    1450 K  |    1450 K  |\n",
            "|       from small pool |       9    |      12    |     102 K  |     102 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 18, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.22s\n",
            "\n",
            "Training Epoch: 19 [128/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [256/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [384/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [512/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [640/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [768/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [896/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1024/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [1152/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1280/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1408/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [1536/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1664/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [1920/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [2048/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [2176/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [2304/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [2432/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [2560/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [2688/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [2816/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [2944/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [3072/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [3200/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [3328/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [3456/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [3584/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [3712/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [3840/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [3968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [4096/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [4224/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [4352/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [4480/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [4608/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [4736/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [4864/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [4992/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [5120/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [5248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [5376/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [5504/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [5632/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 19 [5760/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [5888/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [6016/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [6144/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [6272/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [6400/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [6528/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [6656/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [6784/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [6912/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [7040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [7168/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [7296/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [7424/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [7552/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [7680/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [7808/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [7936/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [8064/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [8192/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [8320/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [8448/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [8576/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [8704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [8832/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [8960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [9088/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [9216/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [9344/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [9472/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [9600/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [9728/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [9856/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [9984/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [10112/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [10240/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [10368/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [10496/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [10624/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [10752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [10880/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [11008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [11136/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [11264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [11392/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [11520/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [11648/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [11776/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [11904/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [12032/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [12160/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [12288/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [12416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [12544/50000]\tLoss: 4.6075\tLR: 0.010000\n",
            "Training Epoch: 19 [12672/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [12800/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [12928/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [13056/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [13184/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [13312/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [13440/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [13568/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [13696/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [13824/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [13952/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [14080/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [14208/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [14336/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [14464/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [14592/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [14720/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [14848/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [14976/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [15104/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [15232/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [15360/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [15488/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [15616/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [15744/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [15872/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [16000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [16128/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [16256/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [16384/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [16512/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [16640/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [16768/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [16896/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [17024/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [17152/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [17280/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [17408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [17536/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [17664/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [17792/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [17920/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [18048/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [18176/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [18304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [18432/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [18560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [18688/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [18816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [18944/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [19072/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 19 [19200/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [19328/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [19456/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [19584/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [19712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [19840/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [19968/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [20096/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 19 [20224/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [20352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [20480/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 19 [20608/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [20736/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [20864/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [20992/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [21120/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [21248/50000]\tLoss: 4.6040\tLR: 0.010000\n",
            "Training Epoch: 19 [21376/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [21504/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [21632/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [21760/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [21888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [22016/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [22144/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [22272/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [22400/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [22528/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [22656/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [22784/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [22912/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [23040/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [23168/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [23296/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [23424/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 19 [23552/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 19 [23680/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [23808/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [23936/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [24064/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [24192/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [24320/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [24448/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [24576/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [24704/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [24832/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [24960/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [25088/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [25216/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 19 [25344/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [25472/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [25600/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [25728/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [25856/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [25984/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [26112/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [26240/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [26368/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [26496/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [26624/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [26752/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [26880/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [27008/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [27136/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [27264/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [27392/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [27520/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [27648/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [27776/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [27904/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [28032/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [28160/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [28288/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [28416/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [28544/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [28672/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [28800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [28928/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [29056/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [29184/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [29312/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [29440/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [29568/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [29696/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [29824/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [29952/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [30080/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [30208/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [30336/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [30464/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [30592/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [30720/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [30848/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [30976/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [31104/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [31232/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [31360/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [31488/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [31616/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [31744/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [31872/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [32000/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [32128/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [32256/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [32384/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [32512/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [32640/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [32768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [32896/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 19 [33024/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [33152/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [33280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [33408/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [33536/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [33664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [33792/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [33920/50000]\tLoss: 4.6076\tLR: 0.010000\n",
            "Training Epoch: 19 [34048/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [34176/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 19 [34304/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 19 [34432/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [34560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [34688/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [34816/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [34944/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 19 [35072/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [35200/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [35328/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [35456/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [35584/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [35712/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [35840/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [35968/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [36096/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 19 [36224/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [36352/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [36480/50000]\tLoss: 4.6073\tLR: 0.010000\n",
            "Training Epoch: 19 [36608/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [36736/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [36864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [36992/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [37120/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [37248/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [37376/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [37504/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [37632/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [37760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [37888/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [38016/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [38144/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 19 [38272/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [38400/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [38528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [38656/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [38784/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [38912/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 19 [39040/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [39168/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [39296/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [39424/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 19 [39552/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [39680/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [39808/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [39936/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [40064/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [40192/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [40320/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [40448/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [40576/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [40704/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [40832/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [40960/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [41088/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [41216/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [41344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [41472/50000]\tLoss: 4.6072\tLR: 0.010000\n",
            "Training Epoch: 19 [41600/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [41728/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 19 [41856/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [41984/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [42112/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [42240/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [42368/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 19 [42496/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 19 [42624/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [42752/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [42880/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [43008/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [43136/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [43264/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [43392/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [43520/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [43648/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [43776/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [43904/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [44032/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [44160/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [44288/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [44416/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [44544/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [44672/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [44800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [44928/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [45056/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 19 [45184/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [45312/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [45440/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 19 [45568/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [45696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [45824/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [45952/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [46080/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 19 [46208/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [46336/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [46464/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [46592/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [46720/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [46848/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [46976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [47104/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 19 [47232/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [47360/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [47488/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 19 [47616/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [47744/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [47872/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [48000/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [48128/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 19 [48256/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 19 [48384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [48512/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [48640/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [48768/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [48896/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 19 [49024/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [49152/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 19 [49280/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [49408/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 19 [49536/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 19 [49664/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 19 [49792/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 19 [49920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 19 [50000/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "epoch 19 training time consumed: 185.76s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 144141 GiB | 144141 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 144031 GiB | 144030 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    110 GiB |    110 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 144141 GiB | 144141 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 144031 GiB | 144030 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    110 GiB |    110 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 144126 GiB | 144126 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 144015 GiB | 144015 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |    110 GiB |    110 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB | 114513 GiB | 114513 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB | 114388 GiB | 114388 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |    124 GiB |    124 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    3898 K  |    3898 K  |\n",
            "|       from large pool |      92    |     145    |    2918 K  |    2918 K  |\n",
            "|       from small pool |     159    |     206    |     980 K  |     980 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    3898 K  |    3898 K  |\n",
            "|       from large pool |      92    |     145    |    2918 K  |    2918 K  |\n",
            "|       from small pool |     159    |     206    |     980 K  |     980 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1639 K  |    1639 K  |\n",
            "|       from large pool |      29    |      73    |    1530 K  |    1530 K  |\n",
            "|       from small pool |       8    |      12    |     108 K  |     108 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 19, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.20s\n",
            "\n",
            "Training Epoch: 20 [128/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [384/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [512/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [640/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [1024/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [1152/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [1280/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [1408/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [1536/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [1664/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [1792/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [1920/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [2048/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [2176/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [2304/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [2432/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [2560/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [2688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [2816/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [2944/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [3072/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [3200/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [3328/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [3456/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [3584/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [3712/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [3840/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [3968/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [4096/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [4224/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [4352/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [4480/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [4608/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [4736/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [4864/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [4992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [5120/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [5248/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [5376/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [5504/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [5632/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [5760/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [5888/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [6016/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [6144/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [6272/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [6400/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [6528/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [6656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [6784/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [6912/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [7040/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [7168/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [7296/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [7424/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [7552/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [7680/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [7808/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [7936/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [8064/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [8192/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [8320/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [8448/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [8576/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [8704/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [8832/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [8960/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [9088/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [9216/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [9344/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [9472/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [9600/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [9728/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [9856/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [9984/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [10112/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [10240/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [10368/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [10496/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [10624/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [10752/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [10880/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [11008/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [11136/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [11264/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [11392/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [11520/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [11648/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [11776/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [11904/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [12032/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [12160/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [12288/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [12416/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [12544/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [12672/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [12800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [12928/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [13056/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [13184/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [13312/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [13440/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [13568/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [13696/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [13824/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [13952/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [14080/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [14208/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [14336/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [14464/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [14592/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [14720/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [14848/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [14976/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [15104/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [15232/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [15360/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [15488/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [15616/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [15744/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [15872/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [16000/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [16128/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [16256/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [16384/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [16512/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [16640/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [16768/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [16896/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [17024/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [17152/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [17280/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [17408/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [17536/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [17664/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [17792/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [17920/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [18048/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [18176/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [18304/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [18432/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [18560/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [18688/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [18816/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [18944/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [19072/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [19200/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [19328/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [19456/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [19584/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [19712/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [19840/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [19968/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [20096/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [20224/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [20352/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [20480/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [20608/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [20736/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [20864/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [20992/50000]\tLoss: 4.6038\tLR: 0.010000\n",
            "Training Epoch: 20 [21120/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [21248/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [21376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [21504/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [21632/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [21760/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [21888/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [22016/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [22144/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [22272/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [22400/50000]\tLoss: 4.6042\tLR: 0.010000\n",
            "Training Epoch: 20 [22528/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [22656/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [22784/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [22912/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [23040/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [23168/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [23296/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [23424/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [23552/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [23680/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [23808/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [23936/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [24064/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [24192/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [24320/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [24448/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [24576/50000]\tLoss: 4.6043\tLR: 0.010000\n",
            "Training Epoch: 20 [24704/50000]\tLoss: 4.6037\tLR: 0.010000\n",
            "Training Epoch: 20 [24832/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [24960/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [25088/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [25216/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [25344/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [25472/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [25600/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [25728/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [25856/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [25984/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [26112/50000]\tLoss: 4.6068\tLR: 0.010000\n",
            "Training Epoch: 20 [26240/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [26368/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [26496/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [26624/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [26752/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [26880/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [27008/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [27136/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [27264/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [27392/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [27520/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [27648/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [27776/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [27904/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [28032/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [28160/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [28288/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [28416/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [28544/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [28672/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [28800/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [28928/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [29056/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [29184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [29312/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [29440/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [29568/50000]\tLoss: 4.6069\tLR: 0.010000\n",
            "Training Epoch: 20 [29696/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [29824/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [29952/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [30080/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [30208/50000]\tLoss: 4.6035\tLR: 0.010000\n",
            "Training Epoch: 20 [30336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [30464/50000]\tLoss: 4.6071\tLR: 0.010000\n",
            "Training Epoch: 20 [30592/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [30720/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [30848/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [30976/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [31104/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [31232/50000]\tLoss: 4.6045\tLR: 0.010000\n",
            "Training Epoch: 20 [31360/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [31488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [31616/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [31744/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [31872/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [32000/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [32128/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [32256/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [32384/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [32512/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [32640/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [32768/50000]\tLoss: 4.6046\tLR: 0.010000\n",
            "Training Epoch: 20 [32896/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [33024/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [33152/50000]\tLoss: 4.6044\tLR: 0.010000\n",
            "Training Epoch: 20 [33280/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [33408/50000]\tLoss: 4.6048\tLR: 0.010000\n",
            "Training Epoch: 20 [33536/50000]\tLoss: 4.6039\tLR: 0.010000\n",
            "Training Epoch: 20 [33664/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [33792/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [33920/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [34048/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [34176/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [34304/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [34432/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [34560/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [34688/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [34816/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [34944/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [35072/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [35200/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [35328/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [35456/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [35584/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [35712/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [35840/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [35968/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [36096/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [36224/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [36352/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [36480/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [36608/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [36736/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [36864/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [36992/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [37120/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [37248/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [37376/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [37504/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [37632/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [37760/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [37888/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [38016/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 20 [38144/50000]\tLoss: 4.6041\tLR: 0.010000\n",
            "Training Epoch: 20 [38272/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [38400/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [38528/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [38656/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [38784/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [38912/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [39040/50000]\tLoss: 4.6050\tLR: 0.010000\n",
            "Training Epoch: 20 [39168/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [39296/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [39424/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [39552/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [39680/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [39808/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [39936/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [40064/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [40192/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [40320/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [40448/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [40576/50000]\tLoss: 4.6049\tLR: 0.010000\n",
            "Training Epoch: 20 [40704/50000]\tLoss: 4.6067\tLR: 0.010000\n",
            "Training Epoch: 20 [40832/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [40960/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [41088/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [41216/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [41344/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [41472/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [41600/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [41728/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [41856/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [41984/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [42112/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [42240/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [42368/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [42496/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [42624/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [42752/50000]\tLoss: 4.6070\tLR: 0.010000\n",
            "Training Epoch: 20 [42880/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [43008/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [43136/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [43264/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [43392/50000]\tLoss: 4.6066\tLR: 0.010000\n",
            "Training Epoch: 20 [43520/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [43648/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [43776/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [43904/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [44032/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [44160/50000]\tLoss: 4.6052\tLR: 0.010000\n",
            "Training Epoch: 20 [44288/50000]\tLoss: 4.6047\tLR: 0.010000\n",
            "Training Epoch: 20 [44416/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [44544/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [44672/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [44800/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [44928/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [45056/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [45184/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [45312/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [45440/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [45568/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [45696/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [45824/50000]\tLoss: 4.6051\tLR: 0.010000\n",
            "Training Epoch: 20 [45952/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "Training Epoch: 20 [46080/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [46208/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [46336/50000]\tLoss: 4.6060\tLR: 0.010000\n",
            "Training Epoch: 20 [46464/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [46592/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [46720/50000]\tLoss: 4.6062\tLR: 0.010000\n",
            "Training Epoch: 20 [46848/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [46976/50000]\tLoss: 4.6064\tLR: 0.010000\n",
            "Training Epoch: 20 [47104/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [47232/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [47360/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [47488/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [47616/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [47744/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [47872/50000]\tLoss: 4.6065\tLR: 0.010000\n",
            "Training Epoch: 20 [48000/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [48128/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [48256/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [48384/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [48512/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [48640/50000]\tLoss: 4.6061\tLR: 0.010000\n",
            "Training Epoch: 20 [48768/50000]\tLoss: 4.6055\tLR: 0.010000\n",
            "Training Epoch: 20 [48896/50000]\tLoss: 4.6054\tLR: 0.010000\n",
            "Training Epoch: 20 [49024/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [49152/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [49280/50000]\tLoss: 4.6053\tLR: 0.010000\n",
            "Training Epoch: 20 [49408/50000]\tLoss: 4.6058\tLR: 0.010000\n",
            "Training Epoch: 20 [49536/50000]\tLoss: 4.6057\tLR: 0.010000\n",
            "Training Epoch: 20 [49664/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [49792/50000]\tLoss: 4.6056\tLR: 0.010000\n",
            "Training Epoch: 20 [49920/50000]\tLoss: 4.6059\tLR: 0.010000\n",
            "Training Epoch: 20 [50000/50000]\tLoss: 4.6063\tLR: 0.010000\n",
            "epoch 20 training time consumed: 184.77s\n",
            "GPU INFO.....\n",
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      | 264643 KiB |   3755 MiB | 151727 GiB | 151726 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 151610 GiB | 151610 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    116 GiB |    116 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         | 264643 KiB |   3755 MiB | 151727 GiB | 151726 GiB |\n",
            "|       from large pool | 253024 KiB |   3747 MiB | 151610 GiB | 151610 GiB |\n",
            "|       from small pool |  11619 KiB |     19 MiB |    116 GiB |    116 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Requested memory      | 262469 KiB |   3754 MiB | 151711 GiB | 151710 GiB |\n",
            "|       from large pool | 250887 KiB |   3745 MiB | 151594 GiB | 151594 GiB |\n",
            "|       from small pool |  11581 KiB |     19 MiB |    116 GiB |    116 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   5516 MiB |   5516 MiB |   5516 MiB |      0 B   |\n",
            "|       from large pool |   5496 MiB |   5496 MiB |   5496 MiB |      0 B   |\n",
            "|       from small pool |     20 MiB |     20 MiB |     20 MiB |      0 B   |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory | 316988 KiB |   1972 MiB | 120539 GiB | 120538 GiB |\n",
            "|       from large pool | 314272 KiB |   1967 MiB | 120407 GiB | 120407 GiB |\n",
            "|       from small pool |   2716 KiB |      6 MiB |    131 GiB |    131 GiB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     251    |     328    |    4104 K  |    4103 K  |\n",
            "|       from large pool |      92    |     145    |    3072 K  |    3072 K  |\n",
            "|       from small pool |     159    |     206    |    1031 K  |    1031 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     251    |     328    |    4104 K  |    4103 K  |\n",
            "|       from large pool |      92    |     145    |    3072 K  |    3072 K  |\n",
            "|       from small pool |     159    |     206    |    1031 K  |    1031 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |      92    |      92    |       0    |\n",
            "|       from large pool |      82    |      82    |      82    |       0    |\n",
            "|       from small pool |      10    |      10    |      10    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      37    |      82    |    1725 K  |    1725 K  |\n",
            "|       from large pool |      29    |      73    |    1611 K  |    1611 K  |\n",
            "|       from small pool |       8    |      12    |     114 K  |     114 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "Evaluating Network.....\n",
            "Test set: Epoch: 20, Average loss: 0.0364, Accuracy: 0.0100, Time consumed:10.17s\n",
            "\n",
            "saving weights file to checkpoint/xception/Tuesday_25_July_2023_14h_29m_51s/xception-20-regular.pth\n"
          ]
        }
      ]
    }
  ]
}